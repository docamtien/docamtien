{
    "Question": [
        {
            "QuestionNumber": 1,
            "QuestionContent": "A company is building an application in the AWS Cloud. The application will store data in Amazon S3\nbuckets in two AWS Regions. The company must use an AWS Key Management Service (AWS KMS)\ncustomer managed key to encrypt all data that is stored in the S3 buckets. The data in both S3 buckets must be\nencrypted and decrypted with the same KMS key. The data and the key must be stored in each of the two\nRegions.\nWhich solution will meet these requirements with the LEAST operational overhead?",
            "Option": [
                "A.  Create an S3 bucket in each Region Configure the S3 buckets to use server-side encryption with\nAmazon S3 managed encryption keys (SSE-S3) Configure replication between the S3 buckets.",
                "B.  Create a customer managed multi-Region KMS key. Create an S3 bucket in each Region. Configure\nreplication between the S3 buckets. Configure the application to use the KMS key with client-side\nencryption.",
                "C.  Create a customer managed KMS key and an S3 bucket in each Region Configure the S3 buckets to use\nserver-side encryption with Amazon S3 managed encryption keys (SSE-S3) Configure replication\nbetween the S3 buckets.",
                "D.  Create a customer managed KMS key and an S3 bucket m each Region Configure the S3 buckets to use\nserver-side encryption with AWS KMS keys (SSE-KMS) Configure replication between the S3 buckets."
            ],
            "Explanation": "Answer: B\nExplanation\nFrom https://docs.aws.amazon.com/kms/latest/developerguide/custom-key-store-overview.html\nFor most users, the default AWS KMS key store, which is protected by FIPS 140-2 validated cryptographic\nmodules, fulfills their security requirements. There is no need to add an extra layer of maintenance\nresponsibility or a dependency on an additional service. However, you might consider creating a custom key\nstore if your organization has any of the following requirements: Key material cannot be stored in a shared\nenvironment. Key material must be subject to a secondary, independent audit path. The HSMs that generate\nand store key material must be certified at FIPS 140-2 Level 3.\nhttps://docs.aws.amazon.com/kms/latest/developerguide/custom-key-store-overview.html\nhttps://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 2,
            "QuestionContent": "A company is migrating a distributed application to AWS The application serves variable workloads The\nlegacy platform consists of a primary server trial coordinates jobs across multiple compute nodes The\ncompany wants to modernize the application with a solution that maximizes resiliency and scalability.\nHow should a solutions architect design the architecture to meet these requirements?",
            "Option": [
                "A.  Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination for the jobs\nImplement the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group.\nConfigure EC2 Auto Scaling to use scheduled scaling",
                "B.  Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination for the jobs\nImplement the compute nodes with Amazon EC2 Instances that are managed in an Auto Scaling group\nConfigure EC2 Auto Scaling based on the size of the queue",
                "C.  Implement the primary server and the compute nodes with Amazon EC2 instances that are managed In\nan Auto Scaling group. Configure AWS CloudTrail as a destination for the fobs Configure EC2 Auto\nScaling based on the load on the primary server",
                "D.  implement the primary server and the compute nodes with Amazon EC2 instances that are managed in\nan Auto Scaling group Configure Amazon EventBridge (Amazon CloudWatch Events) as a destination\nfor the jobs Configure EC2 Auto Scaling based on the load on the compute nodes"
            ],
            "Explanation": "Answer: B\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 3,
            "QuestionContent": "A company runs an application using Amazon ECS. The application creates esi/ed versions of an original\nimage and then makes Amazon S3 API calls to store the resized images in Amazon S3.\nHow can a solutions architect ensure that the application has permission to access Amazon S3?",
            "Option": [
                "A.  Update the S3 role in AWS IAM to allow read/write access from Amazon ECS, and then relaunch the\ncontainer.",
                "B.  Create an IAM role with S3 permissions, and then specify that role as the taskRoleAm in the task\ndefinition.",
                "C.  Create a security group that allows access from Amazon ECS to Amazon S3, and update the launch\nconfiguration used by the ECS cluster.",
                "D.  Create an IAM user with S3 permissions, and then relaunch the Amazon EC2 instances for the ECS\ncluster while logged in as this account."
            ],
            "Explanation": "Answer: B\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 4,
            "QuestionContent": "A company recently started using Amazon Aurora as the data store for its global ecommerce application When\nlarge reports are run developers report that the ecommerce application is performing poorly After reviewing\nmetrics in Amazon CloudWatch, a solutions architect finds that the ReadlOPS and CPUUtilization metrics are\nspiking when monthly reports run.\nWhat is the MOST cost-effective solution?",
            "Option": [
                "A.  Migrate the monthly reporting to Amazon Redshift.",
                "B.  Migrate the monthly reporting to an Aurora Replica",
                "C.  Migrate the Aurora database to a larger instance class",
                "D.  Increase the Provisioned IOPS on the Aurora instance"
            ],
            "Explanation": "Answer: B\nExplanation\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html\n#Aurora.Replication.Replicas Aurora Replicas have two main purposes. You can issue queries to them to scale\nthe read operations for your application. You typically do so by connecting to the reader endpoint of the\ncluster. That way, Aurora can spread the load for read-only connections across as many Aurora Replicas as\nyou have in the cluster. Aurora Replicas also help to increase availability. If the writer instance in a cluster\nbecomes unavailable, Aurora automatically promotes one of the reader instances to take its place as the new\nwriter.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.html\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 5,
            "QuestionContent": "A company hosts a marketing website in an on-premises data center. The website consists of static documents\nand runs on a single server. An administrator updates the website content infrequently and uses an SFTP client\nto upload new documents.\nThe company decides to host its website on AWS and to use Amazon CloudFront. The company\u0027s solutions\narchitect creates a CloudFront distribution. The solutions architect must design the most cost-effective and\nresilient architecture for website hosting to serve as the CloudFront origin.\nWhich solution will meet these requirements?",
            "Option": [
                "A.  Create a virtual server by using Amazon Lightsail. Configure the web server in the Lightsail instance.\nUpload website content by using an SFTP client.",
                "B.  Create an AWS Auto Scaling group for Amazon EC2 instances. Use an Application Load Balancer.\nUpload website content by using an SFTP client.",
                "C.  Create a private Amazon S3 bucket. Use an S3 bucket policy to allow access from a CloudFront origin\naccess identity (OAI). Upload website content by using theAWSCLI.",
                "D.  Create a public Amazon S3 bucket. Configure AWS Transfer for SFTP. Configure the S3 bucket for\nwebsite hosting. Upload website content by using the SFTP client."
            ],
            "Explanation": "Answer: D\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 6,
            "QuestionContent": "A solutions architect is designing a customer-facing application for a company. The application\u0027s database will\nhave a clearly defined access pattern throughout the year and will have a variable number of reads and writes\nthat depend on the time of year. The company must retain audit records for the database for 7 days. The\nrecovery point objective (RPO) must be less than 5 hours.\nWhich solution meets these requirements?",
            "Option": [
                "A.  Use Amazon DynamoDB with auto scaling Use on-demand backups and Amazon DynamoDB Streams",
                "B.  Use Amazon Redshift. Configure concurrency scaling. Activate audit logging. Perform database\nsnapshots every 4 hours.",
                "C.  Use Amazon RDS with Provisioned IOPS Activate the database auditing parameter Perform database\nsnapshots every 5 hours",
                "D.  Use Amazon Aurora MySQL with auto scaling. Activate the database auditing parameter"
            ],
            "Explanation": "Answer: B\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 7,
            "QuestionContent": "An application development team is designing a microservice that will convert large images to smaller,\ncompressed images. When a user uploads an image through the web interface, the microservice should store\nthe image in an Amazon S3 bucket, process and compress the image with an AWS Lambda function, and store\nthe image in its compressed form in a different S3 bucket.\nA solutions architect needs to design a solution that uses durable, stateless components to process the images\nautomatically.\nWhich combination of actions will meet these requirements? (Choose two.)",
            "Option": [
                "A.  Create an Amazon Simple Queue Service (Amazon SQS) queue Configure the S3 bucket to send a\nnotification to the SQS queue when an image is uploaded to the S3 bucket",
                "B.  Configure the Lambda function to use the Amazon Simple Queue Service (Amazon SQS) queue as the\ninvocation source When the SQS message is successfully processed, delete the message in the queue",
                "C.  Configure the Lambda function to monitor the S3 bucket for new uploads When an uploaded image is\ndetected write the file name to a text file in memory and use the text file to keep track of the images that\nwere processed",
                "D.  Launch an Amazon EC2 instance to monitor an Amazon Simple Queue Service (Amazon SQS) queue\nWhen items are added to the queue log the file name in a text file on the EC2 instance and invoke the\nLambda function",
                "E.  Configure an Amazon EventBridge (Amazon CloudWatch Events) event to monitor the S3 bucket When\nan image is uploaded. send an alert to an Amazon Simple Notification Service (Amazon SNS) topic with\nthe application owner\u0027s email address for further processing"
            ],
            "Explanation": "Answer: A B\n",
            "RightAnswer": [
                "A",
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 8,
            "QuestionContent": "A company recently signed a contract with an AWS Managed Service Provider (MSP) Partner for help with an\napplication migration initiative. A solutions architect needs to share an Amazon Machine Image (AMI) from\nan existing AWS account with the MSP Partner\u0027s AWS account. The AMI is backed by Amazon Elastic Block\nStore (Amazon EBS) and uses a customer managed customer master key (CMK) to encrypt EBS volume\nsnapshots.\nWhat is the MOST secure way for the solutions architect to share the AMI with the MSP Partner\u0027s AWS\naccount?",
            "Option": [
                "A.  Make the encrypted AMI and snapshots publicly available. Modify the CMK\u0027s key policy to allow the\nMSP Partner\u0027s AWS account to use the key",
                "B.  Modify the launchPermission property of the AMI. Share the AMI with the MSP Partner\u0027s AWS account\nonly. Modify the CMK\u0027s key policy to allow the MSP Partner\u0027s AWS account to use the key.",
                "C.  Modify the launchPermission property of the AMI Share the AMI with the MSP Partner\u0027s AWS account\nonly. Modify the CMK\u0027s key policy to trust a new CMK that is owned by the MSP Partner for\nencryption.",
                "D.  Export the AMI from the source account to an Amazon S3 bucket in the MSP Partner\u0027s AWS account.\nEncrypt the S3 bucket with a CMK that is owned by the MSP Partner Copy and launch the AMI in the\nMSP Partner\u0027s AWS account."
            ],
            "Explanation": "Answer: B\nExplanation\nShare the existing KMS key with the MSP external account because it has already been used to encrypt the\nAMI snapshot.\nhttps://docs.aws.amazon.com/kms/latest/developerguide/key-policy-modifying-external-accounts.html\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 9,
            "QuestionContent": "A company wants to run its critical applications in containers to meet requirements tor scalability and\navailability The company prefers to focus on maintenance of the critical applications The company does not\nwant to be responsible for provisioning and managing the underlying infrastructure that runs the containerized\nworkload\nWhat should a solutions architect do to meet those requirements?",
            "Option": [
                "A.  Use Amazon EC2 Instances, and Install Docker on the Instances",
                "B.  Use Amazon Elastic Container Service (Amazon ECS) on Amazon EC2 worker nodes",
                "C.  Use Amazon Elastic Container Service (Amazon ECS) on AWS Fargate",
                "D.  Use Amazon EC2 instances from an Amazon Elastic Container Service (Amazon ECS)-op6mized\nAmazon Machine Image (AMI)."
            ],
            "Explanation": "Answer: C\nExplanation\nusing AWS ECS on AWS Fargate since they requirements are for scalability and availability without having to\nprovision and manage the underlying infrastructure to run the containerized workload.\nhttps://docs.aws.amazon.com/AmazonECS/latest/userguide/what-is-fargate.html\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 10,
            "QuestionContent": "An application runs on Amazon EC2 instances in private subnets. The application needs to access an Amazon\nDynamoDB table. What is the MOST secure way to access the table while ensuring that the traffic does not\nleave the AWS network?",
            "Option": [
                "A.  Use a VPC endpoint for DynamoDB.",
                "B.  Use a NAT gateway in a public subnet.",
                "C.  Use a NAT instance in a private subnet.",
                "D.  Use the internet gateway attached to the VPC."
            ],
            "Explanation": "Answer: A\nExplanation\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html\nA VPC endpoint for DynamoDB enables Amazon EC2 instances in your VPC to use their private IP addresses\nto access DynamoDB with no exposure to the public internet. Your EC2 instances do not require public IP\naddresses, and you don\u0027t need an internet gateway, a NAT device, or a virtual private gateway in your VPC.\nYou use endpoint policies to control access to DynamoDB. Traffic between your VPC and the AWS service\ndoes not leave the Amazon network.\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 11,
            "QuestionContent": "A solutions architect must design a highly available infrastructure for a website. The website is powered by\nWindows web servers that run on Amazon EC2 instances. The solutions architect must implement a solution\nthat can mitigate a large-scale DDoS attack that originates from thousands of IP addresses. Downtime is not\nacceptable for the website.\nWhich actions should the solutions architect take to protect the website from such an attack? (Select TWO.)",
            "Option": [
                "A.  Use AWS Shield Advanced to stop the DDoS attack.",
                "B.  Configure Amazon GuardDuty to automatically block the attackers.",
                "C.  Configure the website to use Amazon CloudFront for both static and dynamic content.",
                "D.  Use an AWS Lambda function to automatically add attacker IP addresses to VPC network ACLs.",
                "E.  Use EC2 Spot Instances in an Auto Scaling group with a target tracking scaling policy that is set to 80%\nCPU utilization"
            ],
            "Explanation": "Answer: A C\nExplanation\n(https://aws.amazon.com/cloudfront\n",
            "RightAnswer": [
                "A",
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 12,
            "QuestionContent": "A bicycle sharing company is developing a multi-tier architecture to track the location of its bicycles during\npeak operating hours The company wants to use these data points in its existing analytics platform A solutions\narchitect must determine the most viable multi-tier option to support this architecture The data points must be\naccessible from the REST API.\nWhich action meets these requirements for storing and retrieving location data?",
            "Option": [
                "A.  Use Amazon Athena with Amazon S3",
                "B.  Use Amazon API Gateway with AWS Lambda",
                "C.  Use Amazon QuickSight with Amazon Redshift.",
                "D.  Use Amazon API Gateway with Amazon Kinesis Data Analytics"
            ],
            "Explanation": "Answer: D\nExplanation\nhttps://aws.amazon.com/solutions/implementations/aws-streaming-data-solution-for-amazon-kinesis/\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 13,
            "QuestionContent": "A company has an application that runs on Amazon EC2 instances and uses an Amazon Aurora database. The\nEC2 instances connect to the database by using user names and passwords that are stored locally in a file. The\ncompany wants to minimize the operational overhead of credential management.\nWhat should a solutions architect do to accomplish this goal?",
            "Option": [
                "A.  Use AWS Secrets Manager. Turn on automatic rotation.",
                "B.  Use AWS Systems Manager Parameter Store. Turn on automatic rotation.",
                "C.  Create an Amazon S3 bucket lo store objects that are encrypted with an AWS Key C. Management\nService (AWS KMS) encryption key. Migrate the credential file to the S3 bucket. Point the application\nto the S3 bucket.",
                "D.  Create an encrypted Amazon Elastic Block Store (Amazon EBS) volume (or each EC2 instance. Attach\nthe new EBS volume to each EC2 instance. Migrate the credential file to the new EBS volume. Point the\napplication to the new EBS volume."
            ],
            "Explanation": "Answer: A\nExplanation\nhttps://aws.amazon.com/cn/blogs/security/how-to-connect-to-aws-secrets-manager-service-within-a-virtual-private-cloud/\nhttps://aws.amazon.com/blogs/security/rotate-amazon-rds-database-credentials-automatically-with-aws-secrets-manager/\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 14,
            "QuestionContent": "A solutions architect is designing a new hybrid architecture to extend a company s on-premises infrastructure\nto AWS The company requires a highly available connection with consistent low latency to an AWS Region.\nThe company needs to minimize costs and is willing to accept slower traffic if the primary connection fails.\nWhat should the solutions architect do to meet these requirements?",
            "Option": [
                "A.  Provision an AWS Direct Connect connection to a Region Provision a VPN connection as a backup if\nthe primary Direct Connect connection fails.",
                "B.  Provision a VPN tunnel connection to a Region for private connectivity. Provision a second VPN tunnel\nfor private connectivity and as a backup if the primary VPN connection fails.",
                "C.  Provision an AWS Direct Connect connection to a Region Provision a second Direct Connect\nconnection to the same Region as a backup if the primary Direct Connect connection fails.",
                "D.  Provision an AWS Direct Connect connection to a Region Use the Direct Connect failover attribute\nfrom the AWS CLI to automatically create a backup connection if the primary Direct Connect\nconnection fails."
            ],
            "Explanation": "Answer: A\nExplanation\n\u0022In some cases, this connection alone is not enough. It is always better to guarantee a fallback connection as\nthe backup of DX. There are several options, but implementing it with an AWS Site-To-Site VPN is a real\ncost-effective solution that can be exploited to reduce costs or, in the meantime, wait for the setup of a second\nDX.\u0022\nhttps://www.proud2becloud.com/hybrid-cloud-networking-backup-aws-direct-connect-network-connection-with-aws-site-to-site-vpn/\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 15,
            "QuestionContent": "A company has a data ingestion workflow that includes the following components:\n\u2022 An Amazon Simple Notation Service (Amazon SNS) topic that receives notifications about new data\ndeliveries\n\u2022 An AWS Lambda function that processes and stores the data\nThe ingestion workflow occasionally fails because of network connectivity issues. When tenure occurs the\ncorresponding data is not ingested unless the company manually reruns the job. What should a solutions\narchitect do to ensure that all notifications are eventually processed?",
            "Option": [
                "A.  Configure the Lambda function (or deployment across multiple Availability Zones",
                "B.  Modify me Lambda functions configuration to increase the CPU and memory allocations tor the\n(unction",
                "C.  Configure the SNS topic\u0027s retry strategy to increase both the number of retries and the wait time between\nretries",
                "D.  Configure an Amazon Simple Queue Service (Amazon SQS) queue as the on failure destination Modify\nthe Lambda function to process messages in the queue"
            ],
            "Explanation": "Answer: A\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 16,
            "QuestionContent": "A company has an AWS Glue extract. transform, and load (ETL) job that runs every day at the same time. The\njob processes XML data that is in an Amazon S3 bucket.\nNew data is added to the S3 bucket every day. A solutions architect notices that AWS Glue is processing all\nthe data during each run.\nWhat should the solutions architect do to prevent AWS Glue from reprocessing old data?",
            "Option": [
                "A.  Edit the job to use job bookmarks.",
                "B.  Edit the job to delete data after the data is processed",
                "C.  Edit the job by setting the NumberOfWorkers field to 1.",
                "D.  Use a FindMatches machine learning (ML) transform."
            ],
            "Explanation": "Answer: A\nExplanation\nThis is the purpose of bookmarks: \u0022AWS Glue tracks data that has already been processed during a previous\nrun of an ETL job by persisting state information from the job run. This persisted state information is called a\njob bookmark. Job bookmarks help AWS Glue maintain state information and prevent the reprocessing of old\ndata.\u0022 https://docs.aws.amazon.com/glue/latest/dg/monitor-continuations.html\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 17,
            "QuestionContent": "A security team wants to limit access to specific services or actions in all of the team\u0027s AWS accounts. All\naccounts belong to a large organization in AWS Organizations. The solution must be scalable and there must\nbe a single point where permissions can be maintained.\nWhat should a solutions architect do to accomplish this?",
            "Option": [
                "A.  Create an ACL to provide access to the services or actions.",
                "B.  Create a security group to allow accounts and attach it to user groups.",
                "C.  Create cross-account roles in each account to deny access to the services or actions.",
                "D.  Create a service control policy in the root organizational unit to deny access to the services or actions."
            ],
            "Explanation": "Answer: D\nExplanation\nService control policies (SCPs) are one type of policy that you can use to manage your organization. SCPs\noffer central control over the maximum available permissions for all accounts in your organization, allowing\nyou to ensure your accounts stay within your organization\u0027s access control guidelines. See\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html.\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 18,
            "QuestionContent": "An ecommerce company wants to launch a one-deal-a-day website on AWS. Each day will feature exactly one\nproduct on sale for a period of 24 hours. The company wants to be able to handle millions of requests each\nhour with millisecond latency during peak hours.\nWhich solution will meet these requirements with the LEAST operational overhead?",
            "Option": [
                "A.  Use Amazon S3 to host the full website in different S3 buckets Add Amazon CloudFront distributions\nSet the S3 buckets as origins for the distributions Store the order data in Amazon S3",
                "B.  Deploy the full website on Amazon EC2 instances that run in Auto Scaling groups across multiple\nAvailability Zones Add an Application Load Balancer (ALB) to distribute the website traffic Add\nanother ALB for the backend APIs Store the data in Amazon RDS for MySQL",
                "C.  Migrate the full application to run in containers Host the containers on Amazon Elastic Kubernetes\nService (Amazon EKS) Use the Kubernetes Cluster Autoscaler to increase and decrease the number of\npods to process bursts in traffic Store the data in Amazon RDS for MySQL",
                "D.  Use an Amazon S3 bucket to host the website\u0027s static content Deploy an Amazon CloudFront\ndistribution. Set the S3 bucket as the origin Use Amazon API Gateway and AWS Lambda functions for\nthe backend APIs Store the data in Amazon DynamoDB"
            ],
            "Explanation": "Answer: D\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 19,
            "QuestionContent": "A company hosts a containerized web application on a fleet of on-premises servers that process incoming\nrequests. The number of requests is growing quickly. The on-premises servers cannot handle the increased\nnumber of requests. The company wants to move the application to AWS with minimum code changes and\nminimum development effort.\nWhich solution will meet these requirements with the LEAST operational overhead?",
            "Option": [
                "A.  Use AWS Fargate on Amazon Elastic Container Service (Amazon ECS) to run the containerized web\napplication with Service Auto Scaling. Use an Application Load Balancer to distribute the incoming\nrequests.",
                "B.  Use two Amazon EC2 instances to host the containerized web application. Use an Application Load\nBalancer to distribute the incoming requests",
                "C.  Use AWS Lambda with a new code that uses one of the supported languages. Create multiple Lambda\nfunctions to support the load. Use Amazon API Gateway as an entry point to the Lambda functions.",
                "D.  Use a high performance computing (HPC) solution such as AWS ParallelClusterto establish an HPC\ncluster that can process the incoming requests at the appropriate scale."
            ],
            "Explanation": "Answer: A\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 20,
            "QuestionContent": "A company is migrating its on-premises PostgreSQL database to Amazon Aurora PostgreSQL. The\non-premises database must remain online and accessible during the migration. The Aurora database must\nremain synchronized with the on-premises database.\nWhich combination of actions must a solutions architect take to meet these requirements? (Choose two.)",
            "Option": [
                "A.  Create an ongoing replication task.",
                "B.  Create a database backup of the on-premises database",
                "C.  Create an AWS Database Migration Service (AWS DMS) replication server",
                "D.  Convert the database schema by using the AWS Schema Conversion Tool (AWS SCT).",
                "E.  Create an Amazon EventBridge (Amazon CloudWatch Events) rule to monitor the database\nsynchronization"
            ],
            "Explanation": "Answer: C D\n",
            "RightAnswer": [
                "C",
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 21,
            "QuestionContent": "A company is developing an application that provides order shipping statistics for retrieval by a REST API.\nThe company wants to extract the shipping statistics, organize the data into an easy-to-read HTML format, and\nsend the report to several email addresses at the same time every morning.\nWhich combination of steps should a solutions architect take to meet these requirements? (Choose two.)",
            "Option": [
                "A.  Configure the application to send the data to Amazon Kinesis Data Firehose.",
                "B.  Use Amazon Simple Email Service (Amazon SES) to format the data and to send the report by email.",
                "C.  Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS\nGlue job to query the application\u0027s API for the data.",
                "D.  Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS\nLambda function to query the application\u0027s API for the data.",
                "E.  Store the application data in Amazon S3. Create an Amazon Simple Notification Service (Amazon SNS)\ntopic as an S3 event destination to send the report by"
            ],
            "Explanation": "Answer: D E\n",
            "RightAnswer": [
                "D",
                "E"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 22,
            "QuestionContent": "A development team runs monthly resource-intensive tests on its general purpose Amazon RDS for MySQL\nDB instance with Performance Insights enabled. The testing lasts for 48 hours once a month and is the only\nprocess that uses the database. The team wants to reduce the cost of running the tests without reducing the\ncompute and memory attributes of the DB instance.\nWhich solution meets these requirements MOST cost-effectively?",
            "Option": [
                "A.  Stop the DB instance when tests are completed. Restart the DB instance when required.",
                "B.  Use an Auto Scaling policy with the DB instance to automatically scale when tests are completed.",
                "C.  Create a snapshot when tests are completed. Terminate the DB instance and restore the snapshot when\nrequired.",
                "D.  Modify the DB instance to a low-capacity instance when tests are completed. Modify the DB instance\nagain when required."
            ],
            "Explanation": "Answer: C\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 23,
            "QuestionContent": "A company is developing an ecommerce application that will consist of a load-balanced front end, a\ncontainer-based application, and a relational database. A solutions architect needs to create a highly available\nsolution that operates with as little manual intervention as possible.\nWhich solutions meet these requirements? (Select TWO.)",
            "Option": [
                "A.  Create an Amazon RDS DB instance in Multi-AZ mode.",
                "B.  Create an Amazon RDS DB instance and one or more replicas in another Availability Zone.",
                "C.  Create an Amazon EC2 in stance-based Docker cluster to handle the dynamic application load.",
                "D.  Create an Amazon Elastic Container Service (Amazon ECS) cluster with a Fargate launch type to handle\nthe dynamic application load.",
                "E.  Create an Amazon Elastic Container Service (Amazon ECS) cluster with an Amazon EC2 launch type to\nhandle the dynamic application load."
            ],
            "Explanation": "Answer: A D\nExplanation\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html\n1. Relational database: RDS\n2. Container-based applications: ECS\n\u0022Amazon ECS enables you to launch and stop your container-based applications by using simple API calls.\nYou can also retrieve the state of your cluster from a centralized service and have access to many familiar\nAmazon EC2 features.\u0022\n3. Little manual intervention: Fargate\nYou can run your tasks and services on a serverless infrastructure that is managed by AWS Fargate.\nAlternatively, for more control over your infrastructure, you can run your tasks and services on a cluster of\nAmazon EC2 instances that you manage.\n",
            "RightAnswer": [
                "A",
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 24,
            "QuestionContent": "A company has a data ingestion workflow that consists the following:\nAn Amazon Simple Notification Service (Amazon SNS) topic for notifications about new data\ndeliveries\nAn AWS Lambda function to process the data and record metadata\nThe company observes that the ingestion workflow fails occasionally because of network connectivity issues.\nWhen such a failure occurs, the Lambda function does not ingest the corresponding data unless the company\nmanually reruns the job.\nWhich combination of actions should a solutions architect take to ensure that the Lambda function ingests all\ndata in the future? (Select TWO.)",
            "Option": [
                "A.  Configure the Lambda function In multiple Availability Zones.",
                "B.  Create an Amazon Simple Queue Service (Amazon SQS) queue, and subscribe It to me SNS topic.",
                "C.  Increase the CPU and memory that are allocated to the Lambda function.",
                "D.  Increase provisioned throughput for the Lambda function.",
                "E.  Modify the Lambda function to read from an Amazon Simple Queue Service (Amazon SQS) queue"
            ],
            "Explanation": "Answer: B E\n",
            "RightAnswer": [
                "B",
                "E"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 25,
            "QuestionContent": "A company\u0027s HTTP application is behind a Network Load Balancer (NLB). The NLB\u0027s target group is\nconfigured to use an Amazon EC2 Auto Scaling group with multiple EC2 instances that run the web service.\nThe company notices that the NLB is not detecting HTTP errors for the application. These errors require a\nmanual restart of the EC2 instances that run the web service. The company needs to improve the application\u0027s\navailability without writing custom scripts or code.\nWhat should a solutions architect do to meet these requirements?",
            "Option": [
                "A.  Enable HTTP health checks on the NLB. supplying the URL of the company\u0027s application.",
                "B.  Add a cron job to the EC2 instances to check the local application\u0027s logs once each minute. If HTTP\nerrors are detected, the application will restart.",
                "C.  Replace the NLB with an Application Load Balancer. Enable HTTP health checks by supplying the\nURL of the company\u0027s application. Configure an Auto Scaling action to replace unhealthy instances.",
                "D.  Create an Amazon Cloud Watch alarm that monitors the UnhealthyHostCount metric for the NLB.\nConfigure an Auto Scaling action to replace unhealthy instances when the alarm is in the ALARM state."
            ],
            "Explanation": "Answer: C\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 26,
            "QuestionContent": "A company needs to move data from an Amazon EC2 instance to an Amazon S3 bucket. The company must\nensure that no API calls and no data are routed through public internet routes. Only the EC2 instance can have\naccess to upload data to the S3 bucket.\nWhich solution will meet these requirements?",
            "Option": [
                "A.  Create an interface VPC endpoint for Amazon S3 in the subnet where the EC2 instance is located.\nAttach a resource policy to the S3 bucket to only allow the EC2 instance\u0027s IAM role for access.",
                "B.  Create a gateway VPC endpoint for Amazon S3 in the Availability Zone where the EC2 instance is\nlocated. Attach appropriate security groups to the endpoint. Attach a resource policy lo the S3 bucket to\nonly allow the EC2 instance\u0027s IAM role for access.",
                "C.  Run the nslookup tool from inside the EC2 instance to obtain the private IP address of the S3 bucket\u0027s\nservice API endpoint. Create a route in the VPC route table to provide the EC2 instance with access to\nthe S3 bucket. Attach a resource policy to the S3 bucket to only allow the EC2 instance\u0027s IAM role for\naccess.",
                "D.  Use the AWS provided, publicly available ip-ranges.json tile to obtain the private IP address of the S3\nbucket\u0027s service API endpoint. Create a route in the VPC route table to provide the EC2 instance with\naccess to the S3 bucket. Attach a resource policy to the S3 bucket to only allow the EC2 instance\u0027s IAM\nrole for access."
            ],
            "Explanation": "Answer: B\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 27,
            "QuestionContent": "A company is preparing to launch a public-facing web application in the AWS Cloud. The architecture\nconsists of Amazon EC2 instances within a VPC behind an Elastic Load Balancer (ELB). A third-party service\nis used for the DNS. The company\u0027s solutions architect must recommend a solution to detect and protect\nagainst large-scale DDoS attacks.\nWhich solution meets these requirements?",
            "Option": [
                "A.  Enable Amazon GuardDuty on the account.",
                "B.  Enable Amazon Inspector on the EC2 instances.",
                "C.  Enable AWS Shield and assign Amazon Route 53 to it.",
                "D.  Enable AWS Shield Advanced and assign the ELB to it."
            ],
            "Explanation": "Answer: D\nExplanation\nhttps://aws.amazon.com/shield/faqs/\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 28,
            "QuestionContent": "A company wants to use the AWS Cloud to make an existing application highly available and resilient. The\ncurrent version of the application resides in the company\u0027s data center. The application recently experienced\ndata loss after a database server crashed because of an unexpected power outage.\nThe company needs a solution that avoids any single points of failure. The solution must give the application\nthe ability to scale to meet user demand.\nWhich solution will meet these requirements?",
            "Option": [
                "A.  Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across\nmultiple Availability Zones. Use an Amazon RDS DB instance in a Multi-AZ configuration.",
                "B.  Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group in a single\nAvailability Zone. Deploy the database\non an EC2 instance. Enable EC2 Auto Recovery.",
                "C.  Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across\nmultiple Availability Zones. Use an Amazon RDS DB instance with a read replica in a single\nAvailability Zone. Promote the read replica to replace the primary DB instance if the primary DB\ninstance fails.",
                "D.  Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across\nmultiple Availability Zones Deploy the primary and secondary database servers on EC2 instances across\nmultiple Availability Zones Use Amazon Elastic Block Store (Amazon EBS) Multi-Attach to create\nshared storage between the instances."
            ],
            "Explanation": "Answer: A\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 29,
            "QuestionContent": "A company is hosting a static website on Amazon S3 and is using Amazon Route 53 for DNS. The website is\nexperiencing increased demand from around the world. The company must decrease latency for users who\naccess the website.\nWhich solution meets these requirements MOST cost-effectively?",
            "Option": [
                "A.  Replicate the S3 bucket that contains the website to all AWS Regions. Add Route 53 geolocation\nrouting entries.",
                "B.  Provision accelerators in AWS Global Accelerator. Associate the supplied IP addresses with the S3\nbucket. Edit the Route 53 entries to point to the IP addresses of the accelerators.",
                "C.  Add an Amazon CloudFront distribution in front of the S3 bucket. Edit the Route 53 entries to point to\nthe CloudFront distribution.",
                "D.  Enable S3 Transfer Acceleration on the bucket. Edit the Route 53 entries to point to the new endpoint."
            ],
            "Explanation": "Answer: C\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 30,
            "QuestionContent": "A company is designing a cloud communications platform that is driven by APIs. The application is hosted on\nAmazon EC2 instances behind a Network Load Balancer (NLB). The company uses Amazon API Gateway to\nprovide external users with access to the application through APIs. The company wants to protect the platform\nagainst web exploits like SQL injection and also wants to detect and mitigate large, sophisticated DDoS\nattacks.\nWhich combination of solutions provides the MOST protection? (Select TWO.)",
            "Option": [
                "A.  Use AWS WAF to protect the NLB.",
                "B.  Use AWS Shield Advanced with the NLB.",
                "C.  Use AWS WAF to protect Amazon API Gateway.",
                "D.  Use Amazon GuardDuty with AWS Shield Standard.",
                "E.  Use AWS Shield Standard with Amazon API Gateway."
            ],
            "Explanation": "Answer: B C\nExplanation\nAWS Shield Advanced provides expanded DDoS attack protection for your Amazon EC2 instances, Elastic\nLoad Balancing load balancers, CloudFront distributions, Route 53 hosted zones, and AWS Global\nAccelerator standard accelerators.\nAWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are\nforwarded to your protected web application resources. You can protect the following resource types:\nAmazon CloudFront distribution\nAmazon API Gateway REST API\nApplication Load Balancer\nAWS AppSync GraphQL API\nAmazon Cognito user pool\nhttps://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html\n",
            "RightAnswer": [
                "B",
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 31,
            "QuestionContent": "A company collects temperature, humidity, and atmospheric pressure data in cities across multiple continents.\nThe average volume of data collected per site each day is 500 GB. Each site has a high-speed internet\nconnection. The company\u0027s weather forecasting applications are based in a single Region and analyze the data\ndaily.\nWhat is the FASTEST way to aggregate data from all of these global sites?",
            "Option": [
                "A.  Enable Amazon S3 Transfer Acceleration on the destination bucket. Use multipart uploads to directly\nupload site data to the destination bucket.",
                "B.  Upload site data to an Amazon S3 bucket in the closest AWS Region. Use S3 cross-Region replication\nto copy objects to the destination bucket.",
                "C.  Schedule AWS Snowball jobs daily to transfer data to the closest AWS Region. Use S3 cross-Region\nreplication to copy objects to the destination bucket.",
                "D.  Upload the data to an Amazon EC2 instance in the closest Region. Store the data in an Amazon Elastic\nBlock Store (Amazon EBS) volume. Once a day take an EBS snapshot and copy it to the centralized\nRegion. Restore the EBS volume in the centralized Region and run an analysis on the data daily."
            ],
            "Explanation": "Answer: A\nExplanation\nYou might want to use Transfer Acceleration on a bucket for various reasons, including the following:\nYou have customers that upload to a centralized bucket from all over the world.\nYou transfer gigabytes to terabytes of data on a regular basis across continents.\nYou are unable to utilize all of your available bandwidth over the Internet when uploading to Amazon S3.\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html\nhttps://aws.amazon.com/s3/transfer-acceleration/#:~:text=S3%20Transfer%20Acceleration%20(S3TA)%20reduces,to%20S3%20for%20remote%20applications:\n\u0022Amazon S3 Transfer Acceleration can speed up content transfers to and from Amazon S3 by as much as\n50-500% for long-distance transfer of larger objects. Customers who have either web or mobile applications\nwith widespread users or applications hosted far away from their S3 bucket can experience long and variable\nupload and download speeds over the Internet\u0022\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html\n\u0022Improved throughput - You can upload parts in parallel to improve throughput.\u0022\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 32,
            "QuestionContent": "A company runs its ecommerce application on AWS. Every new order is published as a message in a\nRabbitMQ queue that runs on an Amazon EC2 instance in a single Availability Zone. These messages are\nprocessed by a different application that runs on a separate EC2 instance. This application stores the details in\na PostgreSQL database on another EC2 instance. All the EC2 instances are in the same Availability Zone.\nThe company needs to redesign its architecture to provide the highest availability with the least operational\noverhead.\nWhat should a solutions architect do to meet these requirements?",
            "Option": [
                "A.  Migrate the queue to a redundant pair (active/standby) of RabbitMQ instances on Amazon MQ. Create a\nMulti-AZ Auto Scaling group (or EC2 instances that host the application. Create another Multi-AZ\nAuto Scaling group for EC2 instances that host the PostgreSQL database.",
                "B.  Migrate the queue to a redundant pair (active/standby) of RabbitMQ instances on Amazon MQ. Create a\nMulti-AZ Auto Scaling group for EC2 instances that host the application. Migrate the database to run on\na Multi-AZ deployment of Amazon RDS for PostgreSQL.",
                "C.  Create a Multi-AZ Auto Scaling group for EC2 instances that host the RabbitMQ queue. Create another\nMulti-AZ Auto Scaling group for EC2 instances that host the application. Migrate the database to run\non a Multi-AZ deployment of Amazon RDS fqjPostgreSQL.",
                "D.  Create a Multi-AZ Auto Scaling group for EC2 instances that host the RabbitMQ queue. Create another\nMulti-AZ Auto Scaling group for EC2 instances that host the application. Create a third Multi-AZ Auto\nScaling group for EC2 instances that host the PostgreSQL database."
            ],
            "Explanation": "Answer: B\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 33,
            "QuestionContent": "An ecommerce company hosts its analytics application in the AWS Cloud. The application generates about\n300 MB of data each month. The data is stored in JSON format. The company is evaluating a disaster recovery\nsolution to back up the data. The data must be accessible in milliseconds if it is needed, and the data must be\nkept for 30 days.\nWhich solution meets these requirements MOST cost-effectively?",
            "Option": [
                "A.  Amazon OpenSearch Service (Amazon Elasticsearch Service)",
                "B.  Amazon S3 Glacier",
                "C.  Amazon S3 Standard",
                "D.  Amazon RDS for PostgreSQL"
            ],
            "Explanation": "Answer: C\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 34,
            "QuestionContent": "A gaming company hosts a browser-based application on AWS. The users of the application consume a large\nnumber of videos and images that are stored in Amazon S3. This content is the same for all users.\nThe application has increased in popularity, and millions of users worldwide are accessing these media files.\nThe company wants to provide the files to the users while reducing the load on the origin.\nWhich solution meets these requirements MOST cost-effectively?",
            "Option": [
                "A.  Deploy an AWS Global Accelerator accelerator in front of the web servers.",
                "B.  Deploy an Amazon CloudFront web distribution in front of the S3 bucket.",
                "C.  Deploy an Amazon ElastiCache for Redis instance in front of the web servers.",
                "D.  Deploy an Amazon ElastiCache for Memcached instance in front of the web servers."
            ],
            "Explanation": "Answer: B\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 35,
            "QuestionContent": "A company\u0027s application integrates with multiple software-as-a-service (SaaS) sources for data collection. The\ncompany runs Amazon EC2 instances to receive the data and to upload the data to an Amazon S3 bucket for\nanalysis. The same EC2 instance that receives and uploads the data also sends a notification to the user when\nan upload is complete. The company has noticed slow application performance and wants to improve the\nperformance as much as possible.\nWhich solution will meet these requirements with the LEAST operational overhead?",
            "Option": [
                "A.  Create an Auto Scaling group so that EC2 instances can scale out. Configure an S3 event notification to\nsend events to an Amazon Simple Notification Service (Amazon SNS) topic when the upload to the S3\nbucket is complete.",
                "B.  Create an Amazon AppFlow flow to transfer data between each SaaS source and the S3 bucket.\nConfigure an S3 event notification to send events to an Amazon Simple Notification Service (Amazon\nSNS) topic when the upload to the S3 bucket is complete.",
                "C.  Create an Amazon EventBridge (Amazon CloudWatch Events) rule for each SaaS source to send output\ndata. Configure the S3 bucket as the rule\u0027s target. Create a second EventBridge (CloudWatch Events)\nrule to send events when the upload to the S3 bucket is complete. Configure an Amazon Simple\nNotification Service (Amazon SNS) topic as the second rule\u0027s target.",
                "D.  Create a Docker container to use instead of an EC2 instance. Host the containerized application on\nAmazon Elastic Container Service (Amazon ECS). Configure Amazon CloudWatch Container Insights\nto send events to an Amazon Simple Notification Service (Amazon SNS) topic when the upload to the\nS3 bucket is complete."
            ],
            "Explanation": "Answer: B\nExplanation\nAmazon AppFlow is a fully managed integration service that enables you to securely transfer data between\nSoftware-as-a-Service (SaaS) applications like Salesforce, SAP, Zendesk, Slack, and ServiceNow, and AWS\nservices like Amazon S3 and Amazon Redshift, in just a few clicks. https://aws.amazon.com/appflow/\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 36,
            "QuestionContent": "A company has thousands of edge devices that collectively generate 1 TB of status alerts each day. Each alert\nis approximately 2 KB in size. A solutions architect needs to implement a solution to ingest and store the alerts\nfor future analysis.\nThe company wants a highly available solution. However, the company needs to minimize costs and does not\nwant to manage additional infrastructure. Ad ditionally, the company wants to keep 14 days of data available\nfor immediate analysis and archive any data older than 14 days.\nWhat is the MOST operationally efficient solution that meets these requirements?",
            "Option": [
                "A.  Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts Configure the Kinesis Data\nFirehose stream to deliver the alerts to an Amazon S3 bucket Set up an S3 Lifecycle configuration to\ntransition data to Amazon S3 Glacier after 14 days",
                "B.  Launch Amazon EC2 instances across two Availability Zones and place them behind an Elastic Load\nBalancer to ingest the alerts Create a script on the EC2 instances that will store tne alerts m an Amazon\nS3 bucket Set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days",
                "C.  Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts Configure the Kinesis Data\nFirehose stream to deliver the alerts to an Amazon Elasticsearch Service (Amazon ES) duster Set up the\nAmazon ES cluster to take manual snapshots every day and delete data from the duster that is older than\n14 days",
                "D.  Create an Amazon Simple Queue Service (Amazon SQS i standard queue to ingest the alerts and set the\nmessage retention period to 14 days Configure consumers to poll the SQS queue check the age of the\nmessage and analyze the message data as needed If the message is 14 days old the consumer should\ncopy the message to an Amazon S3 bucket and delete the message from the SQS queue"
            ],
            "Explanation": "Answer: A\nExplanation\nhttps://aws.amazon.com/kinesis/data-firehose/features/?nc=sn\u0026loc=2#:~:text=into%20Amazon%20S3%2C%20Amazon%20Redshift%2C%20Amazon%20OpenSearch%20Service%2C%20Kinesis,Delivery%20streams\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 37,
            "QuestionContent": "A company is preparing to deploy a new serverless workload. A solutions architect must use the principle of\nleast privilege to configure permissions that will be used to run an AWS Lambda function. An Amazon\nEventBridge (Amazon CloudWatch Events) rule will invoke the function.\nWhich solution meets these requirements?",
            "Option": [
                "A.  Add an execution role to the function with lambda:InvokeFunction as the action and * as the principal.",
                "B.  Add an execution role to the function with lambda:InvokeFunction as the action and\nService:amazonaws.com as the principal.",
                "C.  Add a resource-based policy to the function with lambda:\u0027* as the action and\nService:events.amazonaws.com as the principal.",
                "D.  Add a resource-based policy to the function with lambda:InvokeFunction as the action and\nService:events.amazonaws.com as the principal."
            ],
            "Explanation": "Answer: D\nExplanation\nhttps://docs.aws.amazon.com/eventbridge/latest/userguide/resource-based-policies-eventbridge.html#lambda-permissions\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 38,
            "QuestionContent": "A company runs an Oracle database on premises. As part of the company\u2019s migration to AWS, the company\nwants to upgrade the database to the most recent available version. The company also wants to set up disaster\nrecovery (DR) for the database. The company needs to minimize the operational overhead for normal\noperations and DR setup. The company also needs to maintain access to the database\u0027s underlying operating\nsystem.\nWhich solution will meet these requirements?",
            "Option": [
                "A.  Migrate the Oracle database to an Amazon EC2 instance. Set up database replication to a different AWS\nRegion.",
                "B.  Migrate the Oracle database to Amazon RDS for Oracle. Activate Cross-Region automated backups to\nreplicate the snapshots to another AWS Region.",
                "C.  Migrate the Oracle database to Amazon RDS Custom for Oracle. Create a read replica for the database\nin another AWS Region.",
                "D.  Migrate the Oracle database to Amazon RDS for Oracle. Create a standby database in another\nAvailability Zone."
            ],
            "Explanation": "Answer: C\nExplanation\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-custom.html and\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/working-with-custom-oracle.html\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 39,
            "QuestionContent": "A company observes an increase in Amazon EC2 costs in its most recent bill The billing team notices\nunwanted vertical scaling of instance types for a couple of EC2 instances A solutions architect needs to create\na graph comparing the last 2 months of EC2 costs and perform an in-depth analysis to identify the root cause\nof the vertical scaling\nHow should the solutions architect generate the information with the LEAST operational overhead?",
            "Option": [
                "A.  Use AWS Budgets to create a budget report and compare EC2 costs based on instance types",
                "B.  Use Cost Explorer\u0027s granular filtering feature to perform an in-depth analysis of EC2 costs based on\ninstance types",
                "C.  Use graphs from the AWS Billing and Cost Management dashboard to compare EC2 costs based on\ninstance types for the last 2 months",
                "D.  Use AWS Cost and Usage Reports to create a report and send it to an Amazon S3 bucket Use Amazon\nQuickSight with Amazon S3 as a source to generate an interactive graph based on instance types."
            ],
            "Explanation": "Answer: B\nExplanation\nAWS Cost Explorer is a tool that enables you to view and analyze your costs and usage. You can explore your\nusage and costs using the main graph, the Cost Explorer cost and usage reports, or the Cost Explorer RI\nreports. You can view data for up to the last 12 months, forecast how much you\u0027re likely to spend for the next\n12 months, and get recommendations for what Reserved Instances to purchase. You can use Cost Explorer to\nidentify areas that need further inquiry and see trends that you can use to understand your costs.\nhttps://docs.aws.amazon.com/cost-management/latest/userguide/ce-what-is.html\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 40,
            "QuestionContent": "A company is running a multi-tier web application on premises. The web application is containerized and runs\non a number of Linux hosts connected to a PostgreSQL database that contains user records. The operational\noverhead of maintaining the infrastructure and capacity planning is limiting the company\u0027s growth. A solutions\narchitect must improve the application\u0027s infrastructure.\nWhich combination of actions should the solutions architect take to accomplish this? (Choose two.)",
            "Option": [
                "A.  Migrate the PostgreSQL database to Amazon Aurora",
                "B.  Migrate the web application to be hosted on Amazon EC2 instances.",
                "C.  Set up an Amazon CloudFront distribution for the web application content.",
                "D.  Set up Amazon ElastiCache between the web application and the PostgreSQL database.",
                "E.  Migrate the web application to be hosted on AWS Fargate with Amazon Elastic Container Service\n(Amazon ECS)."
            ],
            "Explanation": "Answer: A E\n",
            "RightAnswer": [
                "A",
                "E"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 41,
            "QuestionContent": "A company is preparing to store confidential data in Amazon S3 For compliance reasons the data must be\nencrypted at rest Encryption key usage must be logged tor auditing purposes. Keys must be rotated every year.\nWhich solution meets these requirements and \u00ABthe MOST operationally efferent?",
            "Option": [
                "A.  Server-side encryption with customer-provided keys (SSE-C)",
                "B.  Server-side encryption with Amazon S3 managed keys (SSE-S3)",
                "C.  Server-side encryption with AWS KMS (SSE-KMS) customer master keys (CMKs) with manual\nrotation",
                "D.  Server-side encryption with AWS KMS (SSE-KMS) customer master keys (CMKs) with automate\nrotation"
            ],
            "Explanation": "Answer: D\nExplanation\nhttps://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html\nWhen you enable automatic key rotation for a customer managed key, AWS KMS generates new\ncryptographic material for the KMS key every year. AWS KMS also saves the KMS key\u0027s older cryptographic\nmaterial in perpetuity so it can be used to decrypt data that the KMS key encrypted.\nKey rotation in AWS KMS is a cryptographic best practice that is designed to be transparent and easy to use.\nAWS KMS supports optional automatic key rotation only for customer managed CMKs. Enable and disable\nkey rotation. Automatic key rotation is disabled by default on customer managed CMKs. When you enable (or\nre-enable) key rotation, AWS KMS automatically rotates the CMK 365 days after the enable date and every\n365 days thereafter.\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 42,
            "QuestionContent": "A development team needs to host a website that will be accessed by other teams. The website contents consist\nof HTML, CSS, client-side JavaScript, and images Which method is the MOST cost-effective for hosting the\nwebsite?",
            "Option": [
                "A.  Containerize the website and host it in AWS Fargate.",
                "B.  Create an Amazon S3 bucket and host the website there",
                "C.  Deploy a web server on an Amazon EC2 instance to host the website.",
                "D.  Configure an Application Loa d Balancer with an AWS Lambda target that uses the Express js\nframework."
            ],
            "Explanation": "Answer: B\nExplanation\nIn Static Websites, Web pages are returned by the server which are prebuilt.\nThey use simple languages such as HTML, CSS, or JavaScript.\nThere is no processing of content on the server (according to the user) in Static Websites. Web pages are\nreturned by the server with no change therefore, static Websites are fast.\nThere is no interaction with databases.\nAlso, they are less costly as the host does not need to support server-side processing with different languages.\n============\nIn Dynamic Websites, Web pages are returned by the server which are processed during runtime means they\nare not prebuilt web pages but they are built during runtime according to the user\u2019s demand.\nThese use server-side scripting languages such as PHP, Node.js, ASP.NET and many more supported by the\nserver.\nSo, they are slower than static websites but updates and interaction with databases are possible.\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 43,
            "QuestionContent": "A company has a mulli-tier application that runs six front-end web servers in an Amazon EC2 Auto Scaling\ngroup in a single Availability Zone behind an Application Load Balancer (ALB). A solutions architect needs\nlo modify the infrastructure to be highly available without modifying the application.\nWhich architecture should the solutions architect choose that provides high availability?",
            "Option": [
                "A.  Create an Auto Scaling group that uses three Instances across each of tv/o Regions.",
                "B.  Modify the Auto Scaling group to use three instances across each of two Availability Zones.",
                "C.  Create an Auto Scaling template that can be used to quickly create more instances in another Region.",
                "D.  Change the ALB in front of the Amazon EC2 instances in a round-robin configuration to balance traffic\nto the web tier."
            ],
            "Explanation": "Answer: B\nExplanation\nHigh availability can be enabled for this architecture quite simply by modifying the existing Auto Scaling\ngroup to use multiple availability zones. The ASG will automatically balance the load so you don\u0027t actually\nneed to specify the instances per AZ.\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 44,
            "QuestionContent": "A company wants to migrate an on-premises data center to AWS. The data canter hosts an SFTP server that\nstores its data on an NFS-based file system. The server holds 200 GB of data that needs to be transferred. The\nserver must be hosted on an Amazon EC2 instance that uses an Amazon Elastic File System (Amazon EFS)\nfile system\nWhen combination of steps should a solutions architect take to automate this task? (Select TWO )",
            "Option": [
                "A.  Launch the EC2 instance into the same Avalability Zone as the EFS fie system",
                "B.  install an AWS DataSync agent m the on-premises data center",
                "C.  Create a secondary Amazon Elastic Block Store (Amazon EBS) volume on the EC2 instance tor the data",
                "D.  Manually use an operating system copy command to push the data to the EC2 instance",
                "E.  Use AWS DataSync to create a suitable location configuration for the onprermises SFTP server"
            ],
            "Explanation": "Answer: A B\n",
            "RightAnswer": [
                "A",
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 45,
            "QuestionContent": "A company hosts an application on AWS Lambda functions mat are invoked by an Amazon API Gateway API\nThe Lambda functions save customer data to an Amazon Aurora MySQL database Whenever the company\nupgrades the database, the Lambda functions fail to establish database connections until the upgrade is\ncomplete The result is that customer data Is not recorded for some of the event\nA solutions architect needs to design a solution that stores customer data that is created during database\nupgrades\nWhich solution will meet these requirements?",
            "Option": [
                "A.  Provision an Amazon RDS proxy to sit between the Lambda functions and the database Configure the\nLambda functions to connect to the RDS proxy",
                "B.  Increase the run time of me Lambda functions to the maximum Create a retry mechanism in the code\nthat stores the customer data in the database",
                "C.  Persist the customer data to Lambda local storage. Configure new Lambda functions to scan the local\nstorage to save the customer data to the database.",
                "D.  Store the customer data m an Amazon Simple Queue Service (Amazon SOS) FIFO queue Create a new\nLambda function that polls the queue and stores the customer data in the database"
            ],
            "Explanation": "Answer: D\nExplanation\nhttps://www.learnaws.org/2020/12/13/aws-rds-proxy-deep-dive/\nRDS proxy can improve application availability in such a situation by waiting for the new database instance to\nbe functional and maintaining any requests received from the application during this time. The end result is\nthat the application is more resilient to issues with the underlying database.\nThis will enable solution to hold data till the time DB comes back to normal. RDS proxy is to optimally utilize\nthe connection between Lambda and DB. Lambda can open multiple connection concurrently which can be\ntaxing on DB compute resources, hence RDS proxy was introduced to manage and leverage these connections\nefficiently.\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 46,
            "QuestionContent": "A company wants to run a gaming application on Amazon EC2 instances that are part of an Auto Scaling\ngroup in the AWS Cloud. The application will transmit data by using UDP packets. The company wants to\nensure that the application can scale out and in as traffic increases and decreases.\nWhat should a solutions architect do to meet these requirements?",
            "Option": [
                "A.  Attach a Network Load Balancer to the Auto Scaling group",
                "B.  Attach an Application Load Balancer to the Auto Scaling group.",
                "C.  Deploy an Amazon Route 53 record set with a weighted policy to route traffic appropriately",
                "D.  Deploy a NAT instance that is configured with port forwarding to the EC2 instances in the Auto Scaling\ngroup."
            ],
            "Explanation": "Answer: B\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 47,
            "QuestionContent": "A solutions architect is using Amazon S3 to design the storage architecture of a new digital media application.\nThe media files must be resilient to the loss of an Availability Zone Some files are accessed frequently while\nother files are rarely accessed in an unpredictable pattern. The solutions architect must minimize the costs of\nstoring and retrieving the media files.\nWhich storage option meets these requirements?",
            "Option": [
                "A.  S3 Standard",
                "B.  S3 Intelligent-Tiering",
                "C.  S3 Standard-Infrequent Access {S3 Standard-IA)",
                "D.  S3 One Zone-Infrequent Access (S3 One Zone-IA)"
            ],
            "Explanation": "Answer: B\nExplanation\nS3 Intelligent-Tiering - Perfect use case when you don\u0027t know the frequency of access or irregular patterns of\nusage.\nAmazon S3 offers a range of storage classes designed for different use cases. These include S3 Standard for\ngeneral-purpose storage of frequently accessed data; S3 Intelligent-Tiering for data with unknown or changing\naccess patterns; S3 Standard-Infrequent Access (S3 Standard-IA) and S3 One Zone-Infrequent Access (S3 One\nZone-IA) for long-lived, but less frequently accessed data; and Amazon S3 Glacier (S3 Glacier) and Amazon\nS3 Glacier Deep Archive (S3 Glacier Deep Archive) for long-term archive and digital preservation. If you\nhave data residency requirements that can\u2019t be met by an existing AWS Region, you can use the S3 Outposts\nstorage class to store your S3 data on-premises. Amazon S3 also offers capabilities to manage your data\nthroughout its lifecycle. Once an S3 Lifecycle policy is set, your data will automatically transfer to a different\nstorage class without any changes to your application.\nhttps://aws.amazon.com/getting-started/hands-on/getting-started-using-amazon-s3-intelligent-tiering/?nc1=h_ls\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 48,
            "QuestionContent": "An application runs on an Amazon EC2 instance in a VPC. The application processes logs that are stored in an\nAmazon S3 bucket. The EC2 instance needs to access the S3 bucket without connectivity to the internet.\nWhich solution will provide private network connectivity to Amazon S3?",
            "Option": [
                "A.  Create a gateway VPC endpoint to the S3 bucket.",
                "B.  Stream the logs to Amazon CloudWatch Logs. Export the logs to the S3 bucket.",
                "C.  Create an instance profile on Amazon EC2 to allow S3 access.",
                "D.  Create an Amazon API Gateway API with a private link to access the S3 endpoint."
            ],
            "Explanation": "Answer: A\nExplanation\nVPC endpoint allows you to connect to AWS services using a private network instead of using the public\nInternet\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 49,
            "QuestionContent": "A company is building a web-based application running on Amazon EC2 instances in multiple Availability\nZones. The web application will provide access to a repository of text documents totaling about 900 TB in\nsize. The company anticipates that the web application will experience periods of high demand. A solutions\narchitect must ensure that the storage component for the text documents can scale to meet the demand of the\napplication at all times. The company is concerned about the overall cost of the solution.\nWhich storage solution meets these requirements MOST cost-effectively?",
            "Option": [
                "A.  Amazon Elastic Block Store (Amazon EBS)",
                "B.  Amazon Elastic File System (Amazon EFS)",
                "C.  Amazon Elasticsearch Service (Amazon ES)",
                "D.  Amazon S3"
            ],
            "Explanation": "Answer: D\nExplanation\nAmazon S3 is cheapest and can be accessed from anywhere.\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 50,
            "QuestionContent": "A company is developing a file-sharing application that will use an Amazon S3 bucket for storage. The\ncompany wants to serve all the files through an Amazon CloudFront distribution. The company does not want\nthe files to be accessible through direct navigation to the S3 URL.\nWhat should a solutions architect do to meet these requirements?",
            "Option": [
                "A.  Write individual policies for each S3 bucket to grant read permission for only CloudFront access.",
                "B.  Create an IAM user. Grant the user read permission to objects in the S3 bucket. Assign the user to\nCloudFront.",
                "C.  Write an S3 bucket policy that assigns the CloudFront distribution ID as the Principal and assigns the\ntarget S3 bucket as the Amazon Resource Name (ARN).",
                "D.  Create an origin access identity (OAI). Assign the OAI to the CloudFront distribution. Configure the S3\nbucket permissions so that only the OAI has read permission."
            ],
            "Explanation": "Answer: D\nExplanation\nhttps://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-access-to-amazon-s3/\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html#private-content-restricting-access-to-s3-overview\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 51,
            "QuestionContent": "A company wants to improve its ability to clone large amounts of production data into a test environment in\nthe same AWS Region. The data is stored in Amazon EC2 instances on Amazon Elastic Block Store (Amazon\nEBS) volumes. Modifications to the cloned data must not affect the production environment. The software that\naccesses this data requires consistently high I/O performance.\nA solutions architect needs to minimize the time that is required to clone the production data into the test\nenvironment.\nWhich solution will meet these requirements?",
            "Option": [
                "A.  Take EBS snapshots of the production EBS volumes. Restore the snapshots onto EC2 instance store\nvolumes in the test environment.",
                "B.  Configure the production EBS volumes to use the EBS Multi-Attach feature. Take EBS snapshots of the\nproduction EBS volumes. Attach the production EBS volumes to the EC2 instances in the test\nenvironment.",
                "C.  Take EBS snapshots of the production EBS volumes. Create and initialize new EBS volumes. Attach the\nnew EBS volumes to EC2 instances in the test environment before restoring the volumes from the\nproduction EBS snapshots.",
                "D.  Take EBS snapshots of the production EBS volumes. Turn on the EBS fast snapshot restore feature on\nthe EBS snapshots. Restore the snapshots into new EBS volumes. Attach the new EBS volumes to EC2\ninstances in the test environment."
            ],
            "Explanation": "Answer: D\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 52,
            "QuestionContent": "A company has two applications: a sender application that sends messages with payloads to be processed and a\nprocessing application intended to receive the messages with payloads. The company wants to implement an\nAWS service to handle messages between the two applications. The sender application can send about 1.000\nmessages each hour. The messages may take up to 2 days to be processed. If the messages fail to process, they\nmust be retained so that they do not impact the processing of any remaining messages.\nWhich solution meets these requirements and is the MOST operationally efficient?",
            "Option": [
                "A.  Set up an Amazon EC2 instance running a Redis database. Configure both applications to use the\ninstance. Store, process, and delete the messages, respectively.",
                "B.  Use an Amazon Kinesis data stream to receive the messages from the sender application. Integrate the\nprocessing application with the Kinesis Client Library (KCL).",
                "C.  Integrate the sender and processor applications with an Amazon Simple Queue Service (Amazon SQS)\nqueue. Configure a dead-letter queue to collect the messages that failed to process.",
                "D.  Subscribe the processing application to an Amazon Simple Notification Service (Amazon SNS) topic to\nreceive notifications to process. Integrate the sender application to write to the SNS topic."
            ],
            "Explanation": "Answer: C\nExplanation\nhttps://aws.amazon.com/blogs/compute/building-loosely-coupled-scalable-c-applications-with-amazon-sqs-and-amazon-sns/\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 53,
            "QuestionContent": "A reporting team receives files each day in an Amazon S3 bucket. The reporting team manually reviews and\ncopies the files from this initial S3 bucket to an analysis S3 bucket each day at the same time to use with\nAmazon QuickSight. Additional teams are starting to send more files in larger sizes to the initial S3 bucket.\nThe reporting team wants to move the files automatically analysis S3 bucket as the files enter the initial S3\nbucket. The reporting team also wants to use AWS Lambda functions to run pattern-matching code on the\ncopied data. In addition, the reporting team wants to send the data files to a pipeline in Amazon SageMaker\nPipelines.\nWhat should a solutions architect do to meet these requirements with the LEAST operational overhead?",
            "Option": [
                "A.  Create a Lambda function to copy the files to the analysis S3 bucket. Create an S3 event notification for\nthe analysis S3 bucket. Configure Lambda and SageMaker Pipelines as destinations of the event\nnotification. Configure s30bjectCreated:Put as the event type.",
                "B.  Create a Lambda function to copy the files to the analysis S3 bucket. Configure the analysis S3 bucket\nto send event notifications to Amazon EventBridge (Amazon CloudWatch Events). Configure an\nObjectCreated rule in EventBridge (CloudWatch Events). Configure Lambda and SageMaker Pipelines\nas targets for the rule.",
                "C.  Configure S3 replication between the S3 buckets. Create an S3 event notification for the analysis S3\nbucket. Configure Lambda and SageMaker Pipelines as destinations of the event notification. Configure\ns30bjectCreated:Put as the event type.",
                "D.  Configure S3 replication between the S3 buckets. Configure the analysis S3 bucket to send event\nnotifications to Amazon EventBridge (Amazon CloudWatch Events). Configure an ObjectCreated rule\nin EventBridge (CloudWatch Events). Configure Lambda and SageMaker Pipelines as targets for the\nrule."
            ],
            "Explanation": "Answer: A\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 54,
            "QuestionContent": "A company needs to retain application logs files for a critical application for 10 years. The application team\nregularly accesses logs from the past month for troubleshooting, but logs older than 1 month are rarely\naccessed. The application generates more than 10 TB of logs per month.\nWhich storage option meets these requirements MOST cost-effectively?",
            "Option": [
                "A.  Store the Iogs in Amazon S3 Use AWS Backup lo move logs more than 1 month old to S3 Glacier Deep\nArchive",
                "B.  Store the logs in Amazon S3 Use S3 Lifecycle policies to move logs more than 1 month old to S3\nGlacier Deep Archive",
                "C.  Store the logs in Amazon CloudWatch Logs Use AWS Backup to move logs more then 1 month old to\nS3 Glacier Deep Archive",
                "D.  Store the logs in Amazon CloudWatch Logs Use Amazon S3 Lifecycle policies to move logs more than\n1 month old to S3 Glacier Deep Archive"
            ],
            "Explanation": "Answer: B\nExplanation\nYou need S3 to be able to archive the logs after one month. Cannot do that with CloudWatch Logs.\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 55,
            "QuestionContent": "A company has a highly dynamic batch processing job that uses many Amazon EC2 instances to complete it.\nThe job is stateless in nature, can be started and stopped at any given time with no negative impact, and\ntypically takes upwards of 60 minutes total to complete. The company has asked a solutions architect to design\na scalable and cost-effective solution that meets the requirements of the job.\nWhat should the solutions architect recommend?",
            "Option": [
                "A.  Implement EC2 Spot Instances",
                "B.  Purchase EC2 Reserved Instances",
                "C.  Implement EC2 On-Demand Instances",
                "D.  Implement the processing on AWS Lambda"
            ],
            "Explanation": "Answer: A\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 56,
            "QuestionContent": "A company has a Microsoft NET application that runs on an on-premises Windows Server Trie application\nstores data by using an Oracle Database Standard Edition server The company is planning a migration to AWS\nand wants to minimize development changes while moving the application The AWS application environment\nshould be highly available\nWhich combination ol actions should the company take to meet these requirements? (Select TWO )",
            "Option": [
                "A.  Refactor the application as serverless with AWS Lambda functions running NET Cote",
                "B.  Rehost the application in AWS Elastic Beanstalk with the NET platform in a Mulft-AZ deploymeni",
                "C.  Replatform the application to run on Amazon EC2 with the Amazon Linux Amazon Machine Image\n(AMI)",
                "D.  Use AWS Database Migration Service (AWS DMS) to migrate trom the Oracle database to Amazon\nDynamoDB in a Multi-AZ deployment",
                "E.  Use AWS Database Migration Service (AWS DMS) to migrate from the Oracle database to Oracle on\nAmazon RDS in a Multi-AZ deployment"
            ],
            "Explanation": "Answer: B E\n",
            "RightAnswer": [
                "B",
                "E"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 57,
            "QuestionContent": "A company is running a batch applicalton on Amazon EC2 instances. The application consists of a backend\nwith multiple Amazon RDS databases. The application is causing a htgh number of leads on the databases. A\nsolutions architect must reduce the number of database reads while ensuring high availability.\nWhat should the solutions architect do to meet this requirement?",
            "Option": [
                "A.  Add Amazon RDS read replicas",
                "B.  Use Amazon ElasbCache for Redls",
                "C.  Use Amazon Route 53 DNS caching",
                "D.  Use Amazon ElastiCache for Memcached"
            ],
            "Explanation": "Answer: A\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 58,
            "QuestionContent": "A company is running several business applications in three separate VPCs within me us-east-1 Region. The\napplications must be able to communicate between VPCs. The applications also must be able to consistently\nsend hundreds to gigabytes of data each day to a latency-sensitive application that runs in a single on-premises\ndata center.\nA solutions architect needs to design a network connectivity solution that maximizes cost-effectiveness\nWhich solution moots those requirements?",
            "Option": [
                "A.  Configure three AWS Site-to-Site VPN connections from the data center to AWS Establish connectivity\nby configuring one VPN connection for each VPC",
                "B.  Launch a third-party virtual network appliance in each VPC Establish an iPsec VPN tunnel between the\nData center and each virtual appliance",
                "C.  Set up three AWS Direct Connect connections from the data center to a Direct Connect gateway in\nus-east-1 Establish connectivity by configuring each VPC to use one of the Direct Connect connections",
                "D.  Set up one AWS Direct Connect connection from the data center to AWS. Create a transit gateway, and\nattach each VPC to the transit gateway. Establish connectivity between the Direct Connect connection\nand the transit gateway."
            ],
            "Explanation": "Answer: D\nExplanation\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-aws-transit-gateway.html\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 59,
            "QuestionContent": "A company has an Amazon S3 bucket that contains critical data. The company must protect the data from\naccidental deletion.\nWhich combination of steps should a solutions architect take to meet these requirements? (Choose two.)",
            "Option": [
                "A.  Enable versioning on the S3 bucket.",
                "B.  Enable MFA Delete on the S3 bucket.",
                "C.  Create a bucket policy on the S3 bucket.",
                "D.  Enable default encryption on the S3 bucket.",
                "E.  Create a lifecycle policy for the objects in the S3 bucket."
            ],
            "Explanation": "Answer: A B\n",
            "RightAnswer": [
                "A",
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 60,
            "QuestionContent": "A company wants to move a multi-tiered application from on premises to the AWS Cloud to improve the\napplication\u0027s performance. The application consists of application tiers that communicate with each other by\nway of RESTful services. Transactions are dropped when one tier becomes overloaded. A solutions architect\nmust design a solution that resolves these issues and modernizes the application.\nWhich solution meets these requirements and is the MOST operationally efficient?",
            "Option": [
                "A.  Use Amazon API Gateway and direct transactions to the AWS Lambda functions as the application\nlayer. Use Amazon Simple Queue Service (Amazon SQS) as the communication layer between\napplication services.",
                "B.  Use Amazon CloudWatch metrics to analyze the application performance history to determine the\nserver\u0027s peak utilization during the performance failures. Increase the size of the application server\u0027s\nAmazon EC2 instances to meet the peak requirements.",
                "C.  Use Amazon Simple Notification Service (Amazon SNS) to handle the messaging between application\nservers running on Amazon EC2 in an Auto Scaling group. Use Amazon CloudWatch to monitor the\nSNS queue length and scale up and down as required.",
                "D.  Use Amazon Simple Queue Service (Amazon SQS) to handle the messaging between application\nservers running on Amazon EC2 in an Auto Scaling group. Use Amazon CloudWatch to monitor the\nSQS queue length and scale up when communication failures are detected."
            ],
            "Explanation": "Answer: A\nExplanation\nhttps://aws.amazon.com/getting-started/hands-on/build-serverless-web-app-lambda-apigateway-s3-dynamodb-cognito/module-4/\nBuild a Serverless Web Application with AWS Lambda, Amazon API Gateway, AWS Amplify, Amazon\nDynamoDB, and Amazon Cognito. This example showed similar setup as question: Build a Serverless Web\nApplication with AWS Lambda, Amazon API Gateway, AWS Amplify, Amazon DynamoDB, and Amazon\nCognito\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 61,
            "QuestionContent": "A company s order system sends requests from clients to Amazon EC2 instances The EC2 instances process\nttie orders and men store the orders in a database on Amazon RDS Users report that they must reprocess orders\nwhen the system fails. The company wants a resilient solution that can process orders automatically it a system\noutage occurs.\nWhat shoukl a solutions architect do to meet these requirements?",
            "Option": [
                "A.  Move (he EC2 Instances into an Auto Scaling group Create an Amazon EventBhdge (Amazon\nCloudWatch Events) rule to target an Amazon Elastic Container Service (Amazon ECS) task",
                "B.  Move the EC2 instances into an Auto Scaling group behind an Application Load Balancer (ALB)\nUpdate the order system to send messages to the ALB endpoint.",
                "C.  Move the EC2 instances into an Auto Scaling group Configure the order system to send messages to an\nAmazon Simple Queue Service (Amazon SQS) queue Configure the EC2 instances to consume\nmessages from the queue",
                "D.  Create an Amazon Simple Notification Service (Amazon SNS) topic Create an AWS Lambda function,\nand subscribe the function to the SNS topic Configure the order system to send messages to the SNS\ntopic Send a command to the EC2 instances to process the messages by using AWS Systems Manager\nRun Command"
            ],
            "Explanation": "Answer: C\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 62,
            "QuestionContent": "An Amazon EC2 administrator created the following policy associated with an IAM group containing several\nusers\nWhat is the effect of this policy?",
            "Option": [
                "A.  Users can terminate an EC2 instance in any AWS Region except us-east-1.",
                "B.  Users can terminate an EC2 instance with the IP address 10 100 100 1 in the us-east-1 Region",
                "C.  Users can terminate an EC2 instance in the us-east-1 Region when the user\u0027s source IP is\n10.100.100.254.",
                "D.  Users cannot terminate an EC2 instance in the us-east-1 Region when the user\u0027s source IP is 10.100 100\n254"
            ],
            "Explanation": "Answer: C\nExplanation\nas the policy prevents anyone from doing any EC2 action on any region except us-east-1 and allows only users\nwith source ip 10.100.100.0/24 to terminate instances. So user with source ip 10.100.100.254 can terminate\ninstances in us-east-1 region.\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 63,
            "QuestionContent": "A company needs guaranteed Amazon EC2 capacity in three specific Availability Zones in a specific AWS\nRegion for an upcoming event that will last 1 week.\nWhat should the company do to guarantee the EC2 capacity?",
            "Option": [
                "A.  Purchase Reserved instances that specify the Region needed",
                "B.  Create an On Demand Capacity Reservation that specifies the Region needed",
                "C.  Purchase Reserved instances that specify the Region and three Availability Zones needed",
                "D.  Create an On-Demand Capacity Reservation that specifies the Region and three Availability Zones\nneeded"
            ],
            "Explanation": "Answer: D\nExplanation\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-capacity-reservations.html\nReserve instances: You will have to pay for the whole term (1 year or 3years) which is not cost effective\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 64,
            "QuestionContent": "A company wants to reduce the cost of its existing three-tier web architecture. The web, application, and\ndatabase servers are running on Amazon EC2 instances for the development, test, and production\nenvironments. The EC2 instances average 30% CPU utilization during peak hours and 10% CPU utilization\nduring non-peak hours.\nThe production EC2 instances run 24 hours a day. The development and test EC2 instances run for at least 8\nhours each day. The company plans to implement automation to stop the development and test EC2 instances\nwhen they are not in use.\nWhich EC2 instance purchasing solution will meet the company\u0027s requirements MOST cost-effectively?",
            "Option": [
                "A.  Use Spot Instances for the production EC2 instances. Use Reserved Instances for the development and\ntest EC2 instances.",
                "B.  Use Reserved Instances for the production EC2 instances. Use On-Demand Instances for the\ndevelopment and test EC2 instances.",
                "C.  Use Spot blocks for the production EC2 instances. Use Reserved Instances for the development and test\nEC2 instances.",
                "D.  Use On-Demand Instances for the production EC2 instances. Use Spot blocks for the development and\ntest EC2 instances."
            ],
            "Explanation": "Answer: B\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 65,
            "QuestionContent": "A company wants to migrate its on-premises data center to AWS. According to the company\u0027s compliance\nrequirements, the company can use only the ap-northeast-3 Region. Company administrators are not permitted\nto connect VPCs to the internet.\nWhich solutions will meet these requirements? (Choose two.)",
            "Option": [
                "A.  Use AWS Control Tower to implement data residency guardrails to deny internet access and deny\naccess to all AWS Regions except ap-northeast-3.",
                "B.  Use rules in AWS WAF to prevent internet access. Deny access to all AWS Regions except\nap-northeast-3 in the AWS account settings.",
                "C.  Use AWS Organizations to configure service control policies (SCPS) that prevent VPCs from gaining\ninternet access. Deny access to all AWS Regions except ap-northeast-3.",
                "D.  Create an outbound rule for the network ACL in each VPC to deny all traffic from 0.0.0.0/0. Create an\nIAM policy for each user to prevent the use of any AWS Region other than ap-northeast-3.",
                "E.  Use AWS Config to activate managed rules to detect and alert for internet gateways and to detect and\nalert for new resources deployed outside of ap-northeast-3."
            ],
            "Explanation": "Answer: A C\n",
            "RightAnswer": [
                "A",
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 66,
            "QuestionContent": "A company has created an image analysis application in which users can upload photos and add photo frames\nto their images. The users upload images and metadata to indicate which photo frames they want to add to\ntheir images. The application uses a single Amazon EC2 instance and Amazon DynamoDB to store the\nmetadata.\nThe application is becoming more popular, and the number of users is increasing. The company expects the\nnumber of concurrent users to vary significantly depending on the time of day and day of week. The company\nmust ensure that the application can scale to meet the needs of the growing user base.\nWhich solution meats these requirements?",
            "Option": [
                "A.  Use AWS Lambda to process the photos. Store the photos and metadata in DynamoDB.",
                "B.  Use Amazon Kinesis Data Firehose to process the photos and to store the photos and metadata.",
                "C.  Use AWS Lambda to process the photos. Store the photos in Amazon S3. Retain DynamoDB to store\nthe metadata.",
                "D.  Increase the number of EC2 instances to three. Use Provisioned IOPS SSD (io2) Amazon Elastic Block\nStore (Amazon EBS) volumes to store the photos and metadata."
            ],
            "Explanation": "Answer: C\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 67,
            "QuestionContent": "A company provides an API to its users that automates inquiries for tax computations based on item prices.\nThe company experiences a larger number of inquiries during the holiday season only that cause slower\nresponse times. A solutions architect needs to design a solution that is scalable and elastic.\nWhat should the solutions architect do to accomplish this?",
            "Option": [
                "A.  Provide an API hosted on an Amazon EC2 instance. The EC2 instance performs the required\ncomputations when the API request is made.",
                "B.  Design a REST API using Amazon API Gateway that accepts the item names. API Gateway passes item\nnames to AWS Lambda for tax computations.",
                "C.  Create an Application Load Balancer that has two Amazon EC2 instances behind it. The EC2 instances\nwill compute the tax on the received item names.",
                "D.  Design a REST API using Amazon API Gateway that connects with an API hosted on an Amazon EC2\ninstance. API Gateway accepts and passes the item names to the EC2 instance for tax computations."
            ],
            "Explanation": "Answer: B\nExplanation\nLambda server-less is scalable and elastic than EC2 api gateway solution\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 68,
            "QuestionContent": "A company has a Windows-based application that must be migrated to AWS. The application requires the use\nof a shared Windows file system attached to multiple Amazon EC2 Windows instances that are deployed\nacross multiple Availability Zones.\nWhat should a solutions architect do to meet this requirement?",
            "Option": [
                "A.  Configure AWS Storage Gateway in volume gateway mode. Mount the volume to each Windows\ninstance.",
                "B.  Configure Amazon FSx for Windows File Server. Mount the Amazon FSx file system to each Windows\ninstance.",
                "C.  Configure a file system by using Amazon Elastic File System (Amazon EFS). Mount the EFS file\nsystem to each Windows instance.",
                "D.  Configure an Amazon Elastic Block Store (Amazon EBS) volume with the required size. Attach each\nEC2 instance to the volume. Mount the file system within the volume to each Windows instance."
            ],
            "Explanation": "Answer: B\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 69,
            "QuestionContent": "A company wants to migrate its on-premises application to AWS. The application produces output files that\nvary in size from tens of gigabytes to hundreds of terabytes The application data must be stored in a standard\nfile system structure The company wants a solution that scales automatically, is highly available, and requires\nminimum operational overhead.\nWhich solution will meet these requirements?",
            "Option": [
                "A.  Migrate the application to run as containers on Amazon Elastic Container Service (Amazon ECS) Use\nAmazon S3 for storage",
                "B.  Migrate the application to run as containers on Amazon Elastic Kubernetes Service (Amazon EKS) Use\nAmazon Elastic Block Store (Amazon EBS) for storage",
                "C.  Migrate the application to Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon\nElastic File System (Amazon EFS) for storage.",
                "D.  Migrate the application to Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon\nElastic Block Store (Amazon EBS) for storage."
            ],
            "Explanation": "Answer: C\nExplanation\nEFS is a standard file system, it scales automatically and is highly available.\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 70,
            "QuestionContent": "A company runs a photo processing application that needs to frequently upload and download pictures from\nAmazon S3 buckets that are located in the same AWS Region. A solutions architect has noticed an increased\ncost in data transfer fees and needs to implement a solution to reduce these costs.\nHow can the solutions architect meet this requirement?",
            "Option": [
                "A.  Deploy Amazon API Gateway into a public subnet and adjust the route table to route S3 calls through It.",
                "B.  Deploy a NAT gateway into a public subnet and attach an end point policy that allows access to the S3\nbuckets.",
                "C.  Deploy the application Into a public subnet and allow it to route through an internet gateway to access\nthe S3 Buckets",
                "D.  Deploy an S3 VPC gateway endpoint into the VPC and attach an endpoint policy that allows access to\nthe S3 buckets."
            ],
            "Explanation": "Answer: D\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 71,
            "QuestionContent": "A company runs a shopping application that uses Amazon DynamoDB to store customer information. In case\nof data corruption, a solutions architect needs to design a solution that meets a recovery point objective (RPO)\nof 15 minutes and a recovery time objective (RTO) of 1 hour.\nWhat should the solutions architect recommend to meet these requirements?",
            "Option": [
                "A.  Configure DynamoDB global tables. For RPO recovery, point the application to a different AWS\nRegion.",
                "B.  Configure DynamoDB point-in-time recovery. For RPO recovery, restore to the desired point in time.",
                "C.  Export the DynamoDB data to Amazon S3 Glacier on a daily basis. For RPO recovery, import the data\nfrom S3 Glacier to DynamoDB.",
                "D.  Schedule Amazon Elastic Block Store (Amazon EBS) snapshots for the DynamoDB table every 15\nminutes. For RPO recovery, restore the DynamoDB table by using the EBS snapshot."
            ],
            "Explanation": "Answer: B\nExplanation\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/PointInTimeRecovery.html\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 72,
            "QuestionContent": "A company produces batch data that comes from different databases. The company also produces live stream\ndata from network sensors and application APIs. The company needs to consolidate all the data into one place\nfor business analytics. The company needs to process the incoming data and then stage the data in different\nAmazon S3 buckets. Teams will later run one-time queries and import the data into a business intelligence tool\nto show key performance indicators (KPIs).\nWhich combination of steps will meet these requirements with the LEAST operational overhead? (Choose\ntwo.)",
            "Option": [
                "A.  Use Amazon Athena foe one-time queries Use Amazon QuickSight to create dashboards for KPIs",
                "B.  Use Amazon Kinesis Data Analytics for one-time queries Use Amazon QuickSight to create dashboards\nfor KPIs",
                "C.  Create custom AWS Lambda functions to move the individual records from me databases to an Amazon\nRedshift duster",
                "D.  Use an AWS Glue extract transform, and toad (ETL) job to convert the data into JSON format Load the\ndata into multiple Amazon OpenSearch Service (Amazon Elasticsearch Service) dusters",
                "E.  Use blueprints in AWS Lake Formation to identify the data that can be ingested into a data lake Use\nAWS Glue to crawl the source extract the data and load the data into Amazon S3 in Apache Parquet\nformat"
            ],
            "Explanation": "Answer: C E\n",
            "RightAnswer": [
                "C",
                "E"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 73,
            "QuestionContent": "A company is concerned about the security of its public web application due to recent web attacks. The\napplication uses an Application Load Balancer (ALB). A solutions architect must reduce the risk of DDoS\nattacks against the application.\nWhat should the solutions architect do to meet this requirement?",
            "Option": [
                "A.  Add an Amazon Inspector agent to the ALB.",
                "B.  Configure Amazon Macie to prevent attacks.",
                "C.  Enable AWS Shield Advanced to prevent attacks.",
                "D.  Configure Amazon GuardDuty to monitor the ALB."
            ],
            "Explanation": "Answer: C\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 74,
            "QuestionContent": "A company is hosting a web application on AWS using a single Amazon EC2 instance that stores\nuser-uploaded documents in an Amazon EBS volume. For better scalability and availability, the company\nduplicated the architecture and created a second EC2 instance and EBS volume in another Availability Zone\nplacing both behind an Application Load Balancer After completing this change, users reported that, each time\nthey refreshed the website, they could see one subset of their documents or the other, but never all of the\ndocuments at the same time.\nWhat should a solutions architect propose to ensure users see all of their documents at once?",
            "Option": [
                "A.  Copy the data so both EBS volumes contain all the documents.",
                "B.  Configure the Application Load Balancer to direct a user to the server with the documents",
                "C.  Copy the data from both EBS volumes to Amazon EFS Modify the application to save new documents\nto Amazon EFS",
                "D.  Configure the Application Load Balancer to send the request to both servers Return each document from\nthe correct server."
            ],
            "Explanation": "Answer: C\nExplanation\nAmazon EFS provides file storage in the AWS Cloud. With Amazon EFS, you can create a file system, mount\nthe file system on an Amazon EC2 instance, and then read and write data to and from your file system. You\ncan mount an Amazon EFS file system in your VPC, through the Network File System versions 4.0 and\n4.1 (NFSv4) protocol. We recommend using a current generation Linux NFSv4.1 client, such as those found in\nthe latest Amazon Linux, Redhat, and Ubuntu\nAMIs, in conjunction with the Amazon EFS Mount Helper. For instructions, see Using the amazon-efs-utils\nTools.\nFor a list of Amazon EC2 Linux Amazon Machine Images (AMIs) that support this protocol, see NFS\nSupport. For some AMIs, you\u0027ll need to install an NFS client to mount your file system on your Amazon EC2\ninstance. For instructions, see Installing the NFS Client.\nYou can access your Amazon EFS file system concurrently from multiple NFS clients, so applications that\nscale beyond a single connection can access a file system. Amazon EC2 instances running in multiple\nAvailability Zones within the same AWS Region can access the file system, so that many users can access and\nshare a common data source.\nhttps://docs.aws.amazon.com/efs/latest/ug/how-it-works.html#how-it-works-ec2\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 75,
            "QuestionContent": "A company has an ecommerce checkout workflow that writes an order to a database and calls a service to\nprocess the payment. Users are experiencing timeouts during the checkout process. When users resubmit the\ncheckout form, multiple unique orders are created for the same desired transaction.\nHow should a solutions architect refactor this workflow to prevent the creation of multiple orders?",
            "Option": [
                "A.  Configure the web application to send an order message to Amazon Kinesis Data Firehose. Set the\npayment service to retrieve the message from Kinesis Data Firehose and process the order.",
                "B.  Create a rule in AWS CloudTrail to invoke an AWS Lambda function based on the logged application\npath request Use Lambda to query the database, call the payment service, and pass in the order\ninformation.",
                "C.  Store the order in the database. Send a message that includes the order number to Amazon Simple\nNotification Service (Amazon SNS). Set the payment service to poll Amazon SNS. retrieve the message,\nand process the order.",
                "D.  Store the order in the database. Send a message that includes the order number to an Amazon Simple\nQueue Service (Amazon SQS) FIFO queue. Set the payment service to retrieve the message and process\nthe order. Delete the message from the queue."
            ],
            "Explanation": "Answer: D\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 76,
            "QuestionContent": "A company maintains a searchable repository of items on its website. The data is stored in an Amazon RDS\nfor MySQL database table that contains more than 10 million rows The database has 2 TB of General Purpose\nSSD storage There are millions of updates against this data every day through the company\u0027s website\nThe company has noticed that some insert operations are taking 10 seconds or longer The company has\ndetermined that the database storage performance is the problem\nWhich solution addresses this performance issue?",
            "Option": [
                "A.  Change the storage type to Provisioned IOPS SSD",
                "B.  Change the DB instance to a memory optimized instance class",
                "C.  Change the DB instance to a burstable performance instance class",
                "D.  Enable Multi-AZ RDS read replicas with MySQL native asynchronous replication."
            ],
            "Explanation": "Answer: A\nExplanation\nhttps://aws.amazon.com/ebs/features/\n\u0022Provisioned IOPS volumes are backed by solid-state drives (SSDs) and are the highest performance EBS\nvolumes designed for your critical, I/O intensive database applications.\nThese volumes are ideal for both IOPS-intensive and throughput-intensive workloads that require extremely\nlow latency.\u0022\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Storage.html\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 77,
            "QuestionContent": "A company has a production web application in which users upload documents through a web interlace or a\nmobile app. According to a new regulatory requirement, new documents cannot be modified or deleted after\nthey are stored.\nWhat should a solutions architect do to meet this requirement?",
            "Option": [
                "A.  Store the uploaded documents in an Amazon S3 bucket with S3 Versioning and S3 Object Lock enabled",
                "B.  Store the uploaded documents in an Amazon S3 bucket. Configure an S3 Lifecycle policy to archive the\ndocuments periodically.",
                "C.  Store the uploaded documents in an Amazon S3 bucket with S3 Versioning enabled Configure an ACL\nto restrict all access to read-only.",
                "D.  Store the uploaded documents on an Amazon Elastic File System (Amazon EFS) volume. Access the\ndata by mounting the volume in read-only mode."
            ],
            "Explanation": "Answer: A\nExplanation\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 78,
            "QuestionContent": "A company is designing an application. The application uses an AWS Lambda function to receive information\nthrough Amazon API Gateway and to store the information in an Amazon Aurora PostgreSQL database.\nDuring the proof-of-concept stage, the company has to increase the Lambda quotas significantly to handle the\nhigh volumes of data that the company needs to load into the database. A solutions architect must recommend\na new design to improve scalability and minimize the configuration effort.\nWhich solution will meet these requirements?",
            "Option": [
                "A.  Refactor the Lambda function code to Apache Tomcat code that runs on Amazon EC2 instances.\nConnect the database by using native Java Database Connectivity (JDBC) drivers.",
                "B.  Change the platform from Aurora to Amazon DynamoDB. Provision a DynamoDB Accelerator (DAX)\ncluster. Use the DAX client SDK to point the existing DynamoDB API calls at the DAX cluster.",
                "C.  Set up two Lambda functions. Configure one function to receive the information. Configure the other\nfunction to load the information into the database. Integrate the Lambda functions by using Amazon\nSimple Notification Service (Amazon SNS).",
                "D.  Set up two Lambda functions. Configure one function to receive the information. Configure the other\nfunction to load the information into the database. Integrate the Lambda functions by using an Amazon\nSimple Queue Service (Amazon SQS) queue."
            ],
            "Explanation": "Answer: D\nExplanation\nbottlenecks can be avoided with queues (SQS).\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 79,
            "QuestionContent": "A company runs a containerised application on a Kubernetes cluster m an on premises data center The\ncompany is using a MongoOB drtabii tor dan atanige The company wants to migrate some of these\nenvironments to AWS but no code changes or deployment method changes ate possible at this time The\ncompany needs a solution mat minimizes operational overhead\nWhich solution meets these requirements?",
            "Option": [
                "A.  Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 worker nodes for compute\nand MongoOB on EC2 for data storage",
                "B.  Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for compute and Amazon\nDynamoDB tor data storage",
                "C.  Use Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 worker nodes for compute\nand Amazon DynamoDB for data storage",
                "D.  Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS fargate for compute and Amazon\nDocumentDB (with MongoOB compatfciMy) for data storage"
            ],
            "Explanation": "Answer: D\nExplanation\nAmazon DocumentDB (with MongoDB compatibility) is a fast, reliable, and fully managed database service.\nAmazon DocumentDB makes it easy to set up, operate, and scale MongoDB-compatible databases in the\ncloud. With Amazon DocumentDB, you can run the same application code and use the same drivers and tools\nthat you use with MongoDB.\nhttps://docs.aws.amazon.com/documentdb/latest/developerguide/what-is.html\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 80,
            "QuestionContent": "A corporation has recruited a new cloud engineer who should not have access to the CompanyConfidential\nAmazon S3 bucket. The cloud engineer must have read and write permissions on an S3 bucket named\nAdminTools.\nWhich IAM policy will satisfy these criteria?",
            "Option": [
                "A.  Text, letter Description automatically generated\nPractice Test Amazon Web Services - SAA-C03",
                "B.  Text Description automatically generated",
                "C.  Text, application Description automatically generated\nPractice Test Amazon Web Services - SAA-C03",
                "D.  Text, application Description automatically generated"
            ],
            "Explanation": "Answer: A\nExplanation\nhttps://docs.amazonaws.cn/en_us/IAM/latest/UserGuide/reference_policies_examples_s3_rw-bucket.html\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 81,
            "QuestionContent": "A gaming company has a web application that displays scores. The application runs on Amazon EC2 instances\nbehind an Application Load Balancer. The application stores data in an Amazon RDS for MySQL database.\nUsers are starting to experience long delays and interruptions that are caused by database read performance.\nThe company wants to improve the user experience while minimizing changes to the application\u0027s architecture.\nWhat should a solutions architect do to meet these requirements?",
            "Option": [
                "A.  Use Amazon ElastiCache in front of the database.",
                "B.  Use RDS Proxy between the application and the database.",
                "C.  Migrate the application from EC2 instances to AWS Lambda.",
                "D.  Migrate the database from Amazon RDS for MySQL to Amazon DynamoDB."
            ],
            "Explanation": "Answer: C\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 82,
            "QuestionContent": "An online retail company has more than 50 million active customers and receives more than 25,000 orders\neach day. The company collects purchase data for customers and stores this data in Amazon S3. Additional\ncustomer data is stored in Amazon RDS.\nThe company wants to make all the data available to various teams so that the teams can perform analytics.\nThe solution must provide the ability to manage fine-grained permissions for the data and must minimize\noperational overhead.\nWhich solution will meet these requirements?",
            "Option": [
                "A.  Migrate the purchase data to write directly to Amazon RDS. Use RDS access controls to limit access.",
                "B.  Schedule an AWS Lambda function to periodically copy data from Amazon RDS to Amazon S3. Create\nan AWS Glue crawler. Use Amazon Athena to query the data. Use S3 policies to limit access.",
                "C.  Create a data lake by using AWS Lake Formation. Create an AWS Glue JDBC connection to Amazon\nRDS. Register (he S3 bucket in Lake Formation. Use Lake Formation access controls to limit access.",
                "D.  Create an Amazon Redshift cluster. Schedule an AWS Lambda function to periodically copy data from\nAmazon S3 and Amazon RDS to Amazon Redshift. Use Amazon Redshift access controls to limit\naccess."
            ],
            "Explanation": "Answer: D\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 83,
            "QuestionContent": "A company runs workloads on AWS. The company needs to connect to a service from an external provider.\nThe service is hosted in the provider\u0027s VPC. According to the company\u2019s security team, the connectivity must\nbe private and must be restricted to the target service. The connection must be initiated only from the\ncompany\u2019s VPC.\nWhich solution will mast these requirements?",
            "Option": [
                "A.  Create a VPC peering connection between the company\u0027s VPC and the provider\u0027s VPC. Update the route\ntable to connect to the target service.",
                "B.  Ask the provider to create a virtual private gateway in its VPC. Use AWS PrivateLink to connect to the\ntarget service.",
                "C.  Create a NAT gateway in a public subnet of the company\u0027s VPC. Update the route table to connect to\nthe target service.",
                "D.  Ask the provider to create a VPC endpoint for the target service. Use AWS PrivateLink to connect to the\ntarget service."
            ],
            "Explanation": "Answer: D\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 84,
            "QuestionContent": "A global company is using Amazon API Gateway to design REST APIs for its loyalty club users in the\nus-east-1 Region and the ap-southeast-2 Region. A solutions architect must design a solution to protect these\nAPI Gateway managed REST APIs across multiple accounts from SQL injection and cross-site scripting\nattacks.\nWhich solution will meet these requirements with the LEAST amount of administrative effort?",
            "Option": [
                "A.  Set up AWS WAF in both Regions. Associate Regional web ACLs with an API stage.",
                "B.  Set up AWS Firewall Manager in both Regions. Centrally configure AWS WAF rules.",
                "C.  Set up AWS Shield in bath Regions. Associate Regional web ACLs with an API stage.",
                "D.  Set up AWS Shield in one of the Regions. Associate Regional web ACLs with an API stage."
            ],
            "Explanation": "Answer: B\nExplanation\nUsing AWS WAF has several benefits. Additional protection against web attacks using criteria that you\nspecify. You can define criteria using characteristics of web requests such as the following: Presence of SQL\ncode that is likely to be malicious (known as SQL injection). Presence of a script that is likely to be malicious\n(known as cross-site scripting). AWS Firewall Manager simplifies your administration and maintenance tasks\nacross multiple accounts and resources for a variety of protections.\nhttps://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 85,
            "QuestionContent": "A company is implementing a shared storage solution for a media application that is hosted m the AWS Cloud\nThe company needs the ability to use SMB clients to access data The solution must he fully managed.\nWhich AWS solution meets these requirements?",
            "Option": [
                "A.  Create an AWS Storage Gateway volume gateway. Create a file share that uses the required client\nprotocol Connect the application server to the file share.",
                "B.  Create an AWS Storage Gateway tape gateway Configure (apes to use Amazon S3 Connect the\napplication server lo the tape gateway",
                "C.  Create an Amazon EC2 Windows instance Install and configure a Windows file share role on the\ninstance. Connect the application server to the file share.",
                "D.  Create an Amazon FSx for Windows File Server tile system Attach the fie system to the origin server.\nConnect the application server to the file system"
            ],
            "Explanation": "Answer: D\nExplanation\nhttps://aws.amazon.com/fsx/lustre/\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 86,
            "QuestionContent": "A large media company hosts a web application on AWS. The company wants to start caching confidential\nmedia files so that users around the world will have reliable access to the files. The content is stored in\nAmazon S3 buckets. The company must deliver the content quickly, regardless of where the requests originate\ngeographically.\nWhich solution will meet these requirements?",
            "Option": [
                "A.  Use AWS DataSync to connect the S3 buckets to the web application.",
                "B.  Deploy AWS Global Accelerator to connect the S3 buckets to the web application.",
                "C.  Deploy Amazon CloudFront to connect the S3 buckets to CloudFront edge servers.",
                "D.  Use Amazon Simple Queue Service (Amazon SQS) to connect the S3 buckets to the web application."
            ],
            "Explanation": "Answer: C\nExplanation\nCloudFront uses a local cache to provide the response, AWS Global accelerator proxies requests and connects\nto the application all the time for the response.\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html#private-content-granting-permissions-to-oai\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 87,
            "QuestionContent": "A company runs an applcalion on a large Heel of Amazon EC2 ratances. The application leads and write\nentries into an Amazon DynamoDB (able The size of the DynamoDB table continuously grows but the\napplication needs only data from the last 30 days. The company needs a solution that mmmizes cost and\ndevelopment effort.\nWhich solution meets these requirements?",
            "Option": [
                "A.  Use an AWS CloudFomiahon template to deploy the complete solution Redeploy the CloudFormation\nstack every 30 days and delete the original stack",
                "B.  Use an EC2 Instance that runs a monitonng application from AWS Marketplace Configure the\nmonitoring application to use Amazon DynamoDB Streams to store the timestamp when a new item is\ncreated in the table Use a scnpt that runs on the EC2 instance to delele items that have a timestamp that\nis older than 30 days",
                "C.  Configure Amazon DynamoDB Streams to invoke an AWS Lambda function when a new item is\ncreated in the table Configure the Lambda function to delete items in the table that are older than 30\ndays",
                "D.  Extend the application to add an attribute that has a value of the current timestamp plus 30 days to each\nnew item that is created in the (able Configure DynamoDB to use the attribute as (he TTL attribute"
            ],
            "Explanation": "Answer: D\nExplanation\nAmazon DynamoDB Time to Live (TTL) allows you to define a per-item timestamp to determine when an\nitem is no longer needed. Shortly after the date and time of the specified timestamp, DynamoDB deletes the\nitem from your table without consuming any write throughput. TTL is provided at no extra cost as a means to\nreduce stored data volumes by retaining only the items that remain current for your workload\u2019s needs.\nTTL is useful if you store items that lose relevance after a specific time. The following are example TTL use\ncases:\nRemove user or sensor data after one year of inactivity in an application.\nArchive expired items to an Amazon S3 data lake via Amazon DynamoDB Streams and AWS Lambda.\nRetain sensitive data for a certain amount of time according to contractual or regulatory obligations.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 88,
            "QuestionContent": "A company has a production workload that runs on 1,000 Amazon EC2 Linux instances. The workload is\npowered by third-party software. The company needs to patch the third-party software on all EC2 instances as\nquickly as possible to remediate a critical security vulnerability.\nWhat should a solutions architect do to meet these requirements?",
            "Option": [
                "A.  Create an AWS Lambda function to apply the patch to all EC2 instances.",
                "B.  Configure AWS Systems Manager Patch Manager to apply the patch to all EC2 instances.",
                "C.  Schedule an AWS Systems Manager maintenance window to apply the patch to all EC2 instances.",
                "D.  Use AWS Systems Manager Run Command to run a custom command that applies the patch to all EC2\ninstances."
            ],
            "Explanation": "Answer: D\nExplanation\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/about-windows-app-patching.html\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 89,
            "QuestionContent": "A company is building a containerized application on premises and decides to move the application to AWS.\nThe application will have thousands of users soon after li is deployed. The company Is unsure how to manage\nthe deployment of containers at scale. The company needs to deploy the containerized application in a highly\navailable architecture that minimizes operational overhead.\nWhich solution will meet these requirements?",
            "Option": [
                "A.  Store container images In an Amazon Elastic Container Registry (Amazon ECR) repository. Use an\nAmazon Elastic Container Service (Amazon ECS) cluster with the AWS Fargate launch type to run the\ncontainers. Use target tracking to scale automatically based on demand.",
                "B.  Store container images in an Amazon Elastic Container Registry (Amazon ECR) repository. Use an\nAmazon Elastic Container Service (Amazon ECS) cluster with the Amazon EC2 launch type to run the\ncontainers. Use target tracking to scale automatically based on demand.",
                "C.  Store container images in a repository that runs on an Amazon EC2 instance. Run the containers on EC2\ninstances that are spread across multiple Availability Zones. Monitor the average CPU utilization in\nAmazon CloudWatch. Launch new EC2 instances as needed",
                "D.  Create an Amazon EC2 Amazon Machine Image (AMI) that contains the container image Launch EC2\nInstances in an Auto Scaling group across multiple Availability Zones. Use an Amazon CloudWatch\nalarm to scale out EC2 instances when the average CPU utilization threshold is breached."
            ],
            "Explanation": "Answer: A\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 90,
            "QuestionContent": "A solutions architect is designing a two-tier web application The application consists of a public-facing web\ntier hosted on Amazon EC2 in public subnets The database tier consists of Microsoft SQL Server running on\nAmazon EC2 in a private subnet Security is a high priority for the company\nHow should security groups be configured in this situation? (Select TWO )",
            "Option": [
                "A.  Configure the security group for the web tier to allow inbound traffic on port 443 from 0.0.0.0/0.",
                "B.  Configure the security group for the web tier to allow outbound traffic on port 443 from 0.0.0.0/0.",
                "C.  Configure the security group for the database tier to allow inbound traffic on port 1433 from the security\ngroup for the web tier.",
                "D.  Configure the security group for the database tier to allow outbound traffic on ports 443 and 1433 to the\nsecurity group for the web tier.",
                "E.  Configure the security group for the database tier to allow inbound traffic on ports 443 and 1433 from\nthe security group for the web tier."
            ],
            "Explanation": "Answer: A C\nExplanation\n\u0022Security groups create an outbound rule for every inbound rule.\u0022 Not completely right. Statefull does NOT\nmean that if you create an inbound (or outbound) rule, it will create an outbound (or inbound) rule. What it\ndoes mean is: suppose you create an inbound rule on port 443 for the X ip. When a request enters on port 443\nfrom X ip, it will allow traffic out for that request in the port 443. However, if you look at the outbound rules,\nthere will not be any outbound rule on port 443 unless explicitly create it. In ACLs, which are stateless, you\nwould have to create an inbound rule to allow incoming requests and an outbound rule to allow your\napplication responds to those incoming requests.\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html#SecurityGroupRules\n",
            "RightAnswer": [
                "A",
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 91,
            "QuestionContent": "A company needs to store data in Amazon S3 and must prevent the data from being changed. The company\nwants new objects that are uploaded to Amazon S3 to remain unchangeable for a nonspecific amount of time\nuntil the company decides to modify the objects. Only specific users in the company\u2019s AWS account can have\nthe ability to delete the objects. What should a solutions architect do to meet these requirements?",
            "Option": [
                "A.  Create an S3 Glacier vault Apply a write-once, read-many (WORM) vault lock policy to the objects",
                "B.  Create an S3 bucket with S3 Object Lock enabled Enable versioning Set a retention period of 100 years\nUse governance mode as the S3 bucket\u0027s default retention mode for new objects",
                "C.  Create an S3 bucket Use AWS CloudTrail to (rack any S3 API events that modify the objects Upon\nnotification, restore the modified objects from any backup versions that the company has",
                "D.  Create an S3 bucket with S3 Object Lock enabled Enable versioning Add a legal hold to the objects Add\nthe s3 PutObjectLegalHold permission to the IAM policies of users who need to delete the objects"
            ],
            "Explanation": "Answer: D\nExplanation\n\u0022The Object Lock legal hold operation enables you to place a legal hold on an object version. Like setting a\nretention period, a legal hold prevents an object version from being overwritten or deleted. However, a legal\nhold doesn\u0027t have an associated retention period and remains in effect until removed.\u0022\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/batch-ops-legal-hold.html\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 92,
            "QuestionContent": "A company needs the ability to analyze the log files of its proprietary application. The logs are stored in JSON\nformat in an Amazon S3 bucket Queries will be simple and will run on-demand A solutions architect needs to\nperform the analysis with minimal changes to the existing architecture\nWhat should the solutions architect do to meet these requirements with the LEAST amount of operational\noverhead?",
            "Option": [
                "A.  Use Amazon Redshift to load all the content into one place and run the SQL queries as needed",
                "B.  Use Amazon CloudWatch Logs to store the logs Run SQL queries as needed from the Amazon\nCloudWatch console",
                "C.  Use Amazon Athena directly with Amazon S3 to run the queries as needed",
                "D.  Use AWS Glue to catalog the logs Use a transient Apache Spark cluster on Amazon EMR to run the\nSQL queries as needed"
            ],
            "Explanation": "Answer: C\nExplanation\nAmazon Athena can be used to query JSON in S3\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 93,
            "QuestionContent": "A company has registered its domain name with Amazon Route 53. The company uses Amazon API Gateway\nin the ca-central-1 Region as a public interface for its backend microservice APIs. Third-party services\nconsume the APIs securely. The company wants to design its API Gateway URL with the company\u0027s domain\nname and corresponding certificate so that the third-party services can use HTTPS.\nWhich solution will meet these requirements?",
            "Option": [
                "A.  Create stage variables in API Gateway with Name=\u0022Endpoint-URL\u0022 and Value=\u0022Company Domain\nName\u0022 to overwrite the default URL. Import the public certificate associated with the company\u0027s domain\nname into AWS Certificate Manager (ACM).",
                "B.  Create Route 53 DNS records with the company\u0027s domain name. Point the alias record to the Regional\nAPI Gateway stage endpoint. Import the public certificate associated with the company\u0027s domain name\ninto AWS Certificate Manager (ACM) in the us-east-1 Region.",
                "C.  Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company\u0027s\ndomain name. Import the public certificate associated with the company\u0027s domain name into AWS\nCertificate Manager (ACM) in the same Region. Attach the certificate to the API Gateway endpoint.\nConfigure Route 53 to route traffic to the API Gateway endpoint.",
                "D.  Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company\u0027s\ndomain name. Import the public certificate associated with the company\u0027s domain name into AWS\nCertificate Manager (ACM) in the us-east-1 Region. Attach the certificate to the API Gateway APIs.\nCreate Route 53 DNS records with the company\u0027s domain name. Point an A record to the company\u0027s\ndomain name."
            ],
            "Explanation": "Answer: D\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 94,
            "QuestionContent": "A company needs to review its AWS Cloud deployment to ensure that its Amazon S3 buckets do not have\nunauthorized configuration changes.\nWhat should a solutions architect do to accomplish this goal?",
            "Option": [
                "A.  Turn on AWS Config with the appropriate rules.",
                "B.  Turn on AWS Trusted Advisor with the appropriate checks.",
                "C.  Turn on Amazon Inspector with the appropriate assessment template.",
                "D.  Turn on Amazon S3 server access logging. Configure Amazon EventBridge (Amazon Cloud Watch\nEvents)."
            ],
            "Explanation": "Answer: A\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 95,
            "QuestionContent": "A solutions architect needs to implement a solution to reduce a company\u0027s storage costs. All the company\u0027s\ndata is in the Amazon S3 Standard storage class. The company must keep all data for at least 25 years. Data\nfrom the most recent 2 years must be highly available and immediately retrievable.\nWhich solution will meet these requirements?",
            "Option": [
                "A.  Set up an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive immediately.",
                "B.  Set up an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive after 2 years.",
                "C.  Use S3 Intelligent-Tiering. Activate the archiving option to ensure that data is archived in S3 Glacier\nDeep Archive.",
                "D.  Set up an S3 Lifecycle policy to transition objects to S3 One Zone-Infrequent Access (S3 One Zone-IA)\nimmediately and to S3 Glacier Deep Archive after 2 years."
            ],
            "Explanation": "Answer: B\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 96,
            "QuestionContent": "A company has a website hosted on AWS. The website is behind an Application Load Balancer (ALB) that is\nconfigured to handle HTTP and HTTPS separately. The company wants to forward all requests to the website\nso that the requests will use HTTPS.\nWhat should a solutions architect do to meet this requirement?",
            "Option": [
                "A.  Update the ALB\u0027s network ACL to accept only HTTPS traffic",
                "B.  Create a rule that replaces the HTTP in the URL with HTTPS.",
                "C.  Create a listener rule on the ALB to redirect HTTP traffic to HTTPS.",
                "D.  Replace the ALB with a Network Load Balancer configured to use Server Name Indication (SNI)."
            ],
            "Explanation": "Answer: C\nExplanation\nhttps://aws.amazon.com/premiumsupport/knowledge-center/elb-redirect-http-to-https-using-alb/\nHow can I redirect HTTP requests to HTTPS using an Application Load Balancer? Last updated: 2020-10-30 I\nwant to redirect HTTP requests to HTTPS using Application Load Balancer listener rules. How can I do this?\nResolution Reference:\nhttps://aws.amazon.com/premiumsupport/knowledge-center/elb-redirect-http-to-https-using-alb/\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 97,
            "QuestionContent": "A company uses a three-tier web application to provide training to new employees. The application is accessed\nfor only 12 hours every day. The company is using an Amazon RDS for MySQL DB instance to store\ninformation and wants to minimize costs.\nWhat should a solutions architect do to meet these requirements?",
            "Option": [
                "A.  Configure an IAM policy for AWS Systems Manager Session Manager. Create an IAM role for the\npolicy. Update the trust relationship of the role. Set up automatic start and stop for the DB instance.",
                "B.  Create an Amazon ElastiCache for Redis cache cluster that gives users the ability to access the data from\nthe cache when the DB instance is stopped. Invalidate the cache after the DB instance is started.",
                "C.  Launch an Amazon EC2 instance. Create an IAM role that grants access to Amazon RDS. Attach the\nrole to the EC2 instance. Configure a cron job to start and stop the EC2 instance on the desired schedule.",
                "D.  Create AWS Lambda functions to start and stop the DB instance. Create Amazon EventBridge (Amazon\nCloudWatch Events) scheduled rules to invoke the Lambda functions. Configure the Lambda functions\nas event targets for the rules"
            ],
            "Explanation": "Answer: C\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 98,
            "QuestionContent": "A company runs an on-premises application that is powered by a MySQL database The company is migrating\nthe application to AWS to Increase the application\u0027s elasticity and availability\nThe current architecture shows heavy read activity on the database during times of normal operation Every 4\nhours the company\u0027s development team pulls a full export of the production database to populate a database in\nthe staging environment During this period, users experience unacceptable application latency The\ndevelopment team is unable to use the staging environment until the procedure completes\nA solutions architect must recommend replacement architecture that alleviates the application latency issue\nThe replacement architecture also must give the development team the ability to continue using the staging\nenvironment without delay\nWhich solution meets these requirements?",
            "Option": [
                "A.  Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production. Populate the staging\ndatabase by implementing a backup and restore process that uses the mysqldump utility.",
                "B.  Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production Use database cloning to\ncreate the staging database on-demand",
                "C.  Use Amazon RDS for MySQL with a Mufti AZ deployment and read replicas for production Use the\nstandby instance tor the staging database.",
                "D.  Use Amazon RDS for MySQL with a Multi-AZ deployment and read replicas for production. Populate\nthe staging database by implementing a backup and restore process that uses the mysqldump utility."
            ],
            "Explanation": "Answer: B\nExplanation\nhttps://aws.amazon.com/blogs/aws/amazon-aurora-fast-database-cloning/\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 99,
            "QuestionContent": "A company is running an online transaction processing (OLTP) workload on AWS. This workload uses an\nunencrypted Amazon RDS DB instance in a Multi-AZ deployment. Daily database snapshots are taken from\nthis instance.\nWhat should a solutions architect do to ensure the database and snapshots are always encrypted moving\nforward?",
            "Option": [
                "A.  Encrypt a copy of the latest DB snapshot. Replace existing DB instance by restoring the encrypted\nsnapshot",
                "B.  Create a new encrypted Amazon Elastic Block Store (Amazon EBS) volume and copy the snapshots to\nit Enable encryption on the DB instance",
                "C.  Copy the snapshots and enable encryption using AWS Key Management Service (AWS KMS) Restore\nencrypted snapshot to an existing DB instance",
                "D.  Copy the snapshots to an Amazon S3 bucket that is encrypted using server-side encryption with AWS\nKey Management Service (AWS KMS) managed keys (SSE-KMS)"
            ],
            "Explanation": "Answer: A\nExplanation\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_RestoreFromSnapshot.html#USER_RestoreFromSnapshot.CON\nUnder \u0022Encrypt unencrypted resources\u0022 -\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 100,
            "QuestionContent": "A company wants to measure the effectiveness of its recent marketing campaigns. The company performs\nbatch processing on csv files of sales data and stores the results \u00ABi an Amazon S3 bucket once every hour. The\nS3 bi petabytes of objects. The company runs one-time queries in Amazon Athena to determine which\nproducts are most popular on a particular date for a particular region Queries sometimes fail or take longer\nthan expected to finish.\nWhich actions should a solutions architect take to improve the query performance and reliability? (Select\nTWO.)",
            "Option": [
                "A.  Reduce the S3 object sizes to less than 126 MB",
                "B.  Partition the data by date and region n Amazon S3",
                "C.  Store the files as large, single objects in Amazon S3.",
                "D.  Use Amazon Kinosis Data Analytics to run the Queries as pan of the batch processing operation",
                "E.  Use an AWS duo extract, transform, and load (ETL) process to convert the csv files into Apache Parquet\nformat."
            ],
            "Explanation": "Answer: C E\n",
            "RightAnswer": [
                "C",
                "E"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 101,
            "QuestionContent": "A company runs a web-based portal that provides users with global breaking news, local alerts, and weather\nupdates. The portal delivers each user a personalized view by using mixture of static and dynamic content.\nContent is served over HTTPS through an API server running on an Amazon EC2 instance behind an\nApplication Load Balancer (ALB). The company wants the portal to provide this content to its users across the\nworld as quickly as possible.\nHow should a solutions architect design the application to ensure the LEAST amount of latency for all users?",
            "Option": [
                "A.  Deploy the application stack in a single AWS Region. Use Amazon CloudFront to serve all static and\ndynamic content by specifying the ALB as an origin.",
                "B.  Deploy the application stack in two AWS Regions. Use an Amazon Route 53 latency routing policy to\nserve all content from the ALB in the closest Region.",
                "C.  Deploy the application stack in a single AWS Region. Use Amazon CloudFront to serve the static\ncontent. Serve the dynamic content directly from the ALB.",
                "D.  Deploy the application stack in two AWS Regions. Use an Amazon Route 53 geolocation routing policy\nto serve all content from the ALB in the closest Region."
            ],
            "Explanation": "Answer: A\nExplanation\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/deliver-your-apps-dynamic-content-using-amazon-cloudfront-getting-started-template/\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 102,
            "QuestionContent": "A company is using a SQL database to store movie data that is publicly accessible. The database runs on an\nAmazon RDS Single-AZ DB instance A script runs queries at random intervals each day to record the number\nof new movies that have been added to the database. The script must report a final total during business hours\nThe company\u0027s development team notices that the database performance is inadequate for development tasks\nwhen the script is running. A solutions architect must recommend a solution to resolve this issue. Which\nsolution will meet this requirement with the LEAST operational overhead?",
            "Option": [
                "A.  Modify the DB instance to be a Multi-AZ deployment",
                "B.  Create a read replica of the database Configure the script to query only the read replica",
                "C.  Instruct the development team to manually export the entries in the database at the end of each day",
                "D.  Use Amazon ElastiCache to cache the common queries that the script runs against the database"
            ],
            "Explanation": "Answer: B\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 103,
            "QuestionContent": "A company hosts a two-tier application on Amazon EC2 instances and Amazon RDS. The application\u0027s\ndemand varies based on the time of day. The load is minimal after work hours and on weekends. The EC2\ninstances run in an EC2 Auto Scaling group that is configured with a minimum of two instances and a\nmaximum of five instances. The application must be available at all times, but the company is concerned about\noverall cost.\nWhich solution meets the availability requirement MOST cost-effectively?",
            "Option": [
                "A.  Use all EC2 Spot Instances. Stop the RDS database when it is not in use.",
                "B.  Purchase EC2 Instance Savings Plans to cover five EC2 instances. Purchase an RDS Reserved DB\nInstance",
                "C.  Purchase two EC2 Reserved Instances Use up to three additional EC2 Spot Instances as needed. Stop the\nRDS database when it is not in use.",
                "D.  Purchase EC2 Instance Savings Plans to cover two EC2 instances. Use up to three additional EC2\nOn-Demand Instances as needed. Purchase an RDS Reserved DB Instance."
            ],
            "Explanation": "Answer: D\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 104,
            "QuestionContent": "A solutions architect needs to help a company optimize the cost of running an application on AWS. The\napplication will use Amazon EC2 instances, AWS Fargate, and AWS Lambda for compute within the\narchitecture.\nThe EC2 instances will run the data ingestion layer of the application. EC2 usage will be sporadic and\nunpredictable. Workloads that run on EC2 instances can be interrupted at any time. The application front end\nwill run on Fargate, and Lambda will serve the API layer. The front-end utilization and API layer utilization\nwill be predictable over the course of the next year.\nWhich combination of purchasing options will provide the MOST cost-effective solution for hosting this\napplication? (Choose two.)",
            "Option": [
                "A.  Use Spot Instances for the data ingestion layer",
                "B.  Use On-Demand Instances for the data ingestion layer",
                "C.  Purchase a 1-year Compute Savings Plan for the front end and API layer.",
                "D.  Purchase 1-year All Upfront Reserved instances for the data ingestion layer.",
                "E.  Purchase a 1-year EC2 instance Savings Plan for the front end and API layer."
            ],
            "Explanation": "Answer: A C\n",
            "RightAnswer": [
                "A",
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 105,
            "QuestionContent": "A company\u0027s web application is running on Amazon EC2 instances behind an Application Load Balancer. The\ncompany recently changed its policy, which now requires the application to be accessed from one specific\ncountry only.\nWhich configuration will meet this requirement?",
            "Option": [
                "A.  Configure the security group for the EC2 instances.",
                "B.  Configure the security group on the Application Load Balancer.",
                "C.  Configure AWS WAF on the Application Load Balancer in a VPC.",
                "D.  Configure the network ACL for the subnet that contains the EC2 instances."
            ],
            "Explanation": "Answer: C\nExplanation\nhttps://aws.amazon.com/about-aws/whats-new/2017/10/aws-waf-now-supports-geographic-match/\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 106,
            "QuestionContent": "A company has an application that provides marketing services to stores. The services are based on previous\npurchases by store customers. The stores upload transaction data to the company through SFTP, and the data is\nprocessed and analyzed to generate new marketing offers. Some of the files can exceed 200 GB in size.\nRecently, the company discovered that some of the stores have uploaded files that contain personally\nidentifiable information (PII) that should not have been included. The company wants administrators to be\nalerted if PII is shared again. The company also wants to automate remediation.\nWhat should a solutions architect do to meet these requirements with the LEAST development effort?",
            "Option": [
                "A.  Use an Amazon S3 bucket as a secure transfer point. Use Amazon Inspector to scan me objects in the\nbucket. If objects contain Pll. trigger an S3 Lifecycle policy to remove the objects that contain Pll.",
                "B.  Use an Amazon S3 bucket as a secure transfer point. Use Amazon Macie to scan the objects in the\nbucket. If objects contain Pll. Use Amazon Simple Notification Service (Amazon SNS) to trigger a\nnotification to the administrators to remove the objects mat contain Pll.",
                "C.  Implement custom scanning algorithms in an AWS Lambda function. Trigger the function when objects\nare loaded into the bucket. It objects contain Rll. use Amazon Simple Notification Service (Amazon\nSNS) to trigger a notification to the administrators to remove the objects that contain Pll.",
                "D.  Implement custom scanning algorithms in an AWS Lambda function. Trigger the function when objects\nare loaded into the bucket. If objects contain Pll. use Amazon Simple Email Service (Amazon STS) to\ntrigger a notification to the administrators and trigger on S3 Lifecycle policy to remove the objects mot\ncontain PII."
            ],
            "Explanation": "Answer: B\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 107,
            "QuestionContent": "A company has a large Microsoft SharePoint deployment running on-premises that requires Microsoft\nWindows shared file storage. The company wants to migrate this workload to the AWS Cloud and is\nconsidering various storage options. The storage solution must be highly available and integrated with Active\nDirectory for access control.\nWhich solution will satisfy these requirements?",
            "Option": [
                "A.  Configure Amazon EFS storage and set the Active Directory domain for authentication",
                "B.  Create an SMB Me share on an AWS Storage Gateway tile gateway in two Availability Zones",
                "C.  Create an Amazon S3 bucket and configure Microsoft Windows Server to mount it as a volume",
                "D.  Create an Amazon FSx for Windows File Server file system on AWS and set the Active Directory\ndomain for authentication"
            ],
            "Explanation": "Answer: D\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 108,
            "QuestionContent": "A company runs a global web application on Amazon EC2 instances behind an Application Load Balancer\nThe application stores data in Amazon Aurora. The company needs to create a disaster recovery solution and\ncan tolerate up to 30 minutes of downtime and potential data loss. The solution does not need to handle the\nload when the primary infrastructure is healthy\nWhat should a solutions architect do to meet these requirements?",
            "Option": [
                "A.  Deploy the application with the required infrastructure elements in place Use Amazon Route 53 to\nconfigure active-passive failover Create an Aurora Replica in a second AWS Region",
                "B.  Host a scaled-down deployment of the application in a second AWS Region Use Amazon Route 53 to\nconfigure active-active failover Create an Aurora Replica in the second Region",
                "C.  Replicate the primary infrastructure in a second AWS Region Use Amazon Route 53 to configure\nactive-active failover Create an Aurora database that is restored from the latest snapshot",
                "D.  Back up data with AWS Backup Use the backup to create the required infrastructure in a second AWS\nRegion Use Amazon Route 53 to configure active-passive failover Create an Aurora second primary\ninstance in the second Region"
            ],
            "Explanation": "Answer: C\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 109,
            "QuestionContent": "A company is storing sensitive user information in an Amazon S3 bucket The company wants to provide\nsecure access to this bucket from the application tier running on Ama2on EC2 instances inside a VPC.\nWhich combination of steps should a solutions architect take to accomplish this? (Select TWO.)",
            "Option": [
                "A.  Configure a VPC gateway endpoint for Amazon S3 within the VPC",
                "B.  Create a bucket policy to make the objects to the S3 bucket public",
                "C.  Create a bucket policy that limits access to only the application tier running in the VPC",
                "D.  Create an IAM user with an S3 access policy and copy the IAM credentials to the EC2 instance",
                "E.  Create a NAT instance and have the EC2 instances use the NAT instance to access the S3 bucket"
            ],
            "Explanation": "Answer: A C\nExplanation\nhttps://aws.amazon.com/premiumsupport/knowledge-center/s3-private-connection-no-authentication/\n",
            "RightAnswer": [
                "A",
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 110,
            "QuestionContent": "A company recently launched Linux-based application instances on Amazon EC2 in a private subnet and\nlaunched a Linux-based bastion host on an Amazon EC2 instance in a public subnet of a VPC A solutions\narchitect needs to connect from the on-premises network, through the company\u0027s internet connection to the\nbastion host and to the application servers The solutions architect must make sure that the security groups of\nall the EC2 instances will allow that access\nWhich combination of steps should the solutions architect take to meet these requirements? (Select TWO)",
            "Option": [
                "A.  Replace the current security group of the bastion host with one that only allows inbound access from the\napplication instances",
                "B.  Replace the current security group of the bastion host with one that only allows inbound access from the\ninternal IP range for the company",
                "C.  Replace the current security group of the bastion host with one that only allows inbound access from the\nexternal IP range for the company",
                "D.  Replace the current security group of the application instances with one that allows inbound SSH access\nfrom only the private IP address of the bastion host",
                "E.  Replace the current security group of the application instances with one that allows inbound SSH access\nfrom only the public IP address of the bastion host"
            ],
            "Explanation": "Answer: C D\nExplanation\nhttps://digitalcloud.training/ssh-into-ec2-in-private-subnet/\n",
            "RightAnswer": [
                "C",
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 111,
            "QuestionContent": "A company performs monthly maintenance on its AWS infrastructure. During these maintenance activities, the\ncompany needs to rotate the credentials tor its Amazon ROS tor MySQL databases across multiple AWS\nRegions\nWhich solution will meet these requirements with the LEAST operational overhead?",
            "Option": [
                "A.  Store the credentials as secrets in AWS Secrets Manager. Use multi-Region secret replication for the\nrequired Regions Configure Secrets Manager to rotate the secrets on a schedule",
                "B.  Store the credentials as secrets in AWS Systems Manager by creating a secure string parameter Use\nmulti-Region secret replication for the required Regions Configure Systems Manager to rotate the\nsecrets on a schedule",
                "C.  Store the credentials in an Amazon S3 bucket that has server-side encryption (SSE) enabled Use\nAmazon EventBridge (Amazon CloudWatch Events) to invoke an AWS Lambda function to rotate the\ncredentials",
                "D.  Encrypt the credentials as secrets by using AWS Key Management Service (AWS KMS) multi-Region\ncustomer managed keys Store the secrets in an Amazon DynamoDB global table Use an AWS Lambda\nfunction to retrieve the secrets from DynamoDB Use the RDS API to rotate the secrets."
            ],
            "Explanation": "Answer: A\nExplanation\nhttps://aws.amazon.com/blogs/security/how-to-replicate-secrets-aws-secrets-manager-multiple-regions/\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 112,
            "QuestionContent": "A solutions architect is creating a new Amazon CloudFront distribution for an application. Some of the\ninformation submitted by users is sensitive. The application uses HTTPS but needs another layer of security.\nThe sensitive information should be protected throughout the entire application stack, and access to the\ninformation should be restricted to certain applications.\nWhich action should the solutions architect take?",
            "Option": [
                "A.  Configure a CloudFront signed URL.",
                "B.  Configure a CloudFront signed cookie.",
                "C.  Configure a CloudFront field-level encryption profile.",
                "D.  Configure CloudFront and set the Origin Protocol Policy setting to HTTPS Only for the Viewer Protocol\nPolicy."
            ],
            "Explanation": "Answer: C\nExplanation\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html\n\u0022With Amazon CloudFront, you can enforce secure end-to-end connections to origin servers by using HTTPS.\nField-level encryption adds an additional layer of security that lets you protect specific data throughout system\nprocessing so that only certain applications can see it.\u0022\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 113,
            "QuestionContent": "A company uses Amazon S3 to store its confidential audit documents. The S3 bucket uses bucket policies to\nrestrict access to audit team IAM user credentials according to the principle of least privilege. Company\nmanagers are worried about accidental deletion of documents in the S3 bucket and want a more secure\nsolution.\nWhat should a solutions architect do to secure the audit documents?",
            "Option": [
                "A.  Enable the versioning and MFA Delete features on the S3 bucket.",
                "B.  Enable multi-factor authentication (MFA) on the IAM user credentials for each audit team IAM user\naccount.",
                "C.  Add an S3 Lifecycle policy to the audit team\u0027s IAM user accounts to deny the s3:DeleteObject action\nduring audit dates.",
                "D.  Use AWS Key Management Service (AWS KMS) to encrypt the S3 bucket and restrict audit team IAM\nuser accounts from accessing the KMS key."
            ],
            "Explanation": "Answer: A\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 114,
            "QuestionContent": "A company hosts an application on multiple Amazon EC2 instances The application processes messages from\nan Amazon SQS queue writes to an Amazon RDS table and deletes the message from the queue Occasional\nduplicate records are found in the RDS table. The SQS queue does not contain any duplicate messages.\nWhat should a solutions architect do to ensure messages are being processed once only?",
            "Option": [
                "A.  Use the CreateQueue API call to create a new queue",
                "B.  Use the Add Permission API call to add appropriate permissions",
                "C.  Use the ReceiveMessage API call to set an appropriate wail time",
                "D.  Use the ChangeMessageVisibility APi call to increase the visibility timeout"
            ],
            "Explanation": "Answer: D\nExplanation\nThe visibility timeout begins when Amazon SQS returns a message. During this time, the consumer processes\nand deletes the message. However, if the consumer fails before deleting the message and your system doesn\u0027t\ncall the DeleteMessage action for that message before the visibility timeout expires, the message becomes\nvisible to other consumers and the message is received again. If a message must be received only once, your\nconsumer should delete it within the duration of the visibility timeout.\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html\nKeyword: SQS queue writes to an Amazon RDS From this, Option D best suite \u0026amp; other Options ruled out\n[Option A - You can\u0027t intruduce one more Queue in the existing one; Option B - only Permission \u0026amp;\nOption C - Only Retrieves Messages] FIF O queues are designed to never introduce duplicate messages.\nHowever, your message producer might introduce duplicates in certain scenarios: for example, if the producer\nsends a message, does not receive a response, and then resends the same message. Amazon SQS APIs provide\ndeduplication functionality that prevents your message producer from sending duplicates. Any duplicates\nintroduced by the message producer are removed within a 5-minute deduplication interval. For standard\nqueues, you might occasionally receive a duplicate copy of a message (at-least- once delivery). If you use a\nstandard queue, you must design your applications to be idempotent (that is, they must not be affected\nadversely when processing the same message more than once).\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 115,
            "QuestionContent": "A company hosts its multi-tier applications on AWS. For compliance, governance, auditing, and security, the\ncompany must track configuration changes on its AWS resources and record a history of API calls made to\nthese resources.\nWhat should a solutions architect do to meet these requirements?",
            "Option": [
                "A.  Use AWS CloudTrail to track configuration changes and AWS Config to record API calls",
                "B.  Use AWS Config to track configuration changes and AWS CloudTrail to record API calls",
                "C.  Use AWS Config to track configuration changes and Amazon CloudWatch to record API calls",
                "D.  Use AWS CloudTrail to track configuration changes and Amazon CloudWatch to record API calls"
            ],
            "Explanation": "Answer: B\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 116,
            "QuestionContent": "A company that hosts its web application on AWS wants to ensure all Amazon EC2 instances. Amazon RDS\nDB instances. and Amazon Redshift clusters are configured with tags. The company wants to minimize the\neffort of configuring and operating this check.\nWhat should a solutions architect do to accomplish this?",
            "Option": [
                "A.  Use AWS Config rules to define and detect resources that are not properly tagged.",
                "B.  Use Cost Explorer to display resources that are not properly tagged. Tag those resources manually.",
                "C.  Write API calls to check all resources for proper tag allocation. Periodically run the code on an EC2\ninstance.",
                "D.  Write API calls to check all resources for proper tag allocation. Schedule an AWS Lambda function\nthrough Amazon CloudWatch to periodically run the code."
            ],
            "Explanation": "Answer: A\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 117,
            "QuestionContent": "A company runs a stateless web application in production on a group of Amazon EC2 On-Demand Instances\nbehind an Application Load Balancer. The application experiences heavy usage during an 8-hour period each\nbusiness day. Application usage is moderate and steady overnight Application usage is low during weekends.\nThe company wants to minimize its EC2 costs without affecting the availability of the application.\nWhich solution will meet these requirements?",
            "Option": [
                "A.  Use Spot Instances for the entire workload.",
                "B.  Use Reserved instances for the baseline level of usage Use Spot Instances for any additional capacity\nthat the application needs.",
                "C.  Use On-Demand Instances for the baseline level of usage. Use Spot Instances for any additional\ncapacity that the application needs",
                "D.  Use Dedicated Instances for the baseline level of usage. Use On-Demand Instances for any additional\ncapacity that the application needs"
            ],
            "Explanation": "Answer: B\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 118,
            "QuestionContent": "A company wants to direct its users to a backup static error page if the company\u0027s primary website is\nunavailable. The primary website\u0027s DNS records are hosted in Amazon Route 53. The domain is pointing to an\nApplication Load Balancer (ALB). The company needs a solution that minimizes changes and infrastructure\noverhead.\nWhich solution will meet these requirements?",
            "Option": [
                "A.  Update the Route 53 records to use a latency routing policy. Add a static error page that is hosted in an\nAmazon S3\nbucket to the records so that the traffic is sent to the most responsive endpoints.",
                "B.  Set up a Route 53 active-passive failover configuration. Direct traffic to a static error page that is hosted\nin an\nAmazon S3 bucket when Route 53 health checks determine that the ALB endpoint is unhealthy.",
                "C.  Set up a Route 53 active-active configuration with the ALB and an Amazon EC2 instance that hosts a\nstatic error\npage as endpoints. Configure Route 53 to send requests to the instance only if the health checks fail for\nthe ALB.",
                "D.  Update the Route 53 records to use a multivalue answer routing policy. Create a health check. Direct\ntraffic to the\nwebsite if the health check passes. Direct traffic to a static error page that is hosted in Amazon S3 if the\nhealth check does not pass."
            ],
            "Explanation": "Answer: B\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 119,
            "QuestionContent": "A solutions architect is designing a VPC with public and private subnets. The VPC and subnets use IPv4\nCIDR blocks. There is one public subnet and one private subnet in each of three Availability Zones (AZs) for\nhigh availability. An internet gateway is used to provide internet access for the public subnets. The private\nsubnets require access to the internet to allow Amazon EC2 instances to download software updates.\nWhat should the solutions architect do to enable Internet access for the private subnets?",
            "Option": [
                "A.  Create three NAT gateways, one for each public subnet in each AZ. Create a private route table for each\nAZ that forwards non-VPC traffic to the NAT gateway in its AZ.",
                "B.  Create three NAT instances, one for each private subnet in each AZ. Create a private route table for each\nAZ that forwards non-VPC traffic to the NAT instance in its AZ.",
                "C.  Create a second internet gateway on one of the private subnets. Update the route table for the private\nsubnets that forward non-VPC traffic to the private internet gateway.",
                "D.  Create an egress-only internet gateway on one of the public subnets. Update the route table for the\nprivate subnets that forward non-VPC traffic to the egress- only internet gateway."
            ],
            "Explanation": "Answer: A\nExplanation\nhttps://aws.amazon.com/about-aws/whats-new/2018/03/introducing-amazon-vpc-nat-gateway-in-the-aws-govcloud-us-region/#:~:text=NAT%20Gateway%20is%20a%20highly,instances%20in%20a%20private%20subnet.\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 120,
            "QuestionContent": "A company runs a highly available image-processing application on Amazon EC2 instances in a single VPC\nThe EC2 instances run inside several subnets across multiple Availability Zones. The EC2 instances do not\ncommunicate with each other However, the EC2 instances download images from Amazon S3 and upload\nimages to Amazon S3 through a single NAT gateway The company is concerned about data transfer charges\nWhat is the MOST cost-effective way for the company to avoid Regional data transfer charges?",
            "Option": [
                "A.  Launch the NAT gateway in each Availability Zone",
                "B.  Replace the NAT gateway with a NAT instance",
                "C.  Deploy a gateway VPC endpoint for Amazon S3",
                "D.  Provision an EC2 Dedicated Host to run the EC2 instances"
            ],
            "Explanation": "Answer: A\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 121,
            "QuestionContent": "A company\u0027s application Is having performance issues The application staleful and needs to complete\nm-memory tasks on Amazon EC2 instances. The company used AWS CloudFormation to deploy\ninfrastructure and used the M5 EC2 Instance family As traffic increased, the application performance degraded\nUsers are reporting delays when the users attempt to access the application.\nWhich solution will resolve these issues in the MOST operationally efficient way?",
            "Option": [
                "A.  Replace the EC2 Instances with T3 EC2 instances that run in an Auto Scaling group. Made the changes\nby using the AWS Management Console.",
                "B.  Modify the CloudFormation templates to run the EC2 instances in an Auto Scaling group. Increase the\ndesired capacity and the maximum capacity of the Auto Scaling group manually when an increase is\nnecessary",
                "C.  Modify the CloudFormation templates. Replace the EC2 instances with R5 EC2 instances. Use Amazon\nCloudWatch built-in EC2 memory metrics to track the application performance for future capacity\nplanning.",
                "D.  Modify the CloudFormation templates. Replace the EC2 instances with R5 EC2 instances. Deploy the\nAmazon CloudWatch agent on the EC2 instances to generate custom application latency metrics for\nfuture capacity planning."
            ],
            "Explanation": "Answer: D\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 122,
            "QuestionContent": "A company is building an ecommerce web application on AWS. The application sends information about new\norders to an Amazon API Gateway REST API to process. The company wants to ensure that orders are\nprocessed in the order that they are received.\nWhich solution will meet these requirements?",
            "Option": [
                "A.  Use an API Gateway integration to publish a message to an Amazon Simple Notification Service\n(Amazon SNS) topic when the application receives an order. Subscribe an AWS Lambda function to the\ntopic to perform processing.",
                "B.  Use an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS)\nFIFO queue when the application receives an order. Configure the SQS FIFO queue to invoke an AWS\nLambda function for processing.",
                "C.  Use an API Gateway authorizer to block any requests while the application processes an order.",
                "D.  Use an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS)\nstandard queue when the application receives an order. Configure the SQS standard queue to invoke an\nAWS Lambda function for processing."
            ],
            "Explanation": "Answer: B\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 123,
            "QuestionContent": "A company needs to store its accounting records in Amazon S3. The records must be immediately accessible\nfor 1 year and then must be archived for an additional 9 years. No one at the company, including\nadministrative users and root users, can be able to delete the records during the entire 10-year period. The\nrecords must be stored with maximum resiliency.\nWhich solution will meet these requirements?",
            "Option": [
                "A.  Store the records in S3 Glacier for the entire 10-year period. Use an access control policy to deny\ndeletion of the records for a period of 10 years.",
                "B.  Store the records by using S3 Intelligent-Tiering. Use an IAM policy to deny deletion of the records.\nAfter 10 years, change the IAM policy to allow deletion.",
                "C.  Use an S3 Lifecycle policy to transition the records from S3 Standard to S3 Glacier Deep Archive after\n1 year. Use S3 Object Lock in compliance mode for a period of 10 years.",
                "D.  Use an S3 Lifecycle policy to transition the records from S3 Standard to S3 One Zone-Infrequent\nAccess (S3 One Zone-IA) after 1 year. Use S3 Object Lock in governance mode for a period of 10\nyears."
            ],
            "Explanation": "Answer: C\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 124,
            "QuestionContent": "A company is migrating applications to AWS. The applications are deployed in different accounts. The\ncompany manages the accounts centrally by using AWS Organizations. The company\u0027s security team needs a\nsingle sign-on (SSO) solution across all the company\u0027s accounts. The company must continue managing the\nusers and groups in its on-premises self-managed Microsoft Active Directory.\nWhich solution will meet these requirements?",
            "Option": [
                "A.  Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console. Create a one-way forest trust or\na one-way domain trust to connect the company\u0027s self-managed Microsoft Active Directory with AWS\nSSO by using AWS Directory Service for Microsoft Active Directory.",
                "B.  Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console. Create a two-way forest trust to\nconnect the company\u0027s self-managed Microsoft Active Directory with AWS SSO by using AWS\nDirectory Service for Microsoft Active Directory.",
                "C.  Use AWS Directory Service. Create a two-way trust relationship with the company\u0027s self-managed\nMicrosoft Active Directory.",
                "D.  Deploy an identity provider (IdP) on premises. Enable AWS Single Sign-On (AWS SSO) from the\nAWS SSO console."
            ],
            "Explanation": "Answer: A\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 125,
            "QuestionContent": "A company has an on-premises application that generates a large amount of time-sensitive data that is backed\nup to Amazon S3. The application has grown and there are user complaints about internet bandwidth\nlimitations. A solutions architect needs to design a long-term solution that allows for both timely backups to\nAmazon S3 and with minimal impact on internet connectivity for internal users.\nWhich solution meets these requirements?",
            "Option": [
                "A.  Establish AWS VPN connections and proxy all traffic through a VPC gateway endpoint",
                "B.  Establish a new AWS Direct Connect connection and direct backup traffic through this new connection.",
                "C.  Order daily AWS Snowball devices Load the data onto the Snowball devices and return the devices to\nAWS each day.",
                "D.  Submit a support ticket through the AWS Management Console Request the removal of S3 service\nlimits from the account."
            ],
            "Explanation": "Answer: B\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 126,
            "QuestionContent": "A company has applications that run on Amazon EC2 instances in a VPC. One of the applications needs to call\nthe Amazon S3 API to store and read objects. According to the company\u0027s security regulations, no traffic from\nthe applications is allowed to travel across the internet.\nWhich solution will meet these requirements?",
            "Option": [
                "A.  Configure an S3 interface endpoint.",
                "B.  Configure an S3 gateway endpoint.",
                "C.  Create an S3 bucket in a private subnet.",
                "D.  Create an S3 bucket in the same Region as the EC2 instance."
            ],
            "Explanation": "Answer: B\nExplanation\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/privatelink-interface-endpoints.html#types-of-vpc-endpoints-for-s3\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 127,
            "QuestionContent": "A survey company has gathered data for several years from areas m\\ the United States. The company hosts the\ndata in an Amazon S3 bucket that is 3 TB m size and growing. The company has started to share the data with\na European marketing firm that has S3 buckets The company wants to ensure that its data transfer costs remain\nas low as possible\nWhich solution will meet these requirements?",
            "Option": [
                "A.  Configure the Requester Pays feature on the company\u0027s S3 bucket",
                "B.  Configure S3 Cross-Region Replication from the company\u2019s S3 bucket to one of the marketing firm\u0027s S3\nbuckets.",
                "C.  Configure cross-account access for the marketing firm so that the marketing firm has access to the\ncompany\u2019s S3 bucket.",
                "D.  Configure the company\u2019s S3 bucket to use S3 Intelligent-Tiering Sync the S3 bucket to one of the\nmarketing firm\u2019s S3 buckets"
            ],
            "Explanation": "Answer: A\nExplanation\n\u0022Typically, you configure buckets to be Requester Pays buckets when you want to share data but not incur\ncharges associated with others accessing the data. For example, you might use Requester Pays buckets when\nmaking available large datasets, such as zip code directories, reference data, geospatial information, or web\ncrawling data.\u0022 https://docs.aws.amazon.com/AmazonS3/latest/userguide/RequesterPaysBuckets.html\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 128,
            "QuestionContent": "A company stores call transcript files on a monthly basis. Users access the files randomly within 1 year of the\ncall, but users access the files infrequently after 1 year. The company wants to optimize its solution by giving\nusers the ability to query and retrieve files that are less than 1-year-old as quickly as possible. A delay in\nretrieving older files is acceptable.\nWhich solution will meet these requirements MOST cost-effectively?",
            "Option": [
                "A.  Store individual files with tags in Amazon S3 Glacier Instant Retrieval. Query the tags to retrieve the\nfiles from S3 Glacier Instant Retrieval.",
                "B.  Store individual files in Amazon S3 Intelligent-Tiering. Use S3 Lifecycle policies to move the files to\nS3 Glacier Flexible Retrieval after 1 year. Query and retrieve the files that are in Amazon S3 by using\nAmazon Athena. Query and retrieve the files that are in S3 Glacier by using S3 Glacier Select.",
                "C.  Store individual files with tags in Amazon S3 Standard storage. Store search metadata for each archive\nin Amazon S3 Standard storage. Use S3 Lifecycle policies to move the files to S3 Glacier Instant\nRetrieval after 1 year. Query and retrieve the files by searching for metadata from Amazon S3.",
                "D.  Store individual files in Amazon S3 Standard storage. Use S3 Lifecycle policies to move the files to S3\nGlacier Deep Archive after 1 year. Store search metadata in Amazon RDS. Query the files from\nAmazon RDS. Retrieve the files from S3 Glacier Deep Archive."
            ],
            "Explanation": "Answer: B\nExplanation\n\u0022For archive data that needs immediate access, such as medical images, news media assets, or genomics data,\nchoose the S3 Glacier Instant Retrieval storage class, an archive storage class that delivers the lowest cost\nstorage with milliseconds retrieval. For archive data that does not require immediate access but needs the\nflexibility to retrieve large sets of data at no cost, such as backup or disaster recovery use cases, choose S3\nGlacier Flexible Retrieval (formerly S3 Glacier), with retrieval in minutes or free bulk retrievals in 5-12\nhours.\u0022\nhttps://aws.amazon.com/about-aws/whats-new/2021/11/amazon-s3-glacier-instant-retrieval-storage-class/\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 129,
            "QuestionContent": "A rapidly growing ecommerce company is running its workloads in a single AWS Region. A solutions\narchitect must create a disaster recovery (DR) strategy that includes a dilferenl AWS Region The company\nwants its database to be up to date in the DR Region with the least possible latency The remaining\ninfrastructure in the DR Region needs to run at reduced capecrty and must be able to scale up it necessary\nWhich solution will meel these requirements with the LOWEST recovery time objective (RTO)?",
            "Option": [
                "A.  Use an Amazon Aurora global database with a pilot light deployment",
                "B.  Use an Amazon Aurora global database with a warm standby deployment",
                "C.  Use an Amazon RDS Multi-AZ DB instance wilh a pilot light deployment",
                "D.  Use an Amazon RDS Multi-AZ DB instance with a warm standby deployment"
            ],
            "Explanation": "Answer: B\nExplanation\nhttps://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options-in-the-cloud.html\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 130,
            "QuestionContent": "A company owns an asynchronous API that is used to ingest user requests and, based on the request type,\ndispatch requests to the appropriate microservice for processing. The company is using Amazon API Gateway\nto deploy the API front end, and an AWS Lambda function that invokes Amazon DynamoDB to store user\nrequests before dispatching them to the processing microservices.\nThe company provisioned as much DynamoDB throughput as its budget allows, but the company is still\nexperiencing availability issues and is losing user requests.\nWhat should a solutions architect do to address this issue without impacting existing users?",
            "Option": [
                "A.  Add throttling on the API Gateway with server-side throttling limits.",
                "B.  Use DynamoDB Accelerator (DAX) and Lambda to buffer writes to DynamoDB.",
                "C.  Create a secondary index in DynamoDB for the table with the user requests.",
                "D.  Use the Amazon Simple Queue Service (Amazon SQS) queue and Lambda to buffer writes to\nDynamoDB."
            ],
            "Explanation": "Answer: D\nExplanation\nbecause all other options put some more charges to DynamoDB. But the company supplied as much as they\ncan for DynamoDB. And it is async request and we need to have retry mechanism not to lose the customer\ndata.\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 131,
            "QuestionContent": "A company is running an SMB file server in its data center. The file server stores large files that are accessed\nfrequently for the first few days after the files are created. After 7 days the files are rarely accessed.\nThe total data size is increasing and is close to the company\u0027s total storage capacity. A solutions architect must\nincrease the company\u0027s available storage space without losing low-latency access to the most recently accessed\nfiles. The solutions architect must also provide file lifecycle management to avoid future storage issues.\nWhich solution will meet these requirements?",
            "Option": [
                "A.  Use AWS DataSync to copy data that is older than 7 days from the SMB file server to AWS.",
                "B.  Create an Amazon S3 File Gateway to extend the company\u0027s storage space. Create an S3 Lifecycle\npolicy to transition the data to S3 Glacier Deep Archive after 7 days.",
                "C.  Create an Amazon FSx for Windows File Server file system to extend the company\u0027s storage space.",
                "D.  Install a utility on each user\u0027s computer to access Amazon S3. Create an S3 Lifecycle policy to transition\nthe data to S3 Glacier Flexible Retrieval after 7 days."
            ],
            "Explanation": "Answer: D\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 132,
            "QuestionContent": "Organizers for a global event want to put daily reports online as static HTML pages. The pages are expected to\ngenerate millions of views from users around the world. The files are stored In an Amazon S3 bucket. A\nsolutions architect has been asked to design an efficient and effective solution.\nWhich action should the solutions architect take to accomplish this?",
            "Option": [
                "A.  Generate presigned URLs for the files.",
                "B.  Use cross-Region replication to all Regions.",
                "C.  Use the geoproximtty feature of Amazon Route 53.",
                "D.  Use Amazon CloudFront with the S3 bucket as its origin."
            ],
            "Explanation": "Answer: D\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 133,
            "QuestionContent": "A solutions architect is designing the architecture of a new application being deployed to the AWS Cloud. The\napplication will run on Amazon EC2 On-Demand Instances and will automatically scale across multiple\nAvailability Zones. The EC2 instances will scale up and down frequently throughout the day. An Application\nLoad Balancer (ALB) will handle the load distribution. The architecture needs to support distributed session\ndata management. The company is willing to make changes to code if needed.\nWhat should the solutions architect do to ensure that the architecture supports distributed session data\nmanagement?",
            "Option": [
                "A.  Use Amazon ElastiCache to manage and store session data.",
                "B.  Use session affinity (sticky sessions) of the ALB to manage session data.",
                "C.  Use Session Manager from AWS Systems Manager to manage the session.",
                "D.  Use the GetSessionToken API operation in AWS Security Token Service (AWS STS) to manage the\nsession"
            ],
            "Explanation": "Answer: A\nExplanation\nhttps://aws.amazon.com/vi/caching/session-management/\nIn order to address scalability and to provide a shared data storage for sessions that can be accessible from any\nindividual web server, you can abstract the HTTP sessions from the web servers themselves. A common\nsolution to for this is to leverage an In-Memory Key/Value store such as Redis and Memcached. ElastiCache\nofferings for In-Memory key/value stores include ElastiCache for Redis, which can support replication, and\nElastiCache for Memcached which does not support replication.\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 134,
            "QuestionContent": "A company wants to move its application to a serverless solution. The serverless solution needs to analyze\nexisting and new data by using SL. The company stores the data in an Amazon S3 bucket. The data requires\nencryption and must be replicated to a different AWS Region.\nWhich solution will meet these requirements with the LEAST operational overhead?",
            "Option": [
                "A.  Create a new S3 bucket. Load the data into the new S3 bucket. Use S3 Cross-Region Replication (CRR)\nto replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption with AWS\nKMS multi-Region kays (SSE-KMS). Use Amazon Athena to query the data.",
                "B.  Create a new S3 bucket. Load the data into the new S3 bucket. Use S3 Cross-Region Replication (CRR)\nto replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption with AWS\nKMS multi-Region keys (SSE-KMS). Use Amazon RDS to query the data.",
                "C.  Load the data into the existing S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate\nencrypted objects to an S3 bucket in another Region. Use server-side encryption with Amazon S3\nmanaged encryption keys (SSE-S3). Use Amazon Athena to query the data.",
                "D.  Load the data into the existing S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate\nencrypted objects to an S3 bucket in another Region. Use server-side encryption with Amazon S3\nmanaged encryption keys (SSE-S3). Use Amazon RDS to query the data."
            ],
            "Explanation": "Answer: A\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 135,
            "QuestionContent": "A company receives 10 TB of instrumentation data each day from several machines located at a single factory.\nThe data consists of JSON files stored on a storage area network (SAN) in an on-premises data center located\nwithin the factory. The company wants to send this data to Amazon S3 where it can be accessed by several\nadditional systems that provide critical near-real-lime analytics. A secure transfer is important because the data\nis considered sensitive.\nWhich solution offers the MOST reliable data transfer?",
            "Option": [
                "A.  AWS DataSync over public internet",
                "B.  AWS DataSync over AWS Direct Connect",
                "C.  AWS Database Migration Service (AWS DMS) over public internet",
                "D.  AWS Database Migration Service (AWS DMS) over AWS Direct Connect"
            ],
            "Explanation": "Answer: B\nExplanation\nThese are some of the main use cases for AWS DataSync: \u2022 Data migration \u2013 Move active datasets rapidly\nover the network into Amazon S3, Amazon EFS, or FSx for Windows File Server. DataSync includes\nautomatic encryption and data integrity validation to help make sure that your data arrives securely, intact, and\nready to use.\n\u0022DataSync includes encryption and integrity validation to help make sure your data arrives securely, intact,\nand ready to use.\u0022 https://aws.amazon.com/datasync/faqs/\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 136,
            "QuestionContent": "A company has an automobile sales website that stores its listings in a database on Amazon RDS When an\nautomobile is sold the listing needs to be removed from the website and the data must be sent to multiple\ntarget systems.\nWhich design should a solutions architect recommend?",
            "Option": [
                "A.  Create an AWS Lambda function triggered when the database on Amazon RDS is updated to send the\ninformation to an Amazon Simple Queue Service (Amazon SQS\u003E queue for the targets to consume",
                "B.  Create an AWS Lambda function triggered when the database on Amazon RDS is updated to send the\ninformation to an Amazon Simple Queue Service (Amazon SQS) FIFO queue for the targets to consume",
                "C.  Subscribe to an RDS event notification and send an Amazon Simple Queue Service (Amazon SQS)\nqueue fanned out to multiple Amazon Simple Notification Service (Amazon SNS) topics Use AWS\nLambda functions to update the targets",
                "D.  Subscribe to an RDS event notification and send an Amazon Simple Notification Service (Amazon\nSNS) topic fanned out to multiple Amazon Simple Queue Service (Amazon SQS) queues Use AWS\nLambda functions to update the targets"
            ],
            "Explanation": "Answer: D\nExplanation\nhttps://docs.aws.amazon.com/lambda/latest/dg/services-rds.html\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-sns.html\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 137,
            "QuestionContent": "A company has implemented a self-managed DNS solution on three Amazon EC2 instances behind a Network\nLoad Balancer (NLB) in the us-west-2 Region. Most of the company\u0027s users are located in the United States\nand Europe. The company wants to improve the performance and availability of the solution. The company\nlaunches and configures three EC2 instances in the eu-west-1 Region and adds the EC2 instances as targets for\na new NLB.\nWhich solution can the company use to route traffic to all the EC2 instances?",
            "Option": [
                "A.  Create an Amazon Route 53 geolocation routing policy to route requests to one of the two NLBs. Create\nan Amazon CloudFront distribution. Use the Route 53 record as the distribution\u0027s origin.",
                "B.  Create a standard accelerator in AWS Global Accelerator. Create endpoint groups in us-west-2 and\neu-west-1. Add the two NLBs as endpoints for the endpoint groups.",
                "C.  Attach Elastic IP addresses to the six EC2 instances. Create an Amazon Route 53 geolocation routing\npolicy to route requests to one of the six EC2 instances. Create an Amazon CloudFront distribution. Use\nthe Route 53 record as the distribution\u0027s origin.",
                "D.  Replace the two NLBs with two Application Load Balancers (ALBs). Create an Amazon Route 53\nlatency routing policy to route requests to one of the two ALBs. Create an Amazon CloudFront\ndistribution. Use the Route 53 record as the distribution\u0027s origin."
            ],
            "Explanation": "Answer: B\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 138,
            "QuestionContent": "A company runs a high performance computing (HPC) workload on AWS. The workload required low-latency\nnetwork performance and high network throughput with tightly coupled node-to-node communication. The\nAmazon EC2 instances are properly sized for compute and storage capacity, and are launched using default\noptions.\nWhat should a solutions architect propose to improve the performance of the workload?",
            "Option": [
                "A.  Choose a cluster placement group while launching Amazon EC2 instances.",
                "B.  Choose dedicated instance tenancy while launching Amazon EC2 instances.",
                "C.  Choose an Elastic Inference accelerator while launching Amazon EC2 instances.",
                "D.  Choose the required capacity reservation while launching Amazon EC2 instances."
            ],
            "Explanation": "Answer: A\nExplanation\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-ec2-placementgroup.html\n\u0022A cluster placement group is a logical grouping of instances within a single Availability Zone that benefit\nfrom low network latency, high network throughput\u0022\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 139,
            "QuestionContent": "A solutions architect is designing the cloud architecture for a new application being deployed on AWS. The\nprocess should run in parallel while adding and removing application nodes as needed based on the number of\njobs to be processed. The processor application is stateless. The solutions architect must ensure that the\napplication is loosely coupled and the job items are durably stored.\nWhich design should the solutions architect use?",
            "Option": [
                "A.  Create an Amazon SNS topic to send the jobs that need to be processed Create an Amazon Machine\nImage (AMI) that consists of the processor application Create a launch configuration that uses the AMI\nCreate an Auto Scaling group using the launch configuration Set the scaling policy for the Auto Scaling\ngroup to add and remove nodes based on CPU usage",
                "B.  Create an Amazon SQS queue to hold the jobs that need to be processed Create an Amazon Machine\nimage (AMI) that consists of the processor application Create a launch configuration that uses the AM\u0027\nCreate an Auto Scaling group using the launch configuration Set the scaling policy for the Auto Scaling\ngroup to add and remove nodes based on network usage",
                "C.  Create an Amazon SQS queue to hold the jobs that needs to be processed Create an Amazon Machine\nimage (AMI) that consists of the processor application Create a launch template that uses the AMI\nCreate an Auto Scaling group using the launch template Set the scaling policy for the Auto Scaling\ngroup to add and remove nodes based on the number of items in the SQS queue",
                "D.  Create an Amazon SNS topic to send the jobs that need to be processed Create an Amazon Machine\nImage (AMI) that consists of the processor application Create a launch template that uses the AMI\nCreate an Auto Scaling group using the launch template Set the scaling policy for the Auto Scaling\ngroup to add and remove nodes based on the number of messages published to the SNS topic"
            ],
            "Explanation": "Answer: C\nExplanation\n\u0022Create an Amazon SQS queue to hold the jobs that needs to be processed. Create an Amazon EC2 Auto\nScaling group for the compute application. Set the scaling policy for the Auto Scaling group to add and\nremove nodes based on the number of items in the SQS queue\u0022\nIn this case we need to find a durable and loosely coupled solution for storing jobs. Amazon SQS is ideal for\nthis use case and can be configured to use dynamic scaling based on the number of jobs waiting in the\nqueue.To configure this scaling you can use the backlog per instance metric with the target value being the\nacceptable backlog per instance to maintain. You can calculate these numbers as follows: Backlog per\ninstance: To calculate your backlog per instance, start with the ApproximateNumberOfMessages queue\nattribute to determine the length of the SQS queue\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 140,
            "QuestionContent": "A company needs to save the results from a medical trial to an Amazon S3 repository. The repository must\nallow a few scientists to add new files and must restrict all other users to read-only access. No users can have\nthe ability to modify or delete any files in the repository. The company must keep every file in the repository\nfor a minimum of 1 year after its creation date.\nWhich solution will meet these requirements?",
            "Option": [
                "A.  Use S3 Object Lock In governance mode with a legal hold of 1 year",
                "B.  Use S3 Object Lock in compliance mode with a retention period of 365 days.",
                "C.  Use an IAM role to restrict all users from deleting or changing objects in the S3 bucket Use an S3\nbucket policy to only allow the IAM role",
                "D.  Configure the S3 bucket to invoke an AWS Lambda function every tune an object is added Configure\nthe function to track the hash of the saved object to that modified objects can be marked accordingly"
            ],
            "Explanation": "Answer: C\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 141,
            "QuestionContent": "A company\u0027s containerized application runs on an Amazon EC2 instance. The application needs to download\nsecurity certificates before it can communicate with other business applications. The company wants a highly\nsecure solution to encrypt and decrypt the certificates in near real time. The solution also needs to store data in\nhighly available storage after the data is encrypted.\nWhich solution will meet these requirements with the LEAST operational overhead?",
            "Option": [
                "A.  Create AWS Secrets Manager secrets for encrypted certificates. Manually update the certificates as\nneeded. Control access to the data by using fine-grained IAM access.",
                "B.  Create an AWS Lambda function that uses the Python cryptography library to receive and perform\nencryption operations. Store the function in an Amazon S3 bucket.",
                "C.  Create an AWS Key Management Service (AWS KMS) customer managed key. Allow the EC2 role to\nuse the KMS key for encryption operations. Store the encrypted data on Amazon S3.",
                "D.  Create an AWS Key Management Service (AWS KMS) customer managed key. Allow the EC2 role to\nuse the KMS key for encryption operations. Store the encrypted data on Amazon Elastic Block Store\n(Amazon EBS) volumes."
            ],
            "Explanation": "Answer: D\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 142,
            "QuestionContent": "A company recently launched a variety of new workloads on Amazon EC2 instances in its AWS account. The\ncompany needs to create a strategy to access and administer the instances remotely and securely. The company\nneeds to implement a repeatable process that works with native AWS services and follows the AWS\nWell-Architected Framework.\nWhich solution will meet these requirements with the LEAST operational overhead?",
            "Option": [
                "A.  Use the EC2 serial console to directly access the terminal interface of each instance for administration.",
                "B.  Attach the appropriate IAM role to each existing instance and new instance. Use AWS Systems\nManager Session Manager to establish a remote SSH session.",
                "C.  Create an administrative SSH key pair. Load the public key into each EC2 instance. Deploy a bastion\nhost in a public subnet to provide a tunnel for administration of each instance.",
                "D.  Establish an AWS Site-to-Site VPN connection. Instruct administrators to use their local on-premises\nmachines to connect directly to the instances by using SSH keys across the VPN tunnel."
            ],
            "Explanation": "Answer: B\nExplanation\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/setup-launch-managed-instance.html\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 143,
            "QuestionContent": "A company runs its two-tier ecommerce website on AWS. The web tier consists of a load balancer that sends\ntraffic to Amazon EC2 instances. The database tier uses an Amazon RDS DB instance. The EC2 instances and\nthe RDS DB instance should not be exposed to the public internet. The EC2 instances require internet access\nto complete payment processing of orders through a third-party web service. The application must be highly\navailable.\nWhich combination of configuration options will meet these requirements? (Choose two.)",
            "Option": [
                "A.  Use an Auto Scaling group to launch the EC2 instances in private subnets. Deploy an RDS Multi-AZ\nDB instance in private subnets.",
                "B.  Configure a VPC with two private subnets and two NAT gateways across two Availability Zones.\nDeploy an Application Load Balancer in the private subnets.",
                "C.  Use an Auto Scaling group to launch the EC2 instances in public subnets across two Availability Zones.\nDeploy an RDS Multi-AZ DB instance in private subnets.",
                "D.  Configure a VPC with one public subnet, one private subnet, and two NAT gateways across two\nAvailability Zones. Deploy an Application Load Balancer in the public subnet.",
                "E.  Configure a VPC with two public subnets, two private subnets, and two NAT gateways across two\nAvailability Zones. Deploy an Application Load Balancer in the public subnets."
            ],
            "Explanation": "Answer: A E\nExplanation\nBefore you begin: Decide which two Availability Zones you will use for your EC2 instances. Configure your\nvirtual private cloud (VPC) with at least one public subnet in each of these Availability Zones. These public\nsubnets are used to configure the load balancer. You can launch your EC2 instances in other subnets of these\nAvailability Zones instead.\n",
            "RightAnswer": [
                "A",
                "E"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 144,
            "QuestionContent": "A company is storing backup files by using Amazon S3 Standard storage. The files are accessed frequently for\n1 month. However, the files are not accessed after 1 month. The company must keep the files indefinitely.\nWhich storage solution will meet these requirements MOST cost-effectively?",
            "Option": [
                "A.  Configure S3 Intelligent-Tiering to automatically migrate objects.",
                "B.  Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Glacier Deep Archive\nafter 1 month.",
                "C.  Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Standard-Infrequent\nAccess (S3 Standard-IA) after 1 month.",
                "D.  Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 One Zone-Infrequent\nAccess (S3 One Zone-IA) after 1 month."
            ],
            "Explanation": "Answer: B\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 145,
            "QuestionContent": "A company\u0027s dynamic website is hosted using on-premises servers in the United States. The company is\nlaunching its product in Europe, and it wants to optimize site loading times for new European users. The site\u0027s\nbackend must remain in the United States. The product is being launched in a few days, and an immediate\nsolution is needed.\nWhat should the solutions architect recommend?",
            "Option": [
                "A.  Launch an Amazon EC2 instance in us-east-1 and migrate the site to it.",
                "B.  Move the website to Amazon S3. Use cross-Region replication between Regions.",
                "C.  Use Amazon CloudFront with a custom origin pointing to the on-premises servers.",
                "D.  Use an Amazon Route 53 geo-proximity routing policy pointing to on-premises servers."
            ],
            "Explanation": "Answer: C\nExplanation\nhttps://aws.amazon.com/pt/blogs/aws/amazon-cloudfront-support-for-custom-origins/\nYou can now create a CloudFront distribution using a custom origin. Each distribution will can point to an S3\nor to a custom origin. This could be another storage service, or it could be something more interesting and\nmore dynamic, such as an EC2 instance or even an Elastic Load Balancer\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 146,
            "QuestionContent": "A solutions architect is optimizing a website for an upcoming musical event. Videos of the performances will\nbe streamed in real time and then will be available on demand. The event is expected to attract a global online\naudience.\nWhich service will improve the performance of both the real-lime and on-demand streaming?",
            "Option": [
                "A.  Amazon CloudFront",
                "B.  AWS Global Accelerator",
                "C.  Amazon Route 53",
                "D.  Amazon S3 Transfer Acceleration"
            ],
            "Explanation": "Answer: A\nExplanation\nYou can use CloudFront to deliver video on demand (VOD) or live streaming video using any HTTP origin.\nOne way you can set up video workflows in the cloud is by using CloudFront together with AWS Media\nServices.\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/on-demand-streaming-video.html\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 147,
            "QuestionContent": "A company wants to manage Amazon Machine Images (AMIs). The company currently copies AMIs to the\nsame AWS Region where the AMIs were created. The company needs to design an application that captures\nAWS API calls and sends alerts whenever the Amazon EC2 Createlmage API operation is called within the\ncompany\u0027s account.\nWhich solution will meet these requirements with the LEAST operational overhead?",
            "Option": [
                "A.  Create an AWS Lambda function to query AWS CloudTrail logs and to send an alert when a\nCreatelmage API call is detected.",
                "B.  Configure AWS CloudTrail with an Amazon Simple Notification Service {Amazon SNS) notification\nthat occurs when updated logs are sent to Amazon S3. Use Amazon Athena to create a new table and to\nquery on Createlmage when an API call is detected.",
                "C.  Create an Amazon EventBridge (Amazon CloudWatch Events) rule for the Createlmage API call.\nConfigure the target as an Amazon Simple Notification Service (Amazon SNS) topic to send an alert\nwhen a Createlmage API call is detected.",
                "D.  Configure an Amazon Simple Queue Service (Amazon SQS) FIFO queue as a target for AWS\nCloudTrail logs. Create an AWS Lambda function to send an alert to an Amazon Simple Notification\nService (Amazon SNS) topic when a Createlmage API call is detected."
            ],
            "Explanation": "Answer: B\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 148,
            "QuestionContent": "A company uses AWS Organizations to manage multiple AWS accounts for different departments. The\nmanagement account has an Amazon S3 bucket that contains project reports. The company wants to limit\naccess to this S3 bucket to only users of accounts within the organization in AWS Organizations.\nWhich solution meets these requirements with the LEAST amount of operational overhead?",
            "Option": [
                "A.  Add the aws:PrincipalOrgID global condition key with a reference to the organization ID to the S3\nbucket policy.",
                "B.  Create an organizational unit (OU) for each department. Add the aws:PrincipalOrgPaths global\ncondition key to the S3 bucket policy.",
                "C.  Use AWS CloudTrail to monitor the CreateAccount, InviteAccountToOrganization, LeaveOrganization,\nand RemoveAccountFromOrganization events. Update the S3 bucket policy accordingly.",
                "D.  Tag each user that needs access to the S3 bucket. Add the aws:PrincipalTag global condition key to the\nS3 bucket policy."
            ],
            "Explanation": "Answer: A\nExplanation\nhttps://aws.amazon.com/blogs/security/control-access-to-aws-resources-by-using-the-aws-organization-of-iam-principals/\nThe aws:PrincipalOrgID global key provides an alternative to listing all the account IDs for all AWS accounts\nin an organization. For example, the following Amazon S3 bucket policy allows members of any account in\nthe XXX organization to add an object into the examtopics bucket.\n{\u0022Version\u0022: \u00222020-09-10\u0022,\n\u0022Statement\u0022: {\n\u0022Sid\u0022: \u0022AllowPutObject\u0022,\n\u0022Effect\u0022: \u0022Allow\u0022,\n\u0022Principal\u0022: \u0022*\u0022,\n\u0022Action\u0022: \u0022s3:PutObject\u0022,\n\u0022Resource\u0022: \u0022arn:aws:s3:::examtopics/*\u0022,\n\u0022Condition\u0022: {\u0022StringEquals\u0022:\n{\u0022aws:PrincipalOrgID\u0022:[\u0022XXX\u0022]}}}}\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 149,
            "QuestionContent": "An application allows users at a company\u0027s headquarters to access product data. The product data is stored in\nan Amazon RDS MySQL DB instance. The operations team has isolated an application performance\nslowdown and wants to separate read traffic from write traffic. A solutions architect needs to optimize the\napplication\u0027s performance quickly.\nWhat should the solutions architect recommend?",
            "Option": [
                "A.  Change the existing database to a Multi-AZ deployment. Serve the read requests from the primary\nAvailability Zone.",
                "B.  Change the existing database to a Multi-AZ deployment. Serve the read requests from the secondary\nAvailability Zone.",
                "C.  Create read replicas for the database. Configure the read replicas with half of the compute and storage\nresources as the source database.",
                "D.  Create read replicas for the database. Configure the read replicas with the same compute and storage\nresources as the source database."
            ],
            "Explanation": "Answer: D\nExplanation\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_MySQL.Replication.ReadReplicas.html\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 150,
            "QuestionContent": "A company uses AWS Organizations to create dedicated AWS accounts for each business unit to manage each\nbusiness unit\u0027s account independently upon request. The root email recipient missed a notification that was\nsent to the root user email address of one account. The company wants to ensure that all future notifications\nare not missed. Future notifications must be limited to account administrators.\nWhich solution will meet these requirements?",
            "Option": [
                "A.  Configure the company\u0027s email server to forward notification email messages that are sent to the AWS\naccount root user email address to all users in the organization.",
                "B.  Configure all AWS account root user email addresses as distribution lists that go to a few administrators\nwho can respond to alerts. Configure AWS account alternate contacts in the AWS Organizations console\nor programmatically.",
                "C.  Configure all AWS account root user email messages to be sent to one administrator who is responsible\nfor monitoring alerts and forwarding those alerts to the appropriate groups.",
                "D.  Configure all existing AWS accounts and all newly created accounts to use the same root user email\naddress. Configure AWS account alternate contacts in the AWS Organizations console or\nprogrammatically."
            ],
            "Explanation": "Answer: D\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 151,
            "QuestionContent": "A company hosts a website analytics application on a single Amazon EC2 On-Demand Instance. The analytics\nsoftware is written in PHP and uses a MySQL database. The analytics software, the web server that provides\nPHP, and the database server are all hosted on the EC2 instance. The application is showing signs of\nperformance degradation during busy times and is presenting 5xx errors. The company needs to make the\napplication scale seamlessly.\nWhich solution will meet these requirements MOST cost-effectively?",
            "Option": [
                "A.  Migrate the database to an Amazon RDS for MySQL DB instance. Create an AMI of the web\napplication. Use the AMI to launch a second EC2 On-Demand Instance. Use an Application Load\nBalancer to distribute the load to each EC2 instance.",
                "B.  Migrate the database to an Amazon RDS for MySQL DB instance. Create an AMI of the web\napplication. Use the AMI to launch a second EC2 On-Demand Instance. Use Amazon Route 53\nweighted routing to distribute the load across the two EC2 instances.",
                "C.  Migrate the database to an Amazon Aurora MySQL DB instance. Create an AWS Lambda function to\nstop the EC2 instance and change the instance type. Create an Amazon CloudWatch alarm to invoke the\nLambda function when CPU utilization surpasses 75%.",
                "D.  Migrate the database to an Amazon Aurora MySQL DB instance. Create an AMI of the web application.\nApply the AMI to a launch template. Create an Auto Scaling group with the launch template Configure\nthe launch template to use a Spot Fleet. Attach an Application Load Balancer to the Auto Scaling group."
            ],
            "Explanation": "Answer: D\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 152,
            "QuestionContent": "A company\u2019s website provides users with downloadable historical performance reports. The website needs a\nsolution that will scale to meet the company\u2019s website demands globally. The solution should be\ncost-effective, limit the provisioning of infrastructure resources, and provide the fastest possible response time.\nWhich combination should a solutions architect recommend to meet these requirements?",
            "Option": [
                "A.  Amazon CloudFront and Amazon S3",
                "B.  AWS Lambda and Amazon DynamoDB",
                "C.  Application Load Balancer with Amazon EC2 Auto Scaling",
                "D.  Amazon Route 53 with internal Application Load Balancers"
            ],
            "Explanation": "Answer: A\nExplanation\nCloudfront for rapid response and s3 to minimize infrastructure.\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 153,
            "QuestionContent": "A company wants to use high performance computing (HPC) infrastructure on AWS for financial risk\nmodeling. The company\u0027s HPC workloads run on Linux. Each HPC workflow runs on hundreds of Amazon\nEC2 Spot Instances, is shorl-lived, and generates thousands of output files that are ultimately stored in\npersistent storage for analytics and long-term future use.\nThe company seeks a cloud storage solution that permits the copying of on-premises data to long-term\npersistent storage to make data available for processing by all EC2 instances. The solution should also be a\nhigh performance file system that is integrated with persistent storage to read and write datasets and output\nfiles.\nWhich combination of AWS services meets these requirements?",
            "Option": [
                "A.  Amazon FSx for Lustre integrated with Amazon S3",
                "B.  Amazon FSx for Windows File Server integrated with Amazon S3",
                "C.  Amazon S3 Glacier integrated with Amazon Elastic Block Store (Amazon EBS)",
                "D.  Amazon S3 bucket with a VPC endpoint integrated with an Amazon Elastic Block Store (Amazon EBS)\nGeneral Purpose SSD (gp2) volume"
            ],
            "Explanation": "Answer: A\nExplanation\nhttps://aws.amazon.com/fsx/lustre/\nAmazon FSx for Lustre is a fully managed service that provides cost-effective, high-performance, scalable\nstorage for compute workloads. Many workloads such as machine learning, high performance computing\n(HPC), video rendering, and financial simulations depend on compute instances accessing the same set of data\nthrough high-performance shared storage.\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 154,
            "QuestionContent": "A solutions architect must design a solution that uses Amazon CloudFront with an Amazon S3 origin to store a\nstatic website. The company\u0027s security policy requires that all website traffic be inspected by AWS WAR\nHow should the solutions architect comply with these requirements?",
            "Option": [
                "A.  Configure an S3 bucket policy lo accept requests coming from the AWS WAF Amazon Resource Name\n(ARN) only.",
                "B.  Configure Amazon CloudFront to forward all incoming requests to AWS WAF before requesting\ncontent from the S3 origin.",
                "C.  Configure a security group that allows Amazon CloudFront IP addresses to access Amazon S3 only.\nAssociate AWS WAF to CloudFront.",
                "D.  Configure Amazon CloudFront and Amazon S3 to use an origin access identity (OAI) to restrict access\nto the S3 bucket. Enable AWS WAF on the distribution."
            ],
            "Explanation": "Answer: D\nExplanation\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-awswaf.html\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 155,
            "QuestionContent": "A company wants to migrate its existing on-premises monolithic application to AWS.\nThe company wants to keep as much of the front- end code and the backend code as possible. However, the\ncompany wants to break the application into smaller applications. A different team will manage each\napplication. The company needs a highly scalable solution that minimizes operational overhead.\nWhich solution will meet these requirements?",
            "Option": [
                "A.  Host the application on AWS Lambda Integrate the application with Amazon API Gateway.",
                "B.  Host the application with AWS Amplify. Connect the application to an Amazon API Gateway API that\nis integrated with AWS Lambda.",
                "C.  Host the application on Amazon EC2 instances. Set up an Application Load Balancer with EC2\ninstances in an Auto Scaling group as targets.",
                "D.  Host the application on Amazon Elastic Container Service (Amazon ECS) Set up an Application Load\nBalancer with Amazon ECS as the target."
            ],
            "Explanation": "Answer: D\nExplanation\nhttps://aws.amazon.com/blogs/compute/microservice-delivery-with-amazon-ecs-and-application-load-balancers/\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 156,
            "QuestionContent": "A company has several web servers that need to frequently access a common Amazon RDS MySQL Multi-AZ\nDB instance The company wants a secure method for the web servers to connect to the database while meeting\na security requirement to rotate user credentials frequently.\nWhich solution meets these requirements?",
            "Option": [
                "A.  Store the database user credentials in AWS Secrets Manager Grant the necessary IAM permissions to\nallow the web servers to access AWS Secrets Manager",
                "B.  Store the database user credentials in AWS Systems Manager OpsCenter Grant the necessary IAM\npermissions to allow the web servers to access OpsCenter",
                "C.  Store the database user credentials in a secure Amazon S3 bucket Grant the necessary IAM permissions\nto allow the web servers to retrieve credentials and access the database.",
                "D.  Store the database user credentials in files encrypted with AWS Key Management Service (AWS KMS)\non the web server file system. The web server should be able to decrypt the files and access the database"
            ],
            "Explanation": "Answer: A\nExplanation\nAWS Secrets Manager helps you protect secrets needed to access your applications, services, and IT\nresources. The service enables you to easily rotate, manage, and retrieve database credentials, API keys, and\nother secrets throughout their lifecycle.\nhttps://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html\nSecrets Manager enables you to replace hardcoded credentials in your code, including passwords, with an API\ncall to Secrets Manager to retrieve the secret programmatically. This helps ensure the secret can\u0027t be\ncompromised by someone examining your code, because the secret no longer exists in the code. Also, you can\nconfigure Secrets Manager to automatically rotate the secret for you according to a specified schedule. This\nenables you to replace long-term secrets with short-term ones, significantly reducing the risk of compromise.\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 157,
            "QuestionContent": "A company recently migrated a message processing system to AWS. The system receives messages into an\nActiveMQ queue running on an Amazon EC2 instance. Messages are processed by a consumer application\nrunning on Amazon EC2. The consumer application processes the messages and writes results to a MySQL\ndatabase funning on Amazon EC2. The company wants this application to be highly available with tow\noperational complexity\nWhich architecture otters the HGHEST availability?",
            "Option": [
                "A.  Add a second ActiveMQ server to another Availably Zone Add an additional consumer EC2 instance in\nanother Availability Zone. Replicate the MySQL database to another Availability Zone.",
                "B.  Use Amazon MO with active/standby brokers configured across two Availability Zones Add an\nadditional consumer EC2 instance in another Availability Zone. Replicate the MySQL database to\nanother Availability Zone.",
                "C.  Use Amazon MO with active/standby blotters configured across two Availability Zones. Add an\nadditional consumer EC2 instance in another Availability Zone. Use Amazon ROS tor MySQL with\nMulti-AZ enabled.",
                "D.  Use Amazon MQ with active/standby brokers configured across two Availability Zones Add an Auto\nScaling group for the consumer EC2 instances across two Availability Zones. Use Amazon RDS for\nMySQL with Multi-AZ enabled."
            ],
            "Explanation": "Answer: D\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 158,
            "QuestionContent": "A company uses a popular content management system (CMS) for its corporate website. However, the\nrequired patching and maintenance are burdensome. The company is redesigning its website and wants anew\nsolution. The website will be updated four times a year and does not need to have any dynamic content\navailable. The solution must provide high scalability and enhanced security.\nWhich combination of changes will meet these requirements with the LEAST operational overhead? (Choose\ntwo.)",
            "Option": [
                "A.  Deploy an AWS WAF web ACL in front of the website to provide HTTPS functionality",
                "B.  Create and deploy an AWS Lambda function to manage and serve the website content",
                "C.  Create the new website and an Amazon S3 bucket Deploy the website on the S3 bucket with static\nwebsite hosting enabled",
                "D.  Create the new website. Deploy the website by using an Auto Scaling group of Amazon EC2 instances\nbehind an Application Load Balancer."
            ],
            "Explanation": "Answer: A D\n",
            "RightAnswer": [
                "A",
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 159,
            "QuestionContent": "A company is planning to build a high performance computing (HPC) workload as a service solution that Is\nhosted on AWS A group of 16 AmazonEC2Ltnux Instances requires the lowest possible latency for\nnode-to-node communication. The instances also need a shared block device volume for high-performing\nstorage.\nWhich solution will meet these requirements?",
            "Option": [
                "A.  Use a duster placement group. Attach a single Provisioned IOPS SSD Amazon Elastic Block Store\n(Amazon E BS) volume to all the instances by using Amazon EBS Multi-Attach",
                "B.  Use a cluster placement group. Create shared \u0027lie systems across the instances by using Amazon Elastic\nFile System (Amazon EFS)",
                "C.  Use a partition placement group. Create shared tile systems across the instances by using Amazon\nElastic File System (Amazon EFS).",
                "D.  Use a spread placement group. Attach a single Provisioned IOPS SSD Amazon Elastic Block Store\n(Amazon EBS) volume to all the instances by using Amazon EBS Multi-Attach"
            ],
            "Explanation": "Answer: A\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 160,
            "QuestionContent": "A company sells ringtones created from clips of popular songs. The files containing the ringtones are stored in\nAmazon S3 Standard and are at least 128 KB in size. The company has millions of files, but downloads are\ninfrequent for ringtones older than 90 days. The company needs to save money on storage while keeping the\nmost accessed files readily available for its users.\nWhich action should the company take to meet these requirements MOST cost-effectively?",
            "Option": [
                "A.  Configure S3 Standard-Infrequent Access (S3 Standard-IA) storage for the initial storage tier of the\nobjects.",
                "B.  Move the files to S3 Intelligent-Tiering and configure it to move objects to a less expensive storage tier\nafter 90 days.",
                "C.  Configure S3 inventory to manage objects and move them to S3 Standard-Infrequent Access (S3\nStandard-1A) after 90 days.",
                "D.  Implement an S3 Lifecycle policy that moves the objects from S3 Standard to S3 Standard-Infrequent\nAccess (S3 Standard-1A) after 90 days."
            ],
            "Explanation": "Answer: D\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 161,
            "QuestionContent": "A company\u0027s website uses an Amazon EC2 instance store for its catalog of items. The company wants to make\nsure that the catalog is highly available and that the catalog is stored in a durable location.\nWhat should a solutions architect do to meet these requirements?",
            "Option": [
                "A.  Move the catalog to Amazon ElastiCache for Redis.",
                "B.  Deploy a larger EC2 instance with a larger instance store.",
                "C.  Move the catalog from the instance store to Amazon S3 Glacier Deep Archive.",
                "D.  Move the catalog to an Amazon Elastic File System (Amazon EFS) file system."
            ],
            "Explanation": "Answer: D\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 162,
            "QuestionContent": "A company runs an online marketplace web application on AWS. The application serves hundreds of\nthousands of users during peak hours. The company needs a scalable, near-real-time solution to share the\ndetails of millions of financial transactions with several other internal applications Transactions also need to be\nprocessed to remove sensitive data before being stored in a document database for low-latency retrieval.\nWhat should a solutions architect recommend to meet these requirements?",
            "Option": [
                "A.  Store the transactions data into Amazon DynamoDB Set up a rule in DynamoDB to remove sensitive\ndata from every transaction upon write Use DynamoDB Streams to share the transactions data with\nother applications",
                "B.  Stream the transactions data into Amazon Kinesis Data Firehose to store data in Amazon DynamoDB\nand Amazon S3 Use AWS Lambda integration with Kinesis Data Firehose to remove sensitive data.\nOther applications can consume the data stored in Amazon S3",
                "C.  Stream the transactions data into Amazon Kinesis Data Streams Use AWS Lambda integration to\nremove sensitive data from every transaction and then store the transactions data in Amazon\nDynamoDB Other applications can consume the transactions data off the Kinesis data stream.",
                "D.  Store the batched transactions data in Amazon S3 as files. Use AWS Lambda to process every file and\nremove sensitive data before updating the files in Amazon S3 The Lambda function then stores the data\nin Amazon DynamoDB Other applications can consume transaction files stored in Amazon S3."
            ],
            "Explanation": "Answer: C\nExplanation\nThe destination of your Kinesis Data Firehose delivery stream. Kinesis Data Firehose can send data records to\nvarious destinations, including Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Amazon\nOpenSearch Service, and any HTTP endpoint that is owned by you or any of your third-party service\nproviders. The following are the supported destinations:\n* Amazon OpenSearch Service\n* Amazon S3\n* Datadog\n* Dynatrace\n* Honeycomb\n* HTTP Endpoint\n* Logic Monitor\n* MongoDB Cloud\n* New Relic\n* Splunk\n* Sumo Logic\nhttps://docs.aws.amazon.com/firehose/latest/dev/create-name.html\nhttps://aws.amazon.com/kinesis/data-streams/\nAmazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service.\nKDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as\nwebsite clickstreams, database event streams, financial transactions, social media feeds, IT logs, and\nlocation-tracking events.\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 163,
            "QuestionContent": "An image-processing company has a web application that users use to upload images. The application uploads\nthe images into an Amazon S3 bucket. The company has set up S3 event notifications to publish the object\ncreation events to an Amazon Simple Queue Service (Amazon SQS) standard queue. The SQS queue serves as\nthe event source for an AWS Lambda function that processes the images and sends the results to users through\nemail.\nUsers report that they are receiving multiple email messages for every uploaded image. A solutions architect\ndetermines that SQS messages are invoking the Lambda function more than once, resulting in multiple email\nmessages.\nWhat should the solutions architect do to resolve this issue with the LEAST operational overhead?",
            "Option": [
                "A.  Set up long polling in the SQS queue by increasing the ReceiveMessage wait time to 30 seconds.",
                "B.  Change the SQS standard queue to an SQS FIFO queue. Use the message deduplication ID to discard\nduplicate messages.",
                "C.  Increase the visibility timeout in the SQS queue to a value that is greater than the total of the function\ntimeout and the batch window timeout.",
                "D.  Modify the Lambda function to delete each message from the SQS queue immediately after the message\nis read before processing."
            ],
            "Explanation": "Answer: C\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 164,
            "QuestionContent": "An entertainment company is using Amazon DynamoDB to store media metadata. The application is read\nintensive and experiencing delays. The company does not have staff to handle additional operational overhead\nand needs to improve the performance efficiency of DynamoDB without reconfiguring the application.\nWhat should a solutions architect recommend to meet this requirement?",
            "Option": [
                "A.  Use Amazon ElastiCache for Redis.",
                "B.  Use Amazon DynamoDB Accelerator (DAX).",
                "C.  Replicate data by using DynamoDB global tables.",
                "D.  Use Amazon ElastiCache for Memcached with Auto Discovery enabled."
            ],
            "Explanation": "Answer: B\nExplanation\nhttps://aws.amazon.com/dynamodb/dax/\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 165,
            "QuestionContent": "A company is deploying a new public web application to AWS. The application will run behind an\nApplication Load Balancer (ALB). The application needs to be encrypted at the edge with an SSL/TLS\ncertificate that is issued by an external certificate authority (CA). The certificate must be rotated each year\nbefore the certificate expires.\nWhat should a solutions architect do to meet these requirements?",
            "Option": [
                "A.  Use AWS Certificate Manager (ACM) to issue an SSL/TLS certificate. Apply the certificate to the ALB.\nUse the managed renewal feature to automatically rotate the certificate.",
                "B.  Use AWS Certificate Manager (ACM) to issue an SSL/TLS certificate. Import the key material from the\ncertificate. Apply the certificate to the ALB. Use the managed renewal feature to automatically rotate\nthe certificate.",
                "C.  Use AWS Certificate Manager (ACM) Private Certificate Authority to issue an SSL/TLS certificate\nfrom the root CA. Apply the certificate to the ALB. Use the managed renewal feature to automatically\nrotate the certificate.",
                "D.  Use AWS Certificate Manager (ACM) to import an SSL/TLS certificate. Apply the certificate to the\nALB. Use Amazon EventBridge (Amazon CloudWatch Events) to send a notification when the\ncertificate is nearing expiration. Rotate the certificate manually."
            ],
            "Explanation": "Answer: D\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 166,
            "QuestionContent": "A company is running a publicly accessible serverless application that uses Amazon API Gateway and AWS\nLambda. The application\u0027s traffic recently spiked due to fraudulent requests from botnets.\nWhich steps should a solutions architect take to block requests from unauthorized users? (Select TWO.)",
            "Option": [
                "A.  Create a usage plan with an API key that is shared with genuine users only.",
                "B.  Integrate logic within the Lambda function to ignore the requests from fraudulent IP addresses.",
                "C.  Implement an AWS WAF rule to target malicious requests and trigger actions to filter them out.",
                "D.  Convert the existing public API to a private API. Update the DNS records to redirect users to the new\nAPI endpoint.",
                "E.  Create an IAM role for each user attempting to access the API. A user will assume the role when\nmaking the API call."
            ],
            "Explanation": "Answer: A C\nExplanation\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html#:~:text=Don%27t%20rely%20on%20API%20keys%20as%20your%20only%20means%20of%20authentication%20and%20authorization%20for%20your%20APIs\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html\n",
            "RightAnswer": [
                "A",
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 167,
            "QuestionContent": "A company has an event-driven application that invokes AWS Lambda functions up to 800 times each minute\nwith varying runtimes. The Lambda functions access data that is stored in an Amazon Aurora MySQL OB\ncluster. The company is noticing connection timeouts as user activity increases The database shows no signs of\nbeing overloaded. CPU. memory, and disk access metrics are all low.\nWhich solution will resolve this issue with the LEAST operational overhead?",
            "Option": [
                "A.  Adjust the size of the Aurora MySQL nodes to handle more connections. Configure retry logic in the\nLambda functions for attempts to connect to the database",
                "B.  Set up Amazon ElastiCache tor Redls to cache commonly read items from the database. Configure the\nLambda functions to connect to ElastiCache for reads.",
                "C.  Add an Aurora Replica as a reader node. Configure the Lambda functions to connect to the reader\nendpoint of the OB cluster rather than lo the writer endpoint.",
                "D.  Use Amazon ROS Proxy to create a proxy. Set the DB cluster as the target database Configure the\nLambda functions lo connect to the proxy rather than to the DB cluster."
            ],
            "Explanation": "Answer: D\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 168,
            "QuestionContent": "A company is migrating an application from on-premises servers to Amazon EC2 instances. As part of the\nmigration design requirements, a solutions architect must implement infrastructure metric alarms. The\ncompany does not need to take action if CPU utilization increases to more than 50% for a short burst of time.\nHowever, if the CPU utilization increases to more than 50% and read IOPS on the disk are high at the same\ntime, the company needs to act as soon as possible. The solutions architect also must reduce false alarms.\nWhat should the solutions architect do to meet these requirements?",
            "Option": [
                "A.  Create Amazon CloudWatch composite alarms where possible.",
                "B.  Create Amazon CloudWatch dashboards to visualize the metrics and react to issues quickly.",
                "C.  Create Amazon CloudWatch Synthetics canaries to monitor the application and raise an alarm.",
                "D.  Create single Amazon CloudWatch metric alarms with multiple metric thresholds where possible."
            ],
            "Explanation": "Answer: A\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 169,
            "QuestionContent": "A company has more than 5 TB of file data on Windows file servers that run on premises Users and\napplications interact with the data each day\nThe company is moving its Windows workloads to AWS. As the company continues this process, the\ncompany requires access to AWS and on-premises file storage with minimum latency The company needs a\nsolution that minimizes operational overhead and requires no significant changes to the existing file access\npatterns. The company uses an AWS Site-to-Site VPN connection for connectivity to AWS\nWhat should a solutions architect do to meet these requirements?",
            "Option": [
                "A.  Deploy and configure Amazon FSx for Windows File Server on AWS. Move the on-premises file data\nto FSx for Windows File Server. Reconfigure the workloads to use FSx for Windows File Server on\nAWS.",
                "B.  Deploy and configure an Amazon S3 File Gateway on premises Move the on-premises file data to the\nS3 File Gateway Reconfigure the on-premises workloads and the cloud workloads to use the S3 File\nGateway",
                "C.  Deploy and configure an Amazon S3 File Gateway on premises Move the on-premises file data to\nAmazon S3 Reconfigure the workloads to use either Amazon S3 directly or the S3 File Gateway,\ndepending on each workload\u0027s location",
                "D.  Deploy and configure Amazon FSx for Windows File Server on AWS Deploy and configure an Amazon\nFSx File Gateway on premises Move the on-premises file data to the FSx File Gateway Configure the\ncloud workloads to use FSx for Windows File Server on AWS Configure the on-premises workloads to\nuse the FSx File Gateway"
            ],
            "Explanation": "Answer: D\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 170,
            "QuestionContent": "A new employee has joined a company as a deployment engineer. The deployment engineer will be using\nAWS CloudFormation templates to create multiple AWS resources. A solutions architect wants the\ndeployment engineer to perform job activities while following the principle of least privilege.\nWhich steps should the solutions architect do in conjunction to reach this goal? (Select two.)",
            "Option": [
                "A.  Have the deployment engineer use AWS account roof user credentials for performing AWS\nCloudFormation stack operations.",
                "B.  Create a new IAM user for the deployment engineer and add the IAM user to a group that has the\nPowerUsers IAM policy attached.",
                "C.  Create a new IAM user for the deployment engineer and add the IAM user to a group that has the\nAdministrate/Access IAM policy attached.",
                "D.  Create a new IAM User for the deployment engineer and add the IAM user to a group that has an IAM\npolicy that allows AWS CloudFormation actions only.",
                "E.  Create an IAM role for the deployment engineer to explicitly define the permissions specific to the\nAWS CloudFormation stack and launch stacks using Dial IAM role."
            ],
            "Explanation": "Answer: D E\nExplanation\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_users.html\n",
            "RightAnswer": [
                "D",
                "E"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 171,
            "QuestionContent": "A company is implementing a new business application. The application runs on two Amazon EC2 instances\nand uses an Amazon S3 bucket for document storage. A solutions architect needs to ensure that the EC2\ninstances can access the S3 bucket.\nWhat should the solutions architect do to meet this requirement?",
            "Option": [
                "A.  Create an IAM role that grants access to the S3 bucket. Attach the role to the EC2 instances.",
                "B.  Create an IAM policy that grants access to the S3 bucket. Attach the policy to the EC2 instances.",
                "C.  Create an IAM group that grants access to the S3 bucket. Attach the group to the EC2 instances.",
                "D.  Create an IAM user that grants access to the S3 bucket. Attach the user account to the EC2 instances."
            ],
            "Explanation": "Answer: A\nExplanation\nhttps://aws.amazon.com/premiumsupport/knowledge-center/ec2-instance-access-s3-bucket/\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 172,
            "QuestionContent": "A company hosts more than 300 global websites and applications. The company requires a platform to analyze\nmore than 30 TB of clickstream data each day.\nWhat should a solutions architect do to transmit and process the clickstream data?",
            "Option": [
                "A.  Design an AWS Data Pipeline to archive the data to an Amazon S3 bucket and run an Amazon EMR\nduster with the data to generate analytics",
                "B.  Create an Auto Scaling group of Amazon EC2 instances to process the data and send it to an Amazon S3\ndata lake for Amazon Redshift to use tor analysis",
                "C.  Cache the data to Amazon CloudFron: Store the data in an Amazon S3 bucket When an object is added\nto the S3 bucket, run an AWS Lambda function to process the data tor analysis.",
                "D.  Collect the data from Amazon Kinesis Data Streams. Use Amazon Kinesis Data Firehose to transmit the\ndata to an Amazon S3 data lake Load the data in Amazon Redshift for analysis"
            ],
            "Explanation": "Answer: D\nExplanation\nhttps://aws.amazon.com/es/blogs/big-data/real-time-analytics-with-amazon-redshift-streaming-ingestion/\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 173,
            "QuestionContent": "A company uses 50 TB of data for reporting. The company wants to move this data from on premises to AWS\nA custom application in the company\u0027s data center runs a weekly data transformation job. The company plans\nto pause the application until the data transfer is complete and needs to begin the transfer process as soon as\npossible.\nThe data center does not have any available network bandwidth for additional workloads A solutions architect\nmust transfer the data and must configure the transformation job to continue to run in the AWS Cloud\nWhich solution will meet these requirements with the LEAST operational overhead?",
            "Option": [
                "A.  Use AWS DataSync to move the data Create a custom transformation job by using AWS Glue",
                "B.  Order an AWS Snowcone device to move the data Deploy the transformation application to the device",
                "C.  Order an AWS Snowball Edge Storage Optimized device. Copy the data to the device. Create a custom\ntransformation job by using AWS Glue",
                "D.  Order an AWS D. Snowball Edge Storage Optimized device that includes Amazon EC2 compute Copy\nthe data to the device Create a new EC2 instance on AWS to run the transformation application"
            ],
            "Explanation": "Answer: C\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 174,
            "QuestionContent": "A company is building a new dynamic ordering website. The company wants 10 minimize server maintenance\nand patching. The website must be highly available and must scale read and write capacity as qutddy as\npossible to meet changes in user demand.\nWhich solution will meet ftese requirements?",
            "Option": [
                "A.  Host static content in Amazon S3 Host dynamic content by using Amazon API Gateway and AWS\nLambda Use Amazon DynamoDB with on-demand capacity for the database Configure Amazon\nCtoudFront to deliver the website content",
                "B.  Host static content in Amazon S3 Host dynamic content by using Amazon API Gateway and AWS\nLambda Use Amazon Aurora with Aurora Auto Scaling for the database Configure Amazon CloudFront\nto deliver the website content",
                "C.  Host al the website content on Amazon EC2 instances Create an Auto Scaling group to scale the EC2\nInstances Use an Application Load Balancer to distribute traffic Use Amazon DynamoDB with\nprovisioned write capacity for the database",
                "D.  Host at the website content on Amazon EC2 instances Create an Auto Scaling group to scale the EC2\ninstances Use an Application Load Balancer to distribute traffic Use Amazon Aurora with Aurora Auto\nScaling for the database"
            ],
            "Explanation": "Answer: A\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 175,
            "QuestionContent": "A company wants to migrate its MySQL database from on premises to AWS. The company recently\nexperienced a database outage that significantly impacted the business. To ensure this does not happen again,\nthe company wants a reliable database solution on AWS that minimizes data loss and stores every transaction\non at least two nodes.\nWhich solution meets these requirements?",
            "Option": [
                "A.  Create an Amazon RDS DB instance with synchronous replication to three nodes in three Availability\nZones.",
                "B.  Create an Amazon RDS MySQL DB instance with Multi-AZ functionality enabled to synchronously\nreplicate the data.",
                "C.  Create an Amazon RDS MySQL DB instance and then create a read replica in a separate AWS Region\nthat synchronously replicates the data.",
                "D.  Create an Amazon EC2 instance with a MySQL engine installed that triggers an AWS Lambda function\nto synchronously replicate the data to an Amazon RDS MySQL DB instance."
            ],
            "Explanation": "Answer: B\nExplanation\nQ: What does Amazon RDS manage on my behalf?\nAmazon RDS manages the work involved in setting up a relational database: from provisioning the\ninfrastructure capacity you request to installing the database software. Once your database is up and running,\nAmazon RDS automates common administrative tasks such as performing backups and patching the software\nthat powers your database. With optional Multi-AZ deployments, Amazon RDS also manages synchronous\ndata replication across Availability Zones with automatic failover.\nhttps://aws.amazon.com/rds/faqs/\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 176,
            "QuestionContent": "A company wants to run applications in containers in the AWS Cloud. These applications are stateless and can\ntolerate disruptions within the underlying infrastructure. The company needs a solution that minimizes cost\nand operational overhead.\nWhat should a solutions architect do to meet these requirements?",
            "Option": [
                "A.  Use Spot Instances in an Amazon EC2 Auto Scaling group to run the application containers.",
                "B.  Use Spot Instances in an Amazon Elastic Kubernetes Service (Amazon EKS) managed node group.",
                "C.  Use On-Demand Instances in an Amazon EC2 Auto Scaling group to run the application containers.",
                "D.  Use On-Demand Instances in an Amazon Elastic Kubernetes Service (Amazon EKS) managed node\ngroup."
            ],
            "Explanation": "Answer: A\nExplanation\nhttps://aws.amazon.com/cn/blogs/compute/cost-optimization-and-resilience-eks-with-spot-instances/\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 177,
            "QuestionContent": "A medical records company is hosting an application on Amazon EC2 instances. The application processes\ncustomer data files that are stored on Amazon S3. The EC2 instances are hosted in public subnets. The EC2\ninstances access Amazon S3 over the internet, but they do not require any other network access.\nA new requirement mandates that the network traffic for file transfers take a private route and not be sent over\nthe internet.\nWhich change to the network architecture should a solutions architect recommend to meet this requirement?",
            "Option": [
                "A.  Create a NAT gateway. Configure the route table for the public subnets to send traffic to Amazon S3\nthrough the NAT gateway.",
                "B.  Configure the security group for the EC2 instances to restrict outbound traffic so that only traffic to the\nS3 prefix list is permitted.",
                "C.  Move the EC2 instances to private subnets. Create a VPC endpoint for Amazon S3, and link the\nendpoint to the route table for the private subnets",
                "D.  Remove the internet gateway from the VPC. Set up an AWS Direct Connect connection, and route\ntraffic to Amazon S3 over the Direct Connect connection."
            ],
            "Explanation": "Answer: C\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 178,
            "QuestionContent": "A hospital wants to create digital copies for its large collection of historical written records. The hospital will\ncontinue to add hundreds of new documents each day. The hospital\u0027s data team will scan the documents and\nwill upload the documents to the AWS Cloud.\nA solutions architect must implement a solution to analyze the documents, extract the medical information,\nand store the documents so that an application can run SQL queries on the data. The solution must maximize\nscalability and operational efficiency.\nWhich combination of steps should the solutions architect take to meet these requirements? (Select TWO.)",
            "Option": [
                "A.  Write the document information to an Amazon EC2 instance that runs a MySQL database.",
                "B.  Write the document information to an Amazon S3 bucket. Use Amazon Athena to query the data.",
                "C.  Create an Auto Scaling group of Amazon EC2 instances to run a custom application that processes the\nscanned files and extracts the medical information.",
                "D.  Create an AWS Lambda function that runs when new documents are uploaded. Use Amazon\nRekognition to convert the documents to raw text. Use Amazon Transcribe Medical to detect and extract\nrelevant medical information from the text.",
                "E.  Create an AWS Lambda function that runs when new documents are uploaded. Use Amazon Textract to\nconvert the documents to raw text. Use Amazon Comprehend Medical to detect and extract relevant\nmedical information from the text."
            ],
            "Explanation": "Answer: D E\n",
            "RightAnswer": [
                "D",
                "E"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 179,
            "QuestionContent": "A company has a dynamic web application hosted on two Amazon EC2 instances. The company has its own\nSSL certificate, which is on each instance to perform SSL termination.\nThere has been an increase in traffic recently, and the operations team determined that SSL encryption and\ndecryption is causing the compute capacity of the web servers to reach their maximum limit.\nWhat should a solutions architect do to increase the application\u0027s performance?",
            "Option": [
                "A.  Create a new SSL certificate using AWS Certificate Manager (ACM) install the ACM certificate on\neach instance",
                "B.  Create an Amazon S3 bucket Migrate the SSL certificate to the S3 bucket Configure the EC2 instances\nto reference the bucket for SSL termination",
                "C.  Create another EC2 instance as a proxy server Migrate the SSL certificate to the new instance and\nconfigure it to direct connections to the existing EC2 instances",
                "D.  Import the SSL certificate into AWS Certificate Manager (ACM) Create an Application Load Balancer\nwith an HTTPS listener that uses the SSL certificate from ACM"
            ],
            "Explanation": "Answer: D\nExplanation\nhttps://aws.amazon.com/certificate-manager/:\n\u0022With AWS Certificate Manager, you can quickly request a certificate, deploy it on ACM-integrated AWS\nresources, such as Elastic Load Balancers, Amazon CloudFront distributions, and APIs on API Gateway, and\nlet AWS Certificate Manager handle certificate renewals. It also enables you to create private certificates for\nyour internal resources and manage the certificate lifecycle centrally.\u0022\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 180,
            "QuestionContent": "A company is launching a new application and will display application metrics on an Amazon CloudWatch\ndashboard. The company\u2019s product manager needs to access this dashboard periodically. The product manager\ndoes not have an AWS account. A solution architect must provide access to the product manager by following\nthe principle of least privilege.\nWhich solution will meet these requirements?",
            "Option": [
                "A.  Share the dashboard from the CloudWatch console. Enter the product manager\u2019s email address, and\ncomplete the sharing steps. Provide a shareable link for the dashboard to the product manager.",
                "B.  Create an IAM user specifically for the product manager. Attach the CloudWatch Read Only Access\nmanaged policy to the user. Share the new login credential with the product manager. Share the browser\nURL of the correct dashboard with the product manager.",
                "C.  Create an IAM user for the company\u2019s employees, Attach the View Only Access AWS managed policy\nto the IAM user. Share the new login credentials with the product manager. Ask the product manager to\nnavigate to the CloudWatch console and locate the dashboard by name in the Dashboards section.",
                "D.  Deploy a bastion server in a public subnet. When the product manager requires access to the dashboard,\nstart the server and share the RDP credentials. On the bastion server, ensure that the browser is\nconfigured to open the dashboard URL with cached AWS credentials that have appropriate permissions\nto view the dashboard."
            ],
            "Explanation": "Answer: A\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 181,
            "QuestionContent": "A company runs its Infrastructure on AWS and has a registered base of 700.000 users for res document\nmanagement application The company intends to create a product that converts large pdf files to jpg Imago\nfiles. The .pdf files average 5 MB in size. The company needs to store the original files and the converted files.\nA solutions architect must design a scalable solution to accommodate demand that will grow rapidly over lime.\nWhich solution meets these requirements MOST cost-effectively?",
            "Option": [
                "A.  Save the pdf files to Amazon S3 Configure an S3 PUT event to invoke an AWS Lambda function to\nconvert the files to jpg format and store them back in Amazon S3",
                "B.  Save the pdf files to Amazon DynamoDB. Use the DynamoDB Streams feature to invoke an AWS\nLambda function to convert the files to jpg format and store them hack in DynamoDB",
                "C.  Upload the pdf files to an AWS Elastic Beanstalk application that includes Amazon EC2 instances.\nAmazon Elastic Block Store (Amazon EBS) storage and an Auto Scaling group. Use a program In the\nEC2 instances to convert the files to jpg format Save the .pdf files and the .jpg files In the EBS store.",
                "D.  Upload the .pdf files to an AWS Elastic Beanstalk application that includes Amazon EC2 instances,\nAmazon Elastic File System (Amazon EPS) storage, and an Auto Scaling group. Use a program in the\nEC2 instances to convert the file to jpg format Save the pdf files and the jpg files in the EBS store."
            ],
            "Explanation": "Answer: A\nExplanation\nElastic BeanStalk is expensive, and DocumentDB has a 400KB max to upload files. So Lambda and S3 should\nbe the one.\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 182,
            "QuestionContent": "A company uses NFS to store large video files in on-premises network attached storage. Each video file ranges\nin size from 1MB to 500 GB. The total storage is 70 TB and is no longer growing. The company decides to\nmigrate the video files to Amazon S3. The company must migrate the video files as soon as possible while\nusing the least possible network bandwidth.\nWhich solution will meet these requirements?",
            "Option": [
                "A.  Create an S3 bucket Create an IAM role that has permissions to write to the S3 bucket. Use the AWS\nCLI to copy all files locally to the S3 bucket.",
                "B.  Create an AWS Snowball Edge job. Receive a Snowball Edge device on premises. Use the Snowball\nEdge client to transfer data to the device. Return the device so that AWS can import the data into\nAmazon S3.",
                "C.  Deploy an S3 File Gateway on premises. Create a public service endpoint to connect to the S3 File\nGateway Create an S3 bucket Create a new NFS file share on the S3 File Gateway Point the new file\nshare to the S3 bucket. Transfer the data from the existing NFS file share to the S3 File Gateway.",
                "D.  Set up an AWS Direct Connect connection between the on-premises network and AWS. Deploy an S3\nFile Gateway on premises. Create a public virtual interlace (VIF) to connect to the S3 File Gateway.\nCreate an S3 bucket. Create a new NFS file share on the S3 File Gateway. Point the new file share to the\nS3 bucket. Transfer the data from the existing NFS file share to the S3 File Gateway."
            ],
            "Explanation": "Answer: B\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 183,
            "QuestionContent": "A company is planning to move its data to an Amazon S3 bucket. The data must be encrypted when it is stored\nin the S3 bucket. Additionally, the encryption key must be automatically rotated every year.\nWhich solution will meet these requirements with the LEAST operational overhead?",
            "Option": [
                "A.  Move the data to the S3 bucket. Use server-side encryption with Amazon S3 managed encryption keys\n(SSE-S3). Use the built-in key rotation behavior of SSE-S3 encryption keys.",
                "B.  Create an AWS Key Management Service {AWS KMS) customer managed key. Enable automatic key\nrotation. Set the S3 bucket\u0027s default encryption behavior to use the customer managed KMS key. Move\nthe data to the S3 bucket.",
                "C.  Create an AWS Key Management Service (AWS KMS) customer managed key. Set the S3 bucket\u0027s\ndefault encryption behavior to use the customer managed KMS key. Move the data to the S3 bucket.\nManually rotate the KMS key every year.",
                "D.  Encrypt the data with customer key material before moving the data to the S3 bucket. Create an AWS\nKey Management Service (AWS KMS) key without key material. Import the customer key material into\nthe KMS key. Enable automatic key rotation."
            ],
            "Explanation": "Answer: C\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 184,
            "QuestionContent": "A company is running a popular social media website. The website gives users the ability to upload images to\nshare with other users. The company wants to make sure that the images do not contain inappropriate content.\nThe company needs a solution that minimizes development effort.\nWhat should a solutions architect do to meet these requirements?",
            "Option": [
                "A.  Use Amazon Comprehend to detect inappropriate content. Use human review for low-confidence\npredictions.",
                "B.  Use Amazon Rekognition to detect inappropriate content. Use human review for low-confidence\npredictions.",
                "C.  Use Amazon SageMaker to detect inappropriate content. Use ground truth to label low-confidence\npredictions.",
                "D.  Use AWS Fargate to deploy a custom machine learning model to detect inappropriate content. Use\nground truth to label low-confidence predictions."
            ],
            "Explanation": "Answer: B\nExplanation\nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html?pg=ln\u0026sec=ft\nhttps://docs.aws.amazon.com/rekognition/latest/dg/a2i-rekognition.html\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 185,
            "QuestionContent": "A company needs to keep user transaction data in an Amazon DynamoDB table.\nThe company must retain the data for 7 years.\nWhat is the MOST operationally efficient solution that meets these requirements?",
            "Option": [
                "A.  Use DynamoDB point-in-time recovery to back up the table continuously.",
                "B.  Use AWS Backup to create backup schedules and retention policies for the table.",
                "C.  Create an on-demand backup of the table by using the DynamoDB console. Store the backup in an\nAmazon S3 bucket. Set an S3 Lifecycle configuration for the S3 bucket.",
                "D.  Create an Amazon EventBridge (Amazon CloudWatch Events) rule to invoke an AWS Lambda\nfunction. Configure the Lambda function to back up the table and to store the backup in an Amazon S3\nbucket. Set an S3 Lifecycle configuration for the S3 bucket."
            ],
            "Explanation": "Answer: C\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 186,
            "QuestionContent": "A hospital recently deployed a RESTful API with Amazon API Gateway and AWS Lambda The hospital uses\nAPI Gateway and Lambda to upload reports that are in PDF format and JPEG format The hospital needs to\nmodify the Lambda code to identify protected health information (PHI) in the reports\nWhich solution will meet these requirements with the LEAST operational overhead?",
            "Option": [
                "A.  Use existing Python libraries to extract the text from the reports and to identify the PHI from the\nextracted text.",
                "B.  Use Amazon Textract to extract the text from the reports Use Amazon SageMaker to identify the PHI\nfrom the extracted text.",
                "C.  Use Amazon Textract to extract the text from the reports Use Amazon Comprehend Medical to identify\nthe PHI from the extracted text",
                "D.  Use Amazon Rekognition to extract the text from the reports Use Amazon Comprehend Medical to\nidentify the PHI from the extracted text"
            ],
            "Explanation": "Answer: C\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 187,
            "QuestionContent": "A company has an application that ingests incoming messages. These messages are then quickly consumed by\ndozens of other applications and microservices.\nThe number of messages varies drastically and sometimes spikes as high as 100,000 each second. The\ncompany wants to decouple the solution and increase scalability.\nWhich solution meets these requirements?",
            "Option": [
                "A.  Persist the messages to Amazon Kinesis Data Analytics. All the applications will read and process the\nmessages.",
                "B.  Deploy the application on Amazon EC2 instances in an Auto Scaling group, which scales the number of\nEC2 instances based on CPU metrics.",
                "C.  Write the messages to Amazon Kinesis Data Streams with a single shard. All applications will read from\nthe stream and process the messages.",
                "D.  Publish the messages to an Amazon Simple Notification Service (Amazon SNS) topic with one or more\nAmazon Simple Queue Service (Amazon SQS) subscriptions. All applications then process the\nmessages from the queues."
            ],
            "Explanation": "Answer: D\nExplanation\nhttps://aws.amazon.com/sqs/features/\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 188,
            "QuestionContent": "A solutions architect needs to securely store a database user name and password that an application uses to\naccess an Amazon RDS DB instance. The application that accesses the database runs on an Amazon EC2\ninstance. The solutions architect wants to create a secure parameter in AWS Systems Manager Parameter\nStore.\nWhat should the solutions architect do to meet this requirement?",
            "Option": [
                "A.  Create an IAM role that has read access to the Parameter Store parameter. Allow Decrypt access to an\nAWS Key Management Service (AWS KMS) key that is used to encrypt the parameter. Assign this\nIAM role to the EC2 instance.",
                "B.  Create an IAM policy that allows read access to the Parameter Store parameter. Allow Decrypt access to\nan AWS Key Management Service (AWS KMS) key that is used to encrypt the parameter. Assign this\nIAM policy to the EC2 instance.",
                "C.  Create an IAM trust relationship between the Parameter Store parameter and the EC2 instance. Specify\nAmazon RDS as a principal in the trust policy.",
                "D.  Create an IAM trust relationship between the DB instance and the EC2 instance. Specify Systems\nManager as a principal in the trust policy."
            ],
            "Explanation": "Answer: B\nExplanation\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/reference_aws-services-that-work-with-iam.html\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 189,
            "QuestionContent": "A gaming company is designing a highly available architecture. The application runs on a modified Linux\nkernel and supports only UDP-based traffic. The company needs the front-end tier to provide the best possible\nuser experience. That tier must have low latency, route traffic to the nearest edge location, and provide static\nIP addresses for entry into the application endpoints.\nWhat should a solutions architect do to meet these requirements?",
            "Option": [
                "A.  Configure Amazon Route 53 to forward requests to an Application Load Balancer. Use AWS Lambda\nfor the application in AWS Application Auto Scaling.",
                "B.  Configure Amazon CloudFront to forward requests to a Network Load Balancer. Use AWS Lambda for\nthe application in an AWS Application Auto Scaling group.",
                "C.  Configure AWS Global Accelerator to forward requests to a Network Load Balancer. Use Amazon EC2\ninstances for the application in an EC2 Auto Scaling group.",
                "D.  Configure Amazon API Gateway to forward requests to an Application Load Balancer. Use Amazon\nEC2 instances for the application in an EC2 Auto Scaling group."
            ],
            "Explanation": "Answer: C\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 190,
            "QuestionContent": "A company needs to configure a real-time data ingestion architecture for its application. The company needs\nan API, a process that transforms data as the data is streamed, and a storage solution for the data.\nWhich solution will meet these requirements with the LEAST operational overhead?",
            "Option": [
                "A.  Deploy an Amazon EC2 instance to host an API that sends data to an Amazon Kinesis data stream.\nCreate an Amazon Kinesis Data Firehose delivery stream that uses the Kinesis data stream as a data\nsource. Use AWS Lambda functions to transform the data. Use the Kinesis Data Firehose delivery\nstream to send the data to Amazon S3.",
                "B.  Deploy an Amazon EC2 instance to host an API that sends data to AWS Glue. Stop source/destination\nchecking on the EC2 instance. Use AWS Glue to transform the data and to send the data to Amazon S3.",
                "C.  Configure an Amazon API Gateway API to send data to an Amazon Kinesis data stream. Create an\nAmazon Kinesis Data Firehose delivery stream that uses the Kinesis data stream as a data source. Use\nAWS Lambda functions to transform the data. Use the Kinesis Data Firehose delivery stream to send the\ndata to Amazon S3.",
                "D.  Configure an Amazon API Gateway API to send data to AWS Glue. Use AWS Lambda functions to\ntransform the data. Use AWS Glue to send the data to Amazon S3."
            ],
            "Explanation": "Answer: C\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 191,
            "QuestionContent": "A company has a three-tier web application that is deployed on AWS. The web servers are deployed in a\npublic subnet in a VPC. The application servers and database servers are deployed in private subnets in the\nsame VPC. The company has deployed a third-party virtual firewall appliance from AWS Marketplace in an\ninspection VPC. The appliance is configured with an IP interface that can accept IP packets.\nA solutions architect needs to Integrate the web application with the appliance to inspect all traffic to the\napplication before the traffic teaches the web server. Which solution will moot these requirements with the\nLEAST operational overhead?",
            "Option": [
                "A.  Create a Network Load Balancer the public subnet of the application\u0027s VPC to route the traffic lo the\nappliance for packet inspection",
                "B.  Create an Application Load Balancer in the public subnet of the application\u0027s VPC to route the traffic to\nthe appliance for packet inspection",
                "C.  Deploy a transit gateway m the inspection VPC Configure route tables to route the incoming pockets\nthrough the transit gateway",
                "D.  Deploy a Gateway Load Balancer in the inspection VPC Create a Gateway Load Balancer endpoint to\nreceive the incoming packets and forward the packets to the appliance"
            ],
            "Explanation": "Answer: D\nExplanation\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/scaling-network-traffic-inspection-using-aws-gateway-load-balancer/\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 192,
            "QuestionContent": "A solutions architect is developing a multiple-subnet VPC architecture. The solution will consist of six subnets\nin two Availability Zones. The subnets are defined as public, private and dedicated for databases. Only the\nAmazon EC2 instances running in the private subnets should be able to access a database.\nWhich solution meets these requirements?",
            "Option": [
                "A.  Create a now route table that excludes the route to the public subnets\u0027 CIDR blocks. Associate the route\ntable to the database subnets.",
                "B.  Create a security group that denies ingress from the security group used by instances in the public\nsubnets. Attach the security group to an Amazon RDS DB instance.",
                "C.  Create a security group that allows ingress from the security group used by instances in the private\nsubnets. Attach the security group to an Amazon RDS DB instance.",
                "D.  Create a new peering connection between the public subnets and the private subnets. Create a different\npeering connection between the private subnets and the database subnets."
            ],
            "Explanation": "Answer: C\nExplanation\nSecurity groups are stateful. All inbound traffic is blocked by default. If you create an inbound rule allowing\ntraffic in, that traffic is automatically allowed back out again. You cannot block specific IP address using\nSecurity groups (instead use Network Access Control Lists).\n\u0022You can specify allow rules, but not deny rules.\u0022 \u0022When you first create a security group, it has no inbound\nrules. Therefore, no inbound traffic originating from another host to your instance is allowed until you add\ninbound rules to the security group.\u0022 Source:\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html#VPCSecurityGroups\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 193,
            "QuestionContent": "A social media company allows users to upload images to its website. The website runs on Amazon EC2\ninstances. During upload requests, the website resizes the images to a standard size and stores the resized\nimages in Amazon S3. Users are experiencing slow upload requests to the website.\nThe company needs to reduce coupling within the application and improve website performance. A solutions\narchitect must design the most operationally efficient process for image uploads.\nWhich combination of actions should the solutions architect take to meet these requirements? (Choose two.)",
            "Option": [
                "A.  Configure the application to upload images to S3 Glacier.",
                "B.  Configure the web server to upload the original images to Amazon S3.",
                "C.  Configure the application to upload images directly from each user\u0027s browser to Amazon S3 through the\nuse of a presigned URL.",
                "D.  Configure S3 Event Notifications to invoke an AWS Lambda function when an image is uploaded. Use\nthe function to resize the image",
                "E.  Create an Amazon EventBridge (Amazon CloudWatch Events) rule that invokes an AWS Lambda\nfunction on a schedule to resize uploaded images."
            ],
            "Explanation": "Answer: B D\n",
            "RightAnswer": [
                "B",
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 194,
            "QuestionContent": "A company hosts a data lake on AWS. The data lake consists of data in Amazon S3 and Amazon RDS for\nPostgreSQL. The company needs a reporting solution that provides data visualization and includes all the data\nsources within the data lake. Only the company\u0027s management team should have full access to all the\nvisualizations. The rest of the company should have only limited access.\nWhich solution will meet these requirements?",
            "Option": [
                "A.  Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish\ndashboards to visualize the data. Share the dashboards with the appropriate IAM roles.",
                "B.  Create an analysis in Amazon OuickSighl. Connect all the data sources and create new datasets. Publish\ndashboards to visualize the data. Share the dashboards with the appropriate users and groups.",
                "C.  Create an AWS Glue table and crawler for the data in Amazon S3. Create an AWS Glue extract,\ntransform, and load (ETL) job to produce reports. Publish the reports to Amazon S3. Use S3 bucket\npolicies to limit access to the reports.",
                "D.  Create an AWS Glue table and crawler for the data in Amazon S3. Use Amazon Athena Federated\nQuery to access data within Amazon RDS for PoslgreSQL. Generate reports by using Amazon Athena.\nPublish the reports to Amazon S3. Use S3 bucket policies to limit access to the reports."
            ],
            "Explanation": "Answer: A\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 195,
            "QuestionContent": "A company runs multiple Windows workloads on AWS. The company\u0027s employees use Windows file shares\nthat are hosted on two Amazon EC2 instances. The file shares synchronize data between themselves and\nmaintain duplicate copies. The company wants a highly available and durable storage solution that preserves\nhow users currently access the files.\nWhat should a solutions architect do to meet these requirements?",
            "Option": [
                "A.  Migrate all the data to Amazon S3 Set up IAM authentication for users to access files",
                "B.  Set up an Amazon S3 File Gateway. Mount the S3 File Gateway on the existing EC2 Instances.",
                "C.  Extend the file share environment to Amazon FSx for Windows File Server with a Multi-AZ\nconfiguration. Migrate all the data to FSx for Windows File Server.",
                "D.  Extend the file share environment to Amazon Elastic File System (Amazon EFS) with a Multi-AZ\nconfiguration. Migrate all the data to Amazon EFS."
            ],
            "Explanation": "Answer: C\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 196,
            "QuestionContent": "A media company is evaluating the possibility ot moving rts systems to the AWS Cloud The company needs at\nleast 10 TB of storage with the maximum possible I/O performance for video processing. 300 TB of very\ndurable storage for storing media content, and 900 TB of storage to meet requirements for archival media that\nis not in use anymore\nWhich set of services should a solutions architect recommend to meet these requirements?",
            "Option": [
                "A.  Amazon EBS for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier\nfor archival storage",
                "B.  Amazon EBS for maximum performance, Amazon EFS for durable data storage and Amazon S3 Glacier\nfor archival storage",
                "C.  Amazon EC2 instance store for maximum performance. Amazon EFS for durable data storage and\nAmazon S3 for archival storage",
                "D.  Amazon EC2 Instance store for maximum performance. Amazon S3 for durable data storage, and\nAmazon S3 Glacier for archival storage"
            ],
            "Explanation": "Answer: A\nExplanation\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 197,
            "QuestionContent": "A business\u0027s backup data totals 700 terabytes (TB) and is kept in network attached storage (NAS) at its data\ncenter. This backup data must be available in the event of occasional regulatory inquiries and preserved for a\nperiod of seven years. The organization has chosen to relocate its backup data from its on-premises data center\nto Amazon Web Services (AWS). Within one month, the migration must be completed. The company\u0027s public\ninternet connection provides 500 Mbps of dedicated capacity for data transport.\nWhat should a solutions architect do to ensure that data is migrated and stored at the LOWEST possible cost?",
            "Option": [
                "A.  Order AWS Snowball devices to transfer the data. Use a lifecycle policy to transition the files to\nAmazon S3 Glacier Deep Archive.",
                "B.  Deploy a VPN connection between the data center and Amazon VPC. Use the AWS CLI to copy the\ndata from on premises to Amazon S3 Glacier.",
                "C.  Provision a 500 Mbps AWS Direct Connect connection and transfer the data to Amazon S3. Use a\nlifecycle policy to transition the files to Amazon S3 Glacier Deep Archive.",
                "D.  Use AWS DataSync to transfer the data and deploy a DataSync agent on premises. Use the DataSync\ntask to copy files from the on-premises NAS storage to Amazon S3 Glacier."
            ],
            "Explanation": "Answer: A\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 198,
            "QuestionContent": "A company has a web application lhat is based on Java and PHP The company plans to move the application\nfrom on premises to AWS The company needs the ability to test new site features trequenlty. The company\nalso needs a highly available and managed solution that requires minimum operational overhead\nWhich solution will meel these requirements?",
            "Option": [
                "A.  Create an Amazon S3 bucket Enable static web hosting on the S3 bucket Upload the static content to the\nS3 bucket Use AWS Lambda to process all dynamic content",
                "B.  Deploy the web application to an AWS Elastic Beanstalk environment Use URL swapping to switch\nbetween multiple Elastic Beanstalk environments for feature testing",
                "C.  Deploy the web application lo Amazon EC2 instances that are configured with Java and PHP Use Auto\nScaling groups and an Application Load Balancer to manage the website\u0027s availability",
                "D.  Containerize the web application Deploy the web application to Amazon EC2 instances Use the AWS\nLoad Balancer Controller to dynamically route traffic between containers thai contain the new site\nfeatures for testing"
            ],
            "Explanation": "Answer: B\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 199,
            "QuestionContent": "A telemarketing company is designing its customer call center functionality on AWS. The company needs a\nsolution Diet provides multiples ipsafcar rvcognrfeon and generates transcript files The company wants to\nquery the transcript files to analyze the business patterns The transcript files must be stored for 7 years for\nauditing piloses.\nWhich solution will meet these requirements?",
            "Option": [
                "A.  Use Amazon Rekognition for multiple speaker recognition. Store the transcript files in Amazon S3 Use\nmachine teaming models for transcript file analysis",
                "B.  Use Amazon Transcribe for multiple speaker recognition. Use Amazon Athena tot transcnpt file analysts",
                "C.  Use Amazon Translate lor multiple speaker recognition. Store the transcript files in Amazon Redshift\nUse SQL queues lor transcript file analysis",
                "D.  Use Amazon Rekognition for multiple speaker recognition. Store the transcnpt files in Amazon S3 Use\nAmazon Textract for transcnpt file analysis"
            ],
            "Explanation": "Answer: C\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 200,
            "QuestionContent": "A company runs a production application on a fleet of Amazon EC2 instances. The application reads the data\nfrom an Amazon SQS queue and processes the messages in parallel. The message volume is unpredictable and\noften has intermittent traffic. This application should continually process messages without any downtime.\nWhich solution meets these requirements MOST cost-effectively?",
            "Option": [
                "A.  Use Spot Instances exclusively to handle the maximum capacity required.",
                "B.  Use Reserved Instances exclusively to handle the maximum capacity required.",
                "C.  Use Reserved Instances for the baseline capacity and use Spot Instances to handle additional capacity.",
                "D.  Use Reserved Instances for the baseline capacity and use On-Demand Instances to handle additional\ncapacity."
            ],
            "Explanation": "Answer: D\nExplanation\nWe recommend that you use On-Demand Instances for applications with short-term, irregular workloads that\ncannot be interrupted.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-on-demand-instances.html\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 201,
            "QuestionContent": "A company provides a Voice over Internet Protocol (VoIP) service that uses UDP connections. The service\nconsists of Amazon EC2 instances that run in an Auto Scaling group. The company has deployments across\nmultiple AWS Regions.\nThe company needs to route users to the Region with the lowest latency. The company also needs automated\nfailover between Regions.\nWhich solution will meet these requirements?",
            "Option": [
                "A.  Deploy a Network Load Balancer (NLB) and an associated target group. Associate the target group with\nthe Auto Scaling group. Use the NLB as an AWS Global Accelerator endpoint in each Region.",
                "B.  Deploy an Application Load Balancer (ALB) and an associated target group. Associate the target group\nwith the Auto Scaling group. Use the ALB as an AWS Global Accelerator endpoint in each Region.",
                "C.  Deploy a Network Load Balancer (NLB) and an associated target group. Associate the target group with\nthe Auto Scaling group. Create an Amazon Route 53 latency record that points to aliases for each NLB.\nCreate an Amazon CloudFront distribution that uses the latency record as an origin.",
                "D.  Deploy an Application Load Balancer (ALB) and an associated target group. Associate the target group\nwith the Auto Scaling group. Create an Amazon Route 53 weighted record that points to aliases for each\nALB. Deploy an Amazon CloudFront distribution that uses the weighted record as an origin."
            ],
            "Explanation": "Answer: A\nExplanation\nhttps://aws.amazon.com/global-accelerator/faqs/\nHTTP /HTTPS - ALB ; TCP and UDP - NLB; Lowest latency routing and more throughput. Also supports\nfailover, uses Anycast Ip addressing - Global Accelerator Caching at Egde Locations \u2013 Cloutfront\nWS Global Accelerator automatically checks the health of your applications and routes user traffic only to\nhealthy application endpoints. If the health status changes or you make configuration updates, AWS Global\nAccelerator reacts instantaneously to route your users to the next available endpoint..\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 202,
            "QuestionContent": "A company has a service that produces event data. The company wants to use AWS to process the event data\nas it is received. The data is written in a specific order that must be maintained throughout processing The\ncompany wants to implement a solution that minimizes operational overhead.\nHow should a solutions architect accomplish this?",
            "Option": [
                "A.  Create an Amazon Simple Queue Service (Amazon SQS) FIFO queue to hold messages Set up an AWS\nLambda function to process messages from the queue",
                "B.  Create an Amazon Simple Notification Service (Amazon SNS) topic to deliver notifications containing\npayloads to process Configure an AWS Lambda function as a subscriber.",
                "C.  Create an Amazon Simple Queue Service (Amazon SQS) standard queue to hold messages. Set up an\nAWS Lambda function to process messages from the queue independently",
                "D.  Create an Amazon Simple Notification Service (Amazon SNS) topic to deliver notifications containing\npayloads to process. Configure an Amazon Simple Queue Service (Amazon SQS) queue as a subscriber."
            ],
            "Explanation": "Answer: A\nExplanation\nThe details are revealed in below url:\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html\nFIFO (First-In-First-Out) queues are designed to enhance messaging between applications when the order of\noperations and events is critical, or where duplicates can\u0027t be tolerated. Examples of situations where you\nmight use FIFO queues include the following: To make sure that user-entered commands are run in the right\norder. To display the correct product price by sending price modifications in the right order. To prevent a\nstudent from enrolling in a course before registering for an account.\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 203,
            "QuestionContent": "A company has an AWS account used for software engineering. The AWS account has access to the\ncompany\u0027s on-premises data center through a pair of AWS Direct Connect connections. All non-VPC traffic\nroutes to the virtual private gateway.\nA development team recently created an AWS Lambda function through the console. The development team\nneeds to allow the function to access a database that runs in a private subnet in the company\u0027s data center.\nWhich solution will meet these requirements?",
            "Option": [
                "A.  Configure the Lambda function to run in the VPC with the appropriate security group.",
                "B.  Set up a VPN connection from AWS to the data center. Route the traffic from the Lambda function\nthrough the VPN.",
                "C.  Update the route tables in the VPC to allow the Lambda function to access the on-premises data center\nthrough Direct Connect.",
                "D.  Create an Elastic IP address. Configure the Lambda function to send traffic through the Elastic IP\naddress without an elastic network interface."
            ],
            "Explanation": "Answer: A\nExplanation\nhttps://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html#vpc-managing-eni\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 204,
            "QuestionContent": "A company has a legacy data processing application that runs on Amazon EC2 instances. Data is processed\nsequentially, but the order of results does not matter. The application uses a monolithic architecture. The only\nway that the company can scale the application to meet increased demand is to increase the size of the\ninstances.\nThe company\u0027s developers have decided to rewrite the application to use a microservices architecture on\nAmazon Elastic Container Service (Amazon ECS).\nWhat should a solutions architect recommend for communication between the microservices?",
            "Option": [
                "A.  Create an Amazon Simple Queue Service (Amazon SQS) queue. Add code to the data producers, and\nsend data to the queue. Add code to the data consumers to process data from the queue.",
                "B.  Create an Amazon Simple Notification Service (Amazon SNS) topic. Add code to the data producers,\nand publish notifications to the topic. Add code to the data consumers to subscribe to the topic.",
                "C.  Create an AWS Lambda function to pass messages. Add code to the data producers to call the Lambda\nfunction with a data object. Add code to the data consumers to receive a data object that is passed from\nthe Lambda function.",
                "D.  Create an Amazon DynamoDB table. Enable DynamoDB Streams. Add code to the data producers to\ninsert data into the table. Add code to the data consumers to use the DynamoDB Streams API to detect\nnew table entries and retrieve the data."
            ],
            "Explanation": "Answer: A\nExplanation\nQueue has Limited throughput (300 msg/s without batching, 3000 msg/s with batching whereby up-to 10 msg\nper batch operation; Msg duplicates not allowed in the queue (exactly-once delivery); Msg order is preserved\n(FIFO); Queue name must end with .fifo\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 205,
            "QuestionContent": "An ecommerce company has an order-processing application that uses Amazon API Gateway and an AWS\nLambda function. The application stores data in an Amazon Aurora PostgreSQL database. During a recent\nsales event, a sudden surge in customer orders occurred. Some customers experienced timeouts and the\napplication did not process the orders of those customers A solutions architect determined that the CPU\nutilization and memory utilization were high on the database because of a large number of open connections\nThe solutions architect needs to prevent the timeout errors while making the least possible changes to the\napplication.\nWhich solution will meet these requirements?",
            "Option": [
                "A.  Configure provisioned concurrency for the Lambda function Modify the database to be a global\ndatabase in multiple AWS Regions",
                "B.  Use Amazon RDS Proxy to create a proxy for the database Modify the Lambda function to use the RDS\nProxy endpoint instead of the database endpoint",
                "C.  Create a read replica for the database in a different AWS Region Use query string parameters in API\nGateway to route traffic to the read replica",
                "D.  Migrate the data from Aurora PostgreSQL to Amazon DynamoDB by using AWS Database Migration\nService (AWS DMS| Modify the Lambda function to use the OynamoDB table"
            ],
            "Explanation": "Answer: D\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 206,
            "QuestionContent": "An application runs on Amazon EC2 instances across multiple Availability Zones The instances run in an\nAmazon EC2 Auto Scaling group behind an Application Load Balancer The application performs best when\nthe CPU utilization of the EC2 instances is at or near 40%.\nWhat should a solutions architect do to maintain the desired performance across all instances in the group?",
            "Option": [
                "A.  Use a simple scaling policy to dynamically scale the Auto Scaling group",
                "B.  Use a target tracking policy to dynamically scale the Auto Scaling group",
                "C.  Use an AWS Lambda function to update the desired Auto Scaling group capacity.",
                "D.  Use scheduled scaling actions to scale up and scale down the Auto Scaling group"
            ],
            "Explanation": "Answer: B\nExplanation\nhttps://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-target-tracking.html\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 207,
            "QuestionContent": "A company hosts its web applications in the AWS Cloud. The company configures Elastic Load Balancers to\nuse certificate that are imported into AWS Certificate Manager (ACM). The company\u2019s security team must be\nnotified 30 days before the expiration of each certificate.\nWhat should a solutions architect recommend to meet the requirement?",
            "Option": [
                "A.  Add a rule m ACM to publish a custom message to an Amazon Simple Notification Service (Amazon\nSNS) topic every day beginning 30 days before any certificate will expire.",
                "B.  Create an AWS Config rule that checks for certificates that will expire within 30 days. Configure\nAmazon EventBridge (Amazon CloudWatch Events) to invoke a custom alert by way of Amazon\nSimple Notification Service (Amazon SNS) when AWS Config reports a noncompliant resource",
                "C.  Use AWS trusted Advisor to check for certificates that will expire within to days. Create an Amazon\nCloudWatch alarm that is based on Trusted Advisor metrics for check status changes Configure the\nalarm to send a custom alert by way of Amazon Simple rectification Service (Amazon SNS)",
                "D.  Create an Amazon EventBridge (Amazon CloudWatch Events) rule to detect any certificates that will\nexpire within 30 days. Configure the rule to invoke an AWS Lambda function. Configure the Lambda\nfunction to send a custom alert by way of Amazon Simple Notification Service (Amazon SNS)."
            ],
            "Explanation": "Answer: B\nExplanation\nhttps://aws.amazon.com/premiumsupport/knowledge-center/acm-certificate-expiration/\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 208,
            "QuestionContent": "A company selves a dynamic website from a flee! of Amazon EC2 instances behind an Application Load\nBalancer (ALB) The website needs to support multiple languages to serve customers around the world The\nwebsite\u0027s architecture is running in the us-west-1 Region and is exhibiting high request lalency tor users that\nare located in other parts ot the world\nThe website needs to serve requests quickly and efficiently regardless of a user\u0027s location However the\ncompany does not want to recreate the existing architecture across multiple Regions\nWhat should a solutions architect do to meet these requirements?",
            "Option": [
                "A.  Replace the existing architecture with a website that is served from an Amazon S3 bucket Configure an\nAmazon CloudFront distribution with the S3 bucket as the ongin Set the cache behavior settings to\ncache based on the Accept-Languege request header",
                "B.  Configure an Amazon CloudFront distribution with the ALB as the origin Set Ihe cache behavior\nsettings to cache based on the Accept-Language request header",
                "C.  Create an Amazon API Gateway API that is integrated with the ALB Configure the API to use the\nHTTP integration type Set up an API Gateway stage to enable the API cache based on the\nAccept-Language request header",
                "D.  Launch an EC2 instance in each additional Region and configure NGINX to act as a cache server for\nthat Region Put all the EC2 instances and the ALB behind an Amazon Route 53 record set with a\ngeotocation routing policy"
            ],
            "Explanation": "Answer: B\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 209,
            "QuestionContent": "A company recently migrated to AWS and wants to implement a solution to protect the traffic that flows in\nand out of the production VPC. The company had an inspection server in its on-premises data center. The\ninspection server performed specific operations such as traffic flow inspection and traffic filtering. The\ncompany wants to have the same functionalities in the AWS Cloud.\nWhich solution will meet these requirements?",
            "Option": [
                "A.  Use Amazon GuardDuty for traffic inspection and traffic filtering in the production VPC",
                "B.  Use Traffic Mirroring to mirror traffic from the production VPC for traffic inspection and filtering.",
                "C.  Use AWS Network Firewall to create the required rules for traffic inspection and traffic filtering for the\nproduction VPC.",
                "D.  Use AWS Firewall Manager to create the required rules for traffic inspection and traffic filtering for the\nproduction VPC."
            ],
            "Explanation": "Answer: C\nExplanation\nAWS Network Firewall supports both inspection and filtering as required\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 210,
            "QuestionContent": "A company stores its application logs in an Amazon CloudWatch Logs log group. A new policy requires the\ncompany to store all application logs in Amazon OpenSearch Service (Amazon Elasticsearch Service) in\nnear-real time.\nWhich solution will meet this requirement with the LEAST operational overhead?",
            "Option": [
                "A.  Configure a CloudWatch Logs subscription to stream the logs to Amazon OpenSearch Service (Amazon\nElasticsearch Service).",
                "B.  Create an AWS Lambda function. Use the log group to invoke the function to write the logs to Amazon\nOpenSearch Service (Amazon Elasticsearch Service).",
                "C.  Create an Amazon Kinesis Data Firehose delivery stream. Configure the log group as the delivery\nstream\u0027s source. Configure Amazon OpenSearch Service (Amazon Elasticsearch Service) as the delivery\nstream\u0027s destination.",
                "D.  Install and configure Amazon Kinesis Agent on each application server to deliver the logs to Amazon\nKinesis Data Streams. Configure Kinesis Data Streams to deliver the logs to Amazon OpenSearch\nService (Amazon Elasticsearch Service)"
            ],
            "Explanation": "Answer: B\nExplanation\nhttps://computingforgeeks.com/stream-logs-in-aws-from-cloudwatch-to-elasticsearch/\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 211,
            "QuestionContent": "A company is planning to use an Amazon DynamoDB table for data storage. The company is concerned about\ncost optimization. The table will not be used on most mornings. In the evenings, the read and write traffic will\noften be unpredictable. When traffic spikes occur, they will happen very quickly.\nWhat should a solutions architect recommend?",
            "Option": [
                "A.  Create a DynamoDB table in on-demand capacity mode.",
                "B.  Create a DynamoDB table with a global secondary index.",
                "C.  Create a DynamoDB table with provisioned capacity and auto scaling.",
                "D.  Create a DynamoDB table in provisioned capacity mode, and configure it as a global table."
            ],
            "Explanation": "Answer: A\n",
            "RightAnswer": [
                "A"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 212,
            "QuestionContent": "A company hosts its application on AWS The company uses Amazon Cognito to manage users When users\nlog in to the application the application fetches required data from Amazon DynamoOB by using a REST API\nthat is hosted in Amazon API Gateway. The company wants an AWS managed solution that will control\naccess to the REST API to reduce\ndevelopment efforts\nWhich solution will meet these requirements with the LEAST operational overhead?",
            "Option": [
                "A.  Configure an AWS Lambda function to be an authorize! in API Gateway to validate which user made\nthe request",
                "B.  For each user, create and assign an API key that must be sent with each request Validate the key by\nusing an AWS Lambda function",
                "C.  Send the user\u0027s email address in the header with every request Invoke an AWS Lambda function to\nvalidate that the user with that email address has proper access",
                "D.  Configure an Amazon Cognito user pool authorizer in API Gateway to allow Amazon Cognito to\nvalidate each request"
            ],
            "Explanation": "Answer: D\n",
            "RightAnswer": [
                "D"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 213,
            "QuestionContent": "A company is developing a two-tier web application on AWS. The company\u0027s developers have deployed the\napplication on an Amazon EC2 instance that connects directly to a backend Amazon RDS database. The\ncompany must not hardcode database credentials in the application. The company must also implement a\nsolution to automatically rotate the database credentials on a regular basis.\nWhich solution will meet these requirements with the LEAST operational overhead?",
            "Option": [
                "A.  Store the database credentials in the instance metadata. Use Amazon EventBridge (Amazon\nCloudWatch Events) rules to run a scheduled AWS Lambda function that updates the RDS credentials\nand instance metadata at the same time.",
                "B.  Store the database credentials in a configuration file in an encrypted Amazon S3 bucket. Use Amazon\nEventBridge (Amazon CloudWatch Events) rules to run a scheduled AWS Lambda function that\nupdates the RDS credentials and the credentials in the configuration file at the same time. Use S3\nVersioning to ensure the ability to fall back to previous values.",
                "C.  Store the database credentials as a secret in AWS Secrets Manager. Turn on automatic rotation for the\nsecret. Attach the required permission to the EC2 role to grant access to the secret.",
                "D.  Store the database credentials as encrypted parameters in AWS Systems Manager Parameter Store. Turn\non automatic rotation for the encrypted parameters. Attach the required permission to the EC2 role to\ngrant access to the encrypted parameters."
            ],
            "Explanation": "Answer: C\nExplanation\nhttps://docs.aws.amazon.com/secretsmanager/latest/userguide/create_database_secret.html\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 214,
            "QuestionContent": "A company is designing an application where users upload small files into Amazon S3. After a user uploads a\nfile, the file requires one-time simple processing to transform the data and save the data in JSON format for\nlater analysis.\nEach file must be processed as quickly as possible after it is uploaded. Demand will vary. On some days, users\nwill upload a high number of files. On other days, users will upload a few files or no files.\nWhich solution meets these requirements with the LEAST operational overhead?",
            "Option": [
                "A.  Configure Amazon EMR to read text files from Amazon S3. Run processing scripts to transform the\ndata. Store the resulting JSON file in an Amazon Aurora DB cluster.",
                "B.  Configure Amazon S3 to send an event notification to an Amazon Simple Queue Service (Amazon\nSQS) queue. Use Amazon EC2 instances to read from the queue and process the data. Store the resulting\nJSON file in Amazon DynamoDB.",
                "C.  Configure Amazon S3 to send an event notification to an Amazon Simple Queue Service (Amazon\nSQS) queue. Use an AWS Lambda function to read from the queue and process the data. Store the\nresulting JSON file in Amazon DynamoDB. Most Voted",
                "D.  Configure Amazon EventBridge (Amazon CloudWatch Events) to send an event to Amazon Kinesis\nData Streams when a new file is uploaded. Use an AWS Lambda function to consume the event from\nthe stream and process the data. Store the resulting JSON file in Amazon Aurora DB cluster."
            ],
            "Explanation": "Answer: C\nExplanation\nAmazon S3 sends event notifications about S3 buckets (for example, object created, object removed, or object\nrestored) to an SNS topic in the same Region.\nThe SNS topic publishes the event to an SQS queue in the central Region.\nThe SQS queue is configured as the event source for your Lambda function and buffers the event messages for\nthe Lambda function.\nThe Lambda function polls the SQS queue for messages and processes the Amazon S3 event notifications\naccording to your application\u2019s requirements.\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/subscribe-a-lambda-function-to-event-notifications-from-s3-buckets-in-different-aws-regions.html\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 215,
            "QuestionContent": "A company wants to build a scalable key management Infrastructure to support developers who need to\nencrypt data in their applications.\nWhat should a solutions architect do to reduce the operational burden?",
            "Option": [
                "A.  Use multifactor authentication (MFA) to protect the encryption keys.",
                "B.  Use AWS Key Management Service (AWS KMS) to protect the encryption keys",
                "C.  Use AWS Certificate Manager (ACM) to create, store, and assign the encryption keys",
                "D.  Use an IAM policy to limit the scope of users who have access permissions to protect the encryption\nkeys"
            ],
            "Explanation": "Answer: B\nExplanation\nhttps://aws.amazon.com/kms/faqs/#:~:text=If%20you%20are%20a%20developer%20who%20needs%20to%20digitally,a%20broad%20set%20of%20industry%20and%20regional%20compliance%20regimes.\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 216,
            "QuestionContent": "A company runs an ecommerce application on Amazon EC2 instances behind an Application Load Balancer.\nThe instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. The Auto\nScaling group scales based on CPU utilization metrics. The ecommerce application stores the transaction data\nin a MySQL 8.0 database that is hosted on a large EC2 instance.\nThe database\u0027s performance degrades quickly as application load increases. The application handles more read\nrequests than write transactions. The company wants a solution that will automatically scale the database to\nmeet the demand of unpredictable read workloads while maintaining high availability.\nWhich solution will meet these requirements?",
            "Option": [
                "A.  Use Amazon Redshift with a single node for leader and compute functionality.",
                "B.  Use Amazon RDS with a Single-AZ deployment Configure Amazon RDS to add reader instances in a\ndifferent Availability Zone.",
                "C.  Use Amazon Aurora with a Multi-AZ deployment. Configure Aurora Auto Scaling with Aurora\nReplicas.",
                "D.  Use Amazon ElastiCache for Memcached with EC2 Spot Instances."
            ],
            "Explanation": "Answer: C\nExplanation\nAURORA is 5x performance improvement over MySQL on RDS and handles more read requests than write,;\nmaintaining high availability = Multi-AZ deployment\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 217,
            "QuestionContent": "A global company hosts its web application on Amazon EC2 instances behind an Application Load Balancer\n(ALB). The web application has static data and dynamic data. The company stores its static data in an Amazon\nS3 bucket. The company wants to improve performance and reduce latency for the static data and dynamic\ndata. The company is using its own domain name registered with Amazon Route 53.\nWhat should a solutions architect do to meet these requirements?",
            "Option": [
                "A.  Create an Amazon CloudFront distribution that has the S3 bucket and the ALB as origins Configure\nRoute 53 to route traffic to the CloudFront distribution.",
                "B.  Create an Amazon CloudFront distribution that has the ALB as an origin Create an AWS Global\nAccelerator standard accelerator that has the S3 bucket as an endpoint. Configure Route 53 to route\ntraffic to the CloudFront distribution.",
                "C.  Create an Amazon CloudFront distribution that has the S3 bucket as an origin Create an AWS Global\nAccelerator standard accelerator that has the ALB and the CloudFront distribution as endpoints Create a\ncustom domain name that points to the accelerator DNS name Use the custom domain name as an\nendpoint for the web application.",
                "D.  Create an Amazon CloudFront distribution that has the ALB as an origin C. Create an AWS Global\nAccelerator standard accelerator that has the S3 bucket as an endpoint Create two domain names. Point\none domain name to the CloudFront DNS name for dynamic content, Point the other domain name to\nthe accelerator DNS name for static content Use the domain names as endpoints for the web application."
            ],
            "Explanation": "Answer: C\nExplanation\nStatic content can be cached at Cloud front Edge locations from S3 and dynamic content EC2 behind the ALB\nwhose performance can be improved by Global Accelerator whose one endpoint is ALB and other Cloud front.\nSo with regards to custom domain name endpoint is web application is R53 alias records for the custom\ndomain point to web application\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/improving-availability-and-performance-for-application-load-balancers-using-one-click-integration-with-aws-global-accelerator/\n",
            "RightAnswer": [
                "C"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 218,
            "QuestionContent": "A company has an application that generates a large number of files, each approximately 5 MB in size. The\nfiles are stored in Amazon S3. Company policy requires the files to be stored for 4 years before they can be\ndeleted Immediate accessibility is always required as the files contain critical business data that is not easy to\nreproduce. The files are frequently accessed in the first 30 days of the object creation but are rarely accessed\nafter the first 30 days\nWhich storage solution is MOST cost-effective?",
            "Option": [
                "A.  Create an S3 bucket lifecycle policy to move Mm from S3 Standard to S3 Glacier 30 days from object\ncreation Delete the Tiles 4 years after object creation",
                "B.  Create an S3 bucket lifecycle policy to move tiles from S3 Standard to S3 One Zone-infrequent Access\n(S3 One Zone-IA] 30 days from object creation. Delete the fees 4 years after object creation",
                "C.  Create an S3 bucket lifecycle policy to move files from S3 Standard-infrequent Access (S3 Standard\n-lA) 30 from object creation. Delete the ties 4 years after object creation",
                "D.  Create an S3 bucket Lifecycle policy to move files from S3 Standard to S3 Standard-Infrequent Access\n(S3 Standard-IA) 30 days from object creation Move the files to S3 Glacier 4 years after object carton."
            ],
            "Explanation": "Answer: B\nExplanation\nhttps://aws.amazon.com/s3/storage-classes/?trk=66264cd8-3b73-416c-9693-ea7cf4fe846a\u0026sc_channel=ps\u0026s_kwcid=AL!4422!3!536452716950!p!!g!!aws%20s3%20pricing\u0026ef_id=Cj0KCQjwnbmaBhD-ARIsAGTPcfVHUZN5_BMrzl5zBcaC8KnqpnNZvjbZzqPkH6k7q4JcYO5KFLx0YYgaAm6nEALw_wcB:G:s\u0026s_kwcid=AL!4422!3!536452716950!p!!g!!aws%20s3%20pricing\n",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        },
        {
            "QuestionNumber": 219,
            "QuestionContent": "A company is running a business-critical web application on Amazon EC2 instances behind an Application\nLoad Balancer. The EC2 instances are in an Auto Scaling group. The application uses an Amazon Aurora\nPostgreSQL database that is deployed in a single Availability Zone. The company wants the application to be\nhighly available with minimum downtime and minimum loss of data.\nWhich solution will meet these requirements with the LEAST operational effort?",
            "Option": [
                "A.  Place the EC2 instances in different AWS Regions. Use Amazon Route 53 health checks to redirect\ntraffic. Use Aurora PostgreSQL Cross-Region Replication.",
                "B.  Configure the Auto Scaling group to use multiple Availability Zones. Configure the database as\nMulti-AZ. Configure an Amazon RDS Proxy instance for the database.",
                "C.  Configure the Auto Scaling group to use one Availability Zone. Generate hourly snapshots of the\ndatabase. Recover the database from the snapshots in the event of a failure.",
                "D.  Configure the Auto Scaling group to use multiple AWS Regions. Write the data from the application to\nAmazon S3. Use S3 Event Notifications to launch an AWS Lambda function to write the data to the\ndatabase."
            ],
            "Explanation": "Answer: B\nmarks4sure.com was founded in 2007. We provide latest \u0026 high quality IT / Business Certification Training Exam\nQuestions, Study Guides, Practice Tests.\nWe help you pass any IT / Business Certification Exams with 100% Pass Guaranteed or Full Refund. Especially\nCisco, CompTIA, Citrix, EMC, HP, Oracle, VMware, Juniper, Check Point, LPI, Nortel, EXIN and so on.\nView list of all certification exams: All vendors\n   \n   \n   \nWe prepare state-of-the art practice tests for certification exams. You can reach us at any of the email addresses listed\nbelow.\nSales: sales@marks4sure.com\nFeedback: feedback@marks4sure.com\nSupport: support@marks4sure.com\nAny problems about IT certification or our products, You can write us back and we will get back to you within 24\nhours.",
            "RightAnswer": [
                "B"
            ],
            "QuestionChoose": []
        }
    ],
    "isFinish": false
}