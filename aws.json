{"Question":[{"QuestionNumber":1,"QuestionContent":" \nA company hosts an application on AWS Lambda functions mat are invoked by an Amazon API Gateway API \nThe Lambda functions save customer data to an Amazon Aurora MySQL database Whenever the company \nupgrades the database, the Lambda functions fail to establish database connections until the upgrade is \ncomplete The result is that customer data Is not recorded for some of the event \n \nA solutions architect needs to design a solution that stores customer data that is created during database \nupgrades \n \nWhich solution will meet these requirements? \n ","Option":["A. Provision an Amazon RDS proxy to sit between the Lambda functions and the database Configure the \nLambda functions to connect to the RDS proxy ","B. Increase the run time of me Lambda functions to the maximum Create a retry mechanism in the code \nthat stores the customer data in the database ","C. Persist the customer data to Lambda local storage. Configure new Lambda functions to scan the local \nstorage to save the customer data to the database. ","D. Store the customer data m an Amazon Simple Queue Service (Amazon SOS) FIFO queue Create a new \nLambda function that polls the queue and stores the customer data in the database "],"Explanation":"Answer: D \n \nExplanation \nhttps://www.learnaws.org/2020/12/13/aws-rds-proxy-deep-dive/ \n \nRDS proxy can improve application availability in such a situation by waiting for the new database instance to \nbe functional and maintaining any requests received from the application during this time. The end result is \nthat the application is more resilient to issues with the underlying database. \n \nThis will enable solution to hold data till the time DB comes back to normal. RDS proxy is to optimally utilize \nthe connection between Lambda and DB. Lambda can open multiple connection concurrently which can be \ntaxing on DB compute resources, hence RDS proxy was introduced to manage and leverage these connections \nefficiently. \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":2,"QuestionContent":" \nA company recently launched Linux-based application instances on Amazon EC2 in a private subnet and \nlaunched a Linux-based bastion host on an Amazon EC2 instance in a public subnet of a VPC A solutions \narchitect needs to connect from the on-premises network, through the company\u0027s internet connection to the \nbastion host and to the application servers The solutions architect must make sure that the security groups of \nall the EC2 instances will allow that access \n \n  2 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nWhich combination of steps should the solutions architect take to meet these requirements? (Select TWO) \n ","Option":["A. Replace the current security group of the bastion host with one that only allows inbound access from the \napplication instances ","B. Replace the current security group of the bastion host with one that only allows inbound access from the \ninternal IP range for the company ","C. Replace the current security group of the bastion host with one that only allows inbound access from the \nexternal IP range for the company ","D. Replace the current security group of the application instances with one that allows inbound SSH access \nfrom only the private IP address of the bastion host ","E. Replace the current security group of the application instances with one that allows inbound SSH access \nfrom only the public IP address of the bastion host "],"Explanation":"Answer: C D \n \nExplanation \nhttps://digitalcloud.training/ssh-into-ec2-in-private-subnet/ \n \n","RightAnswer":["C","D"],"QuestionChoose":[]},{"QuestionNumber":3,"QuestionContent":" \nA company is building an ecommerce web application on AWS. The application sends information about new \norders to an Amazon API Gateway REST API to process. The company wants to ensure that orders are \nprocessed in the order that they are received. \n \nWhich solution will meet these requirements? \n ","Option":["A. Use an API Gateway integration to publish a message to an Amazon Simple Notification Service \n(Amazon SNS) topic when the application receives an order. Subscribe an AWS Lambda function to the \ntopic to perform processing. ","B. Use an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) \nFIFO queue when the application receives an order. Configure the SQS FIFO queue to invoke an AWS \nLambda function for processing. ","C. Use an API Gateway authorizer to block any requests while the application processes an order. ","D. Use an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) \nstandard queue when the application receives an order. Configure the SQS standard queue to invoke an \nAWS Lambda function for processing. "],"Explanation":"Answer: B \n \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":4,"QuestionContent":" \n  3 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \n \nA company needs the ability to analyze the log files of its proprietary application. The logs are stored in JSON \nformat in an Amazon S3 bucket Queries will be simple and will run on-demand A solutions architect needs to \nperform the analysis with minimal changes to the existing architecture \n \nWhat should the solutions architect do to meet these requirements with the LEAST amount of operational \noverhead? \n ","Option":["A. Use Amazon Redshift to load all the content into one place and run the SQL queries as needed ","B. Use Amazon CloudWatch Logs to store the logs Run SQL queries as needed from the Amazon \nCloudWatch console ","C. Use Amazon Athena directly with Amazon S3 to run the queries as needed ","D. Use AWS Glue to catalog the logs Use a transient Apache Spark cluster on Amazon EMR to run the \nSQL queries as needed "],"Explanation":"Answer: C \n \nExplanation \nAmazon Athena can be used to query JSON in S3 \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":5,"QuestionContent":" \nA company maintains a searchable repository of items on its website. The data is stored in an Amazon RDS \nfor MySQL database table that contains more than 10 million rows The database has 2 TB of General Purpose \nSSD storage There are millions of updates against this data every day through the company\u0027s website \n \nThe company has noticed that some insert operations are taking 10 seconds or longer The company has \ndetermined that the database storage performance is the problem \n \nWhich solution addresses this performance issue? \n ","Option":["A. Change the storage type to Provisioned IOPS SSD ","B. Change the DB instance to a memory optimized instance class ","C. Change the DB instance to a burstable performance instance class ","D. Enable Multi-AZ RDS read replicas with MySQL native asynchronous replication. "],"Explanation":"Answer: A \n \nExplanation \nhttps://aws.amazon.com/ebs/features/ \n \n\u0022Provisioned IOPS volumes are backed by solid-state drives (SSDs) and are the highest performance EBS \n \n  4 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nvolumes designed for your critical, I/O intensive database applications. \n \nThese volumes are ideal for both IOPS-intensive and throughput-intensive workloads that require extremely \nlow latency.\u0022 \n \nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Storage.html \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":6,"QuestionContent":" \nA company stores call transcript files on a monthly basis. Users access the files randomly within 1 year of the \ncall, but users access the files infrequently after 1 year. The company wants to optimize its solution by giving \nusers the ability to query and retrieve files that are less than 1-year-old as quickly as possible. A delay in \nretrieving older files is acceptable. \n \nWhich solution will meet these requirements MOST cost-effectively? \n ","Option":["A. Store individual files with tags in Amazon S3 Glacier Instant Retrieval. Query the tags to retrieve the \nfiles from S3 Glacier Instant Retrieval. ","B. Store individual files in Amazon S3 Intelligent-Tiering. Use S3 Lifecycle policies to move the files to \nS3 Glacier Flexible Retrieval after 1 year. Query and retrieve the files that are in Amazon S3 by using \nAmazon Athena. Query and retrieve the files that are in S3 Glacier by using S3 Glacier Select. ","C. Store individual files with tags in Amazon S3 Standard storage. Store search metadata for each archive \nin Amazon S3 Standard storage. Use S3 Lifecycle policies to move the files to S3 Glacier Instant \nRetrieval after 1 year. Query and retrieve the files by searching for metadata from Amazon S3. ","D. Store individual files in Amazon S3 Standard storage. Use S3 Lifecycle policies to move the files to S3 \nGlacier Deep Archive after 1 year. Store search metadata in Amazon RDS. Query the files from \nAmazon RDS. Retrieve the files from S3 Glacier Deep Archive. "],"Explanation":"Answer: B \n \nExplanation \n\u0022For archive data that needs immediate access, such as medical images, news media assets, or genomics data, \nchoose the S3 Glacier Instant Retrieval storage class, an archive storage class that delivers the lowest cost \nstorage with milliseconds retrieval. For archive data that does not require immediate access but needs the \nflexibility to retrieve large sets of data at no cost, such as backup or disaster recovery use cases, choose S3 \nGlacier Flexible Retrieval (formerly S3 Glacier), with retrieval in minutes or free bulk retrievals in 5-12 \nhours.\u0022 \nhttps://aws.amazon.com/about-aws/whats-new/2021/11/amazon-s3-glacier-instant-retrieval-storage-class/ \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":7,"QuestionContent":" \nA company runs its Infrastructure on AWS and has a registered base of 700.000 users for res document \nmanagement application The company intends to create a product that converts large pdf files to jpg Imago \nfiles. The .pdf files average 5 MB in size. The company needs to store the original files and the converted files. \nA solutions architect must design a scalable solution to accommodate demand that will grow rapidly over lime. \n \n  5 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nWhich solution meets these requirements MOST cost-effectively? \n ","Option":["A. Save the pdf files to Amazon S3 Configure an S3 PUT event to invoke an AWS Lambda function to \nconvert the files to jpg format and store them back in Amazon S3 ","B. Save the pdf files to Amazon DynamoDB. Use the DynamoDB Streams feature to invoke an AWS \nLambda function to convert the files to jpg format and store them hack in DynamoDB ","C. Upload the pdf files to an AWS Elastic Beanstalk application that includes Amazon EC2 instances. \nAmazon Elastic Block Store (Amazon EBS) storage and an Auto Scaling group. Use a program In the \nEC2 instances to convert the files to jpg format Save the .pdf files and the .jpg files In the EBS store. ","D. Upload the .pdf files to an AWS Elastic Beanstalk application that includes Amazon EC2 instances, \nAmazon Elastic File System (Amazon EPS) storage, and an Auto Scaling group. Use a program in the \nEC2 instances to convert the file to jpg format Save the pdf files and the jpg files in the EBS store. "],"Explanation":"Answer: A \n \nExplanation \nElastic BeanStalk is expensive, and DocumentDB has a 400KB max to upload files. So Lambda and S3 should \nbe the one. \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":8,"QuestionContent":" \nA company wants to migrate its on-premises application to AWS. The application produces output files that \nvary in size from tens of gigabytes to hundreds of terabytes The application data must be stored in a standard \nfile system structure The company wants a solution that scales automatically, is highly available, and requires \nminimum operational overhead. \n \nWhich solution will meet these requirements? \n ","Option":["A. Migrate the application to run as containers on Amazon Elastic Container Service (Amazon ECS) Use \nAmazon S3 for storage ","B. Migrate the application to run as containers on Amazon Elastic Kubernetes Service (Amazon EKS) Use \nAmazon Elastic Block Store (Amazon EBS) for storage ","C. Migrate the application to Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon \nElastic File System (Amazon EFS) for storage. ","D. Migrate the application to Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon \nElastic Block Store (Amazon EBS) for storage. "],"Explanation":"Answer: C \n \nExplanation \nEFS is a standard file system, it scales automatically and is highly available. \n \n  6 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":9,"QuestionContent":" \nA company uses 50 TB of data for reporting. The company wants to move this data from on premises to AWS \nA custom application in the company\u0027s data center runs a weekly data transformation job. The company plans \nto pause the application until the data transfer is complete and needs to begin the transfer process as soon as \npossible. \n \nThe data center does not have any available network bandwidth for additional workloads A solutions architect \nmust transfer the data and must configure the transformation job to continue to run in the AWS Cloud \n \nWhich solution will meet these requirements with the LEAST operational overhead? \n ","Option":["A. Use AWS DataSync to move the data Create a custom transformation job by using AWS Glue ","B. Order an AWS Snowcone device to move the data Deploy the transformation application to the device ","C. Order an AWS Snowball Edge Storage Optimized device. Copy the data to the device. Create a custom \ntransformation job by using AWS Glue ","D. Order an AWS D. Snowball Edge Storage Optimized device that includes Amazon EC2 compute Copy \nthe data to the device Create a new EC2 instance on AWS to run the transformation application "],"Explanation":"Answer: C \n \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":10,"QuestionContent":" \nA company\u0027s dynamic website is hosted using on-premises servers in the United States. The company is \nlaunching its product in Europe, and it wants to optimize site loading times for new European users. The site\u0027s \nbackend must remain in the United States. The product is being launched in a few days, and an immediate \nsolution is needed. \n \nWhat should the solutions architect recommend? \n ","Option":["A. Launch an Amazon EC2 instance in us-east-1 and migrate the site to it. ","B. Move the website to Amazon S3. Use cross-Region replication between Regions. ","C. Use Amazon CloudFront with a custom origin pointing to the on-premises servers. ","D. Use an Amazon Route 53 geo-proximity routing policy pointing to on-premises servers. "],"Explanation":"Answer: C \n \nExplanation \nhttps://aws.amazon.com/pt/blogs/aws/amazon-cloudfront-support-for-custom-origins/ \n \n  7 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nYou can now create a CloudFront distribution using a custom origin. Each distribution will can point to an S3 \nor to a custom origin. This could be another storage service, or it could be something more interesting and \nmore dynamic, such as an EC2 instance or even an Elastic Load Balancer \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":11,"QuestionContent":" \nA company is preparing to launch a public-facing web application in the AWS Cloud. The architecture \nconsists of Amazon EC2 instances within a VPC behind an Elastic Load Balancer (ELB). A third-party service \nis used for the DNS. The company\u0027s solutions architect must recommend a solution to detect and protect \nagainst large-scale DDoS attacks. \n \nWhich solution meets these requirements? \n ","Option":["A. Enable Amazon GuardDuty on the account. ","B. Enable Amazon Inspector on the EC2 instances. ","C. Enable AWS Shield and assign Amazon Route 53 to it. ","D. Enable AWS Shield Advanced and assign the ELB to it. "],"Explanation":"Answer: D \n \nExplanation \nhttps://aws.amazon.com/shield/faqs/ \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":12,"QuestionContent":" \nA company hosts more than 300 global websites and applications. The company requires a platform to analyze \nmore than 30 TB of clickstream data each day. \n \nWhat should a solutions architect do to transmit and process the clickstream data? \n ","Option":["A. Design an AWS Data Pipeline to archive the data to an Amazon S3 bucket and run an Amazon EMR \nduster with the data to generate analytics ","B. Create an Auto Scaling group of Amazon EC2 instances to process the data and send it to an Amazon S3 \ndata lake for Amazon Redshift to use tor analysis ","C. Cache the data to Amazon CloudFron: Store the data in an Amazon S3 bucket When an object is added \nto the S3 bucket, run an AWS Lambda function to process the data tor analysis. ","D. Collect the data from Amazon Kinesis Data Streams. Use Amazon Kinesis Data Firehose to transmit the \ndata to an Amazon S3 data lake Load the data in Amazon Redshift for analysis "],"Explanation":"Answer: D \n \nExplanation \n \n  8 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \n \nhttps://aws.amazon.com/es/blogs/big-data/real-time-analytics-with-amazon-redshift-streaming-ingestion/ \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":13,"QuestionContent":" \nA company recently migrated a message processing system to AWS. The system receives messages into an \nActiveMQ queue running on an Amazon EC2 instance. Messages are processed by a consumer application \nrunning on Amazon EC2. The consumer application processes the messages and writes results to a MySQL \ndatabase funning on Amazon EC2. The company wants this application to be highly available with tow \noperational complexity \n \nWhich architecture otters the HGHEST availability? \n ","Option":["A. Add a second ActiveMQ server to another Availably Zone Add an additional consumer EC2 instance in \nanother Availability Zone. Replicate the MySQL database to another Availability Zone. ","B. Use Amazon MO with active/standby brokers configured across two Availability Zones Add an \nadditional consumer EC2 instance in another Availability Zone. Replicate the MySQL database to \nanother Availability Zone. ","C. Use Amazon MO with active/standby blotters configured across two Availability Zones. Add an \nadditional consumer EC2 instance in another Availability Zone. Use Amazon ROS tor MySQL with \nMulti-AZ enabled. ","D. Use Amazon MQ with active/standby brokers configured across two Availability Zones Add an Auto \nScaling group for the consumer EC2 instances across two Availability Zones. Use Amazon RDS for \nMySQL with Multi-AZ enabled. "],"Explanation":"Answer: D \n \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":14,"QuestionContent":" \nA company is storing backup files by using Amazon S3 Standard storage. The files are accessed frequently for \n1 month. However, the files are not accessed after 1 month. The company must keep the files indefinitely. \n \nWhich storage solution will meet these requirements MOST cost-effectively? \n ","Option":["A. Configure S3 Intelligent-Tiering to automatically migrate objects. ","B. Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Glacier Deep Archive \nafter 1 month. ","C. Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Standard-Infrequent \nAccess (S3 Standard-IA) after 1 month. ","D. Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 One Zone-Infrequent \nAccess (S3 One Zone-IA) after 1 month. \n  9 of 230 Practice Test Amazon Web Services - SAA-C03 "],"Explanation":"Answer: B \n \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":15,"QuestionContent":" \nA company is designing an application. The application uses an AWS Lambda function to receive information \nthrough Amazon API Gateway and to store the information in an Amazon Aurora PostgreSQL database. \n \nDuring the proof-of-concept stage, the company has to increase the Lambda quotas significantly to handle the \nhigh volumes of data that the company needs to load into the database. A solutions architect must recommend \na new design to improve scalability and minimize the configuration effort. \n \nWhich solution will meet these requirements? \n ","Option":["A. Refactor the Lambda function code to Apache Tomcat code that runs on Amazon EC2 instances. \nConnect the database by using native Java Database Connectivity (JDBC) drivers. ","B. Change the platform from Aurora to Amazon DynamoDB. Provision a DynamoDB Accelerator (DAX) \ncluster. Use the DAX client SDK to point the existing DynamoDB API calls at the DAX cluster. ","C. Set up two Lambda functions. Configure one function to receive the information. Configure the other \nfunction to load the information into the database. Integrate the Lambda functions by using Amazon \nSimple Notification Service (Amazon SNS). ","D. Set up two Lambda functions. Configure one function to receive the information. Configure the other \nfunction to load the information into the database. Integrate the Lambda functions by using an Amazon \nSimple Queue Service (Amazon SQS) queue. "],"Explanation":"Answer: B \n \nExplanation \nbottlenecks can be avoided with queues (SQS). \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":16,"QuestionContent":" \nA solutions architect must design a highly available infrastructure for a website. The website is powered by \nWindows web servers that run on Amazon EC2 instances. The solutions architect must implement a solution \nthat can mitigate a large-scale DDoS attack that originates from thousands of IP addresses. Downtime is not \nacceptable for the website. \n \nWhich actions should the solutions architect take to protect the website from such an attack? (Select TWO.) \n ","Option":["A. Use AWS Shield Advanced to stop the DDoS attack. ","B. Configure Amazon GuardDuty to automatically block the attackers. ","C. Configure the website to use Amazon CloudFront for both static and dynamic content. \n 10 of 230  Practice Test Amazon Web Services - SAA-C03 ","D. Use an AWS Lambda function to automatically add attacker IP addresses to VPC network ACLs. ","E. Use EC2 Spot Instances in an Auto Scaling group with a target tracking scaling policy that is set to 80% \nCPU utilization "],"Explanation":"Answer: A C \n \nExplanation \n(https://aws.amazon.com/cloudfront \n \n","RightAnswer":["A","C"],"QuestionChoose":[]},{"QuestionNumber":17,"QuestionContent":" \nA company is building an application in the AWS Cloud. The application will store data in Amazon S3 \nbuckets in two AWS Regions. The company must use an AWS Key Management Service (AWS KMS) \ncustomer managed key to encrypt all data that is stored in the S3 buckets. The data in both S3 buckets must be \nencrypted and decrypted with the same KMS key. The data and the key must be stored in each of the two \nRegions. \n \nWhich solution will meet these requirements with the LEAST operational overhead? \n ","Option":["A. Create an S3 bucket in each Region Configure the S3 buckets to use server-side encryption with \nAmazon S3 managed encryption keys (SSE-S3) Configure replication between the S3 buckets. ","B. Create a customer managed multi-Region KMS key. Create an S3 bucket in each Region. Configure \nreplication between the S3 buckets. Configure the application to use the KMS key with client-side \nencryption. ","C. Create a customer managed KMS key and an S3 bucket in each Region Configure the S3 buckets to use \nserver-side encryption with Amazon S3 managed encryption keys (SSE-S3) Configure replication \nbetween the S3 buckets. ","D. Create a customer managed KMS key and an S3 bucket m each Region Configure the S3 buckets to use \nserver-side encryption with AWS KMS keys (SSE-KMS) Configure replication between the S3 buckets. "],"Explanation":"Answer: B \n \nExplanation \nFrom https://docs.aws.amazon.com/kms/latest/developerguide/custom-key-store-overview.html \n \nFor most users, the default AWS KMS key store, which is protected by FIPS 140-2 validated cryptographic \nmodules, fulfills their security requirements. There is no need to add an extra layer of maintenance \nresponsibility or a dependency on an additional service. However, you might consider creating a custom key \nstore if your organization has any of the following requirements: Key material cannot be stored in a shared \nenvironment. Key material must be subject to a secondary, independent audit path. The HSMs that generate \nand store key material must be certified at FIPS 140-2 Level 3. \nhttps://docs.aws.amazon.com/kms/latest/developerguide/custom-key-store-overview.html \n \nhttps://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html \n \n 11 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \n \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":18,"QuestionContent":" \nA bicycle sharing company is developing a multi-tier architecture to track the location of its bicycles during \npeak operating hours The company wants to use these data points in its existing analytics platform A solutions \narchitect must determine the most viable multi-tier option to support this architecture The data points must be \naccessible from the REST API. \n \nWhich action meets these requirements for storing and retrieving location data? \n ","Option":["A. Use Amazon Athena with Amazon S3 ","B. Use Amazon API Gateway with AWS Lambda ","C. Use Amazon QuickSight with Amazon Redshift. ","D. Use Amazon API Gateway with Amazon Kinesis Data Analytics "],"Explanation":"Answer: D \nExplanation \nhttps://aws.amazon.com/solutions/implementations/aws-streaming-data-solution-for-amazon-kinesis/ \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":19,"QuestionContent":" \nA company has an application that provides marketing services to stores. The services are based on previous \npurchases by store customers. The stores upload transaction data to the company through SFTP, and the data is \nprocessed and analyzed to generate new marketing offers. Some of the files can exceed 200 GB in size. \n \nRecently, the company discovered that some of the stores have uploaded files that contain personally \nidentifiable information (PII) that should not have been included. The company wants administrators to be \nalerted if PII is shared again. The company also wants to automate remediation. \n \nWhat should a solutions architect do to meet these requirements with the LEAST development effort? \n ","Option":["A. Use an Amazon S3 bucket as a secure transfer point. Use Amazon Inspector to scan me objects in the \nbucket. If objects contain Pll. trigger an S3 Lifecycle policy to remove the objects that contain Pll. ","B. Use an Amazon S3 bucket as a secure transfer point. Use Amazon Macie to scan the objects in the \nbucket. If objects contain Pll. Use Amazon Simple Notification Service (Amazon SNS) to trigger a \nnotification to the administrators to remove the objects mat contain Pll. ","C. Implement custom scanning algorithms in an AWS Lambda function. Trigger the function when objects \nare loaded into the bucket. It objects contain Rll. use Amazon Simple Notification Service (Amazon \nSNS) to trigger a notification to the administrators to remove the objects that contain Pll. ","D. Implement custom scanning algorithms in an AWS Lambda function. Trigger the function when objects \nare loaded into the bucket. If objects contain Pll. use Amazon Simple Email Service (Amazon STS) to \ntrigger a notification to the administrators and trigger on S3 Lifecycle policy to remove the objects mot \n 12 of 230  Practice Test Amazon Web Services - SAA-C03 \ncontain PII. "],"Explanation":"Answer: B \n \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":20,"QuestionContent":" \nA company needs guaranteed Amazon EC2 capacity in three specific Availability Zones in a specific AWS \nRegion for an upcoming event that will last 1 week. \n \nWhat should the company do to guarantee the EC2 capacity? \n ","Option":["A. Purchase Reserved instances that specify the Region needed ","B. Create an On Demand Capacity Reservation that specifies the Region needed ","C. Purchase Reserved instances that specify the Region and three Availability Zones needed ","D. Create an On-Demand Capacity Reservation that specifies the Region and three Availability Zones \nneeded "],"Explanation":"Answer: D \n \nExplanation \nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-capacity-reservations.html \n \nReserve instances: You will have to pay for the whole term (1 year or 3years) which is not cost effective \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":21,"QuestionContent":" \nA company has created an image analysis application in which users can upload photos and add photo frames \nto their images. The users upload images and metadata to indicate which photo frames they want to add to \ntheir images. The application uses a single Amazon EC2 instance and Amazon DynamoDB to store the \nmetadata. \n \nThe application is becoming more popular, and the number of users is increasing. The company expects the \nnumber of concurrent users to vary significantly depending on the time of day and day of week. The company \nmust ensure that the application can scale to meet the needs of the growing user base. \n \nWhich solution meats these requirements? \n ","Option":["A. Use AWS Lambda to process the photos. Store the photos and metadata in DynamoDB. ","B. Use Amazon Kinesis Data Firehose to process the photos and to store the photos and metadata. ","C. Use AWS Lambda to process the photos. Store the photos in Amazon S3. Retain DynamoDB to store \nthe metadata. \n 13 of 230  Practice Test Amazon Web Services - SAA-C03 ","D. Increase the number of EC2 instances to three. Use Provisioned IOPS SSD (io2) Amazon Elastic Block \nStore (Amazon EBS) volumes to store the photos and metadata. "],"Explanation":"Answer: C \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":22,"QuestionContent":" \nA company hosts an application on multiple Amazon EC2 instances The application processes messages from \nan Amazon SQS queue writes to an Amazon RDS table and deletes the message from the queue Occasional \nduplicate records are found in the RDS table. The SQS queue does not contain any duplicate messages. \n \nWhat should a solutions architect do to ensure messages are being processed once only? \n ","Option":["A. Use the CreateQueue API call to create a new queue ","B. Use the Add Permission API call to add appropriate permissions ","C. Use the ReceiveMessage API call to set an appropriate wail time ","D. Use the ChangeMessageVisibility APi call to increase the visibility timeout "],"Explanation":"Answer: D \n \nExplanation \nThe visibility timeout begins when Amazon SQS returns a message. During this time, the consumer processes \nand deletes the message. However, if the consumer fails before deleting the message and your system doesn\u0027t \ncall the DeleteMessage action for that message before the visibility timeout expires, the message becomes \nvisible to other consumers and the message is received again. If a message must be received only once, your \nconsumer should delete it within the duration of the visibility timeout. \nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html \n \nKeyword: SQS queue writes to an Amazon RDS From this, Option D best suite \u0026amp; other Options ruled out \n[Option A - You can\u0027t intruduce one more Queue in the existing one; Option B - only Permission \u0026amp; \nOption C - Only Retrieves Messages] FIF O queues are designed to never introduce duplicate messages. \nHowever, your message producer might introduce duplicates in certain scenarios: for example, if the producer \nsends a message, does not receive a response, and then resends the same message. Amazon SQS APIs provide \ndeduplication functionality that prevents your message producer from sending duplicates. Any duplicates \nintroduced by the message producer are removed within a 5-minute deduplication interval. For standard \nqueues, you might occasionally receive a duplicate copy of a message (at-least- once delivery). If you use a \nstandard queue, you must design your applications to be idempotent (that is, they must not be affected \nadversely when processing the same message more than once). \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":23,"QuestionContent":" \nAn application development team is designing a microservice that will convert large images to smaller, \ncompressed images. When a user uploads an image through the web interface, the microservice should store \nthe image in an Amazon S3 bucket, process and compress the image with an AWS Lambda function, and store \nthe image in its compressed form in a different S3 bucket. \n \n 14 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \n \nA solutions architect needs to design a solution that uses durable, stateless components to process the images \nautomatically. \n \nWhich combination of actions will meet these requirements? (Choose two.) \n ","Option":["A. Create an Amazon Simple Queue Service (Amazon SQS) queue Configure the S3 bucket to send a \nnotification to the SQS queue when an image is uploaded to the S3 bucket ","B. Configure the Lambda function to use the Amazon Simple Queue Service (Amazon SQS) queue as the \ninvocation source When the SQS message is successfully processed, delete the message in the queue ","C. Configure the Lambda function to monitor the S3 bucket for new uploads When an uploaded image is \ndetected write the file name to a text file in memory and use the text file to keep track of the images that \nwere processed ","D. Launch an Amazon EC2 instance to monitor an Amazon Simple Queue Service (Amazon SQS) queue \nWhen items are added to the queue log the file name in a text file on the EC2 instance and invoke the \nLambda function ","E. Configure an Amazon EventBridge (Amazon CloudWatch Events) event to monitor the S3 bucket When \nan image is uploaded. send an alert to an Amazon Simple Notification Service (Amazon SNS) topic with \nthe application owner\u0027s email address for further processing "],"Explanation":"Answer: A B \n \nExplanation \nCreating an Amazon Simple Queue Service (SQS) queue and configuring the S3 bucket to send a \nnotification to the SQS queue when an image is uploaded to the S3 bucket will ensure that the Lambda \nfunction is triggered in a stateless and durable manner. \n \nConfiguring the Lambda function to use the SQS queue as the invocation source, and deleting the \nmessage in the queue after it is successfully processed will ensure that the Lambda function processes \nthe image in a stateless and durable manner. \n \nAmazon SQS is a fully managed message queuing service that enables you to decouple and scale \nmicroservices, distributed systems, and serverless applications. SQS eliminates the complexity and overhead \nassociated with managing and operating-message oriented middleware, and empowers developers to focus on \ndifferentiating work. When new images are uploaded to the S3 bucket, SQS will trigger the Lambda function \nto process the image and compress it. Once the image is processed, the SQS message is deleted, ensuring that \nthe Lambda function is stateless and durable. \n \n","RightAnswer":["A","B"],"QuestionChoose":[]},{"QuestionNumber":24,"QuestionContent":" \nA company has applications that run on Amazon EC2 instances in a VPC. One of the applications needs to call \nthe Amazon S3 API to store and read objects. According to the company\u0027s security regulations, no traffic from \nthe applications is allowed to travel across the internet. \n \nWhich solution will meet these requirements? \n \n 15 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n ","Option":["A. Configure an S3 interface endpoint. ","B. Configure an S3 gateway endpoint. ","C. Create an S3 bucket in a private subnet. ","D. Create an S3 bucket in the same Region as the EC2 instance. "],"Explanation":"Answer: B \n \nExplanation \nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/privatelink-interface-endpoints.html#types-of-vpc-end \nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":25,"QuestionContent":" \nA company\u0027s website uses an Amazon EC2 instance store for its catalog of items. The company wants to make \nsure that the catalog is highly available and that the catalog is stored in a durable location. \n \nWhat should a solutions architect do to meet these requirements? \n ","Option":["A. Move the catalog to Amazon ElastiCache for Redis. ","B. Deploy a larger EC2 instance with a larger instance store. ","C. Move the catalog from the instance store to Amazon S3 Glacier Deep Archive. ","D. Move the catalog to an Amazon Elastic File System (Amazon EFS) file system. "],"Explanation":"Answer: A \n \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":26,"QuestionContent":" \nA company is hosting a static website on Amazon S3 and is using Amazon Route 53 for DNS. The website is \nexperiencing increased demand from around the world. The company must decrease latency for users who \naccess the website. \n \nWhich solution meets these requirements MOST cost-effectively? \n ","Option":["A. Replicate the S3 bucket that contains the website to all AWS Regions. Add Route 53 geolocation \nrouting entries. ","B. Provision accelerators in AWS Global Accelerator. Associate the supplied IP addresses with the S3 \nbucket. Edit the Route 53 entries to point to the IP addresses of the accelerators. ","C. Add an Amazon CloudFront distribution in front of the S3 bucket. Edit the Route 53 entries to point to \n 16 of 230  Practice Test Amazon Web Services - SAA-C03 \nthe CloudFront distribution. ","D. Enable S3 Transfer Acceleration on the bucket. Edit the Route 53 entries to point to the new endpoint. "],"Explanation":"Answer: C \n \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":27,"QuestionContent":" \nA company is preparing to deploy a new serverless workload. A solutions architect must use the principle of \nleast privilege to configure permissions that will be used to run an AWS Lambda function. An Amazon \nEventBridge (Amazon CloudWatch Events) rule will invoke the function. \n \nWhich solution meets these requirements? \n ","Option":["A. Add an execution role to the function with lambda:InvokeFunction as the action and * as the principal. ","B. Add an execution role to the function with lambda:InvokeFunction as the action and \nService:amazonaws.com as the principal. ","C. Add a resource-based policy to the function with lambda:\u0027* as the action and \nService:events.amazonaws.com as the principal. ","D. Add a resource-based policy to the function with lambda:InvokeFunction as the action and \nService:events.amazonaws.com as the principal. "],"Explanation":"Answer: D \n \nExplanation \nhttps://docs.aws.amazon.com/eventbridge/latest/userguide/resource-based-policies-eventbridge.html#lambda-pe \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":28,"QuestionContent":" \nA company has a production workload that runs on 1,000 Amazon EC2 Linux instances. The workload is \npowered by third-party software. The company needs to patch the third-party software on all EC2 instances as \nquickly as possible to remediate a critical security vulnerability. \n \nWhat should a solutions architect do to meet these requirements? \n ","Option":["A. Create an AWS Lambda function to apply the patch to all EC2 instances. ","B. Configure AWS Systems Manager Patch Manager to apply the patch to all EC2 instances. ","C. Schedule an AWS Systems Manager maintenance window to apply the patch to all EC2 instances. ","D. Use AWS Systems Manager Run Command to run a custom command that applies the patch to all EC2 \ninstances. \n 17 of 230  Practice Test Amazon Web Services - SAA-C03 "],"Explanation":"Answer: B \n \nExplanation \nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/about-windows-app-patching.html \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":29,"QuestionContent":" \nA hospital recently deployed a RESTful API with Amazon API Gateway and AWS Lambda The hospital uses \nAPI Gateway and Lambda to upload reports that are in PDF format and JPEG format The hospital needs to \nmodify the Lambda code to identify protected health information (PHI) in the reports \n \nWhich solution will meet these requirements with the LEAST operational overhead? \n ","Option":["A. Use existing Python libraries to extract the text from the reports and to identify the PHI from the \nextracted text. ","B. Use Amazon Textract to extract the text from the reports Use Amazon SageMaker to identify the PHI \nfrom the extracted text. ","C. Use Amazon Textract to extract the text from the reports Use Amazon Comprehend Medical to identify \nthe PHI from the extracted text ","D. Use Amazon Rekognition to extract the text from the reports Use Amazon Comprehend Medical to \nidentify the PHI from the extracted text "],"Explanation":"Answer: C \n \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":30,"QuestionContent":" \nA company is running a popular social media website. The website gives users the ability to upload images to \nshare with other users. The company wants to make sure that the images do not contain inappropriate content. \nThe company needs a solution that minimizes development effort. \n \nWhat should a solutions architect do to meet these requirements? \n ","Option":["A. Use Amazon Comprehend to detect inappropriate content. Use human review for low-confidence \npredictions. ","B. Use Amazon Rekognition to detect inappropriate content. Use human review for low-confidence \npredictions. ","C. Use Amazon SageMaker to detect inappropriate content. Use ground truth to label low-confidence \npredictions. ","D. Use AWS Fargate to deploy a custom machine learning model to detect inappropriate content. Use \nground truth to label low-confidence predictions. \n 18 of 230  Practice Test Amazon Web Services - SAA-C03 "],"Explanation":"Answer: B \n \nExplanation \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html?pg=ln\u0026sec=ft \nhttps://docs.aws.amazon.com/rekognition/latest/dg/a2i-rekognition.html \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":31,"QuestionContent":" \nA company is deploying a new public web application to AWS. The application will run behind an \nApplication Load Balancer (ALB). The application needs to be encrypted at the edge with an SSL/TLS \ncertificate that is issued by an external certificate authority (CA). The certificate must be rotated each year \nbefore the certificate expires. \n \nWhat should a solutions architect do to meet these requirements? \n ","Option":["A. Use AWS Certificate Manager (ACM) to issue an SSL/TLS certificate. Apply the certificate to the ALB. \nUse the managed renewal feature to automatically rotate the certificate. ","B. Use AWS Certificate Manager (ACM) to issue an SSL/TLS certificate. Import the key material from the \ncertificate. Apply the certificate to the ALB. Use the managed renewal feature to automatically rotate \nthe certificate. ","C. Use AWS Certificate Manager (ACM) Private Certificate Authority to issue an SSL/TLS certificate \nfrom the root CA. Apply the certificate to the ALB. Use the managed renewal feature to automatically \nrotate the certificate. ","D. Use AWS Certificate Manager (ACM) to import an SSL/TLS certificate. Apply the certificate to the \nALB. Use Amazon EventBridge (Amazon CloudWatch Events) to send a notification when the \ncertificate is nearing expiration. Rotate the certificate manually. "],"Explanation":"Answer: D \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":32,"QuestionContent":" \nA company has an application that ingests incoming messages. These messages are then quickly consumed by \ndozens of other applications and microservices. \n \nThe number of messages varies drastically and sometimes spikes as high as 100,000 each second. The \ncompany wants to decouple the solution and increase scalability. \n \nWhich solution meets these requirements? \n ","Option":["A. Persist the messages to Amazon Kinesis Data Analytics. All the applications will read and process the \nmessages. ","B. Deploy the application on Amazon EC2 instances in an Auto Scaling group, which scales the number of \nEC2 instances based on CPU metrics. \n 19 of 230  Practice Test Amazon Web Services - SAA-C03 ","C. Write the messages to Amazon Kinesis Data Streams with a single shard. All applications will read from \nthe stream and process the messages. ","D. Publish the messages to an Amazon Simple Notification Service (Amazon SNS) topic with one or more \nAmazon Simple Queue Service (Amazon SQS) subscriptions. All applications then process the \nmessages from the queues. "],"Explanation":"Answer: D \n \nExplanation \nhttps://aws.amazon.com/sqs/features/ \n \nBy routing incoming requests to Amazon SQS, the company can decouple the job requests from the processing \ninstances. This allows them to scale the number of instances based on the size of the queue, providing more \nresources when needed. Additionally, using an Auto Scaling group based on the queue size will automatically \nscale the number of instances up or down depending on the workload. Updating the software to read from the \nqueue will allow it to process the job requests in a more efficient manner, improving the performance of the \nsystem. \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":33,"QuestionContent":" \nA company runs an ecommerce application on Amazon EC2 instances behind an Application Load Balancer. \nThe instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. The Auto \nScaling group scales based on CPU utilization metrics. The ecommerce application stores the transaction data \nin a MySQL 8.0 database that is hosted on a large EC2 instance. \n \nThe database\u0027s performance degrades quickly as application load increases. The application handles more read \nrequests than write transactions. The company wants a solution that will automatically scale the database to \nmeet the demand of unpredictable read workloads while maintaining high availability. \n \nWhich solution will meet these requirements? \n ","Option":["A. Use Amazon Redshift with a single node for leader and compute functionality. ","B. Use Amazon RDS with a Single-AZ deployment Configure Amazon RDS to add reader instances in a \ndifferent Availability Zone. ","C. Use Amazon Aurora with a Multi-AZ deployment. Configure Aurora Auto Scaling with Aurora \nReplicas. ","D. Use Amazon ElastiCache for Memcached with EC2 Spot Instances. "],"Explanation":"Answer: C \n \nExplanation \nAURORA is 5x performance improvement over MySQL on RDS and handles more read requests than write,; \nmaintaining high availability = Multi-AZ deployment \n \n 20 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":34,"QuestionContent":" \nA company is developing an application that provides order shipping statistics for retrieval by a REST API. \nThe company wants to extract the shipping statistics, organize the data into an easy-to-read HTML format, and \nsend the report to several email addresses at the same time every morning. \n \nWhich combination of steps should a solutions architect take to meet these requirements? (Choose two.) \n ","Option":["A. Configure the application to send the data to Amazon Kinesis Data Firehose. ","B. Use Amazon Simple Email Service (Amazon SES) to format the data and to send the report by email. ","C. Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS \nGlue job to query the application\u0027s API for the data. ","D. Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS \nLambda function to query the application\u0027s API for the data. ","E. Store the application data in Amazon S3. Create an Amazon Simple Notification Service (Amazon SNS) \ntopic as an S3 event destination to send the report by "],"Explanation":"Answer: D E \n \n \n","RightAnswer":["D","E"],"QuestionChoose":[]},{"QuestionNumber":35,"QuestionContent":" \nAn application allows users at a company\u0027s headquarters to access product data. The product data is stored in \nan Amazon RDS MySQL DB instance. The operations team has isolated an application performance \nslowdown and wants to separate read traffic from write traffic. A solutions architect needs to optimize the \napplication\u0027s performance quickly. \n \nWhat should the solutions architect recommend? \n ","Option":["A. Change the existing database to a Multi-AZ deployment. Serve the read requests from the primary \nAvailability Zone. ","B. Change the existing database to a Multi-AZ deployment. Serve the read requests from the secondary \nAvailability Zone. ","C. Create read replicas for the database. Configure the read replicas with half of the compute and storage \nresources as the source database. ","D. Create read replicas for the database. Configure the read replicas with the same compute and storage \nresources as the source database. "],"Explanation":"Answer: D \n \nExplanation \n \n 21 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_MySQL.Replication.ReadReplicas.html \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":36,"QuestionContent":" \nA company runs an on-premises application that is powered by a MySQL database The company is migrating \nthe application to AWS to Increase the application\u0027s elasticity and availability \n \nThe current architecture shows heavy read activity on the database during times of normal operation Every 4 \nhours the company\u0027s development team pulls a full export of the production database to populate a database in \nthe staging environment During this period, users experience unacceptable application latency The \ndevelopment team is unable to use the staging environment until the procedure completes \n \nA solutions architect must recommend replacement architecture that alleviates the application latency issue \nThe replacement architecture also must give the development team the ability to continue using the staging \nenvironment without delay \n \nWhich solution meets these requirements? \n ","Option":["A. Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production. Populate the staging \ndatabase by implementing a backup and restore process that uses the mysqldump utility. ","B. Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production Use database cloning to \ncreate the staging database on-demand ","C. Use Amazon RDS for MySQL with a Mufti AZ deployment and read replicas for production Use the \nstandby instance tor the staging database. ","D. Use Amazon RDS for MySQL with a Multi-AZ deployment and read replicas for production. Populate \nthe staging database by implementing a backup and restore process that uses the mysqldump utility. "],"Explanation":"Answer: B \n \nExplanation \nhttps://aws.amazon.com/blogs/aws/amazon-aurora-fast-database-cloning/ \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":37,"QuestionContent":" \nA company has an AWS Glue extract. transform, and load (ETL) job that runs every day at the same time. The \njob processes XML data that is in an Amazon S3 bucket. \n \nNew data is added to the S3 bucket every day. A solutions architect notices that AWS Glue is processing all \nthe data during each run. \n \nWhat should the solutions architect do to prevent AWS Glue from reprocessing old data? \n ","Option":["A. Edit the job to use job bookmarks. ","B. Edit the job to delete data after the data is processed \n 22 of 230  Practice Test Amazon Web Services - SAA-C03 ","C. Edit the job by setting the NumberOfWorkers field to 1. ","D. Use a FindMatches machine learning (ML) transform. "],"Explanation":"Answer: C \n \nExplanation \nThis is the purpose of bookmarks: \u0022AWS Glue tracks data that has already been processed during a previous \nrun of an ETL job by persisting state information from the job run. This persisted state information is called a \njob bookmark. Job bookmarks help AWS Glue maintain state information and prevent the reprocessing of old \ndata.\u0022 https://docs.aws.amazon.com/glue/latest/dg/monitor-continuations.html \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":38,"QuestionContent":" \nA solutions architect is developing a multiple-subnet VPC architecture. The solution will consist of six subnets \nin two Availability Zones. The subnets are defined as public, private and dedicated for databases. Only the \nAmazon EC2 instances running in the private subnets should be able to access a database. \n \nWhich solution meets these requirements? \n ","Option":["A. Create a now route table that excludes the route to the public subnets\u0027 CIDR blocks. Associate the route \ntable to the database subnets. ","B. Create a security group that denies ingress from the security group used by instances in the public \nsubnets. Attach the security group to an Amazon RDS DB instance. ","C. Create a security group that allows ingress from the security group used by instances in the private \nsubnets. Attach the security group to an Amazon RDS DB instance. ","D. Create a new peering connection between the public subnets and the private subnets. Create a different \npeering connection between the private subnets and the database subnets. "],"Explanation":"Answer: C \n \nExplanation \nSecurity groups are stateful. All inbound traffic is blocked by default. If you create an inbound rule allowing \ntraffic in, that traffic is automatically allowed back out again. You cannot block specific IP address using \nSecurity groups (instead use Network Access Control Lists). \n \n\u0022You can specify allow rules, but not deny rules.\u0022 \u0022When you first create a security group, it has no inbound \nrules. Therefore, no inbound traffic originating from another host to your instance is allowed until you add \ninbound rules to the security group.\u0022 Source: \nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html#VPCSecurityGroups \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":39,"QuestionContent":" \n 23 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \nA company has more than 5 TB of file data on Windows file servers that run on premises Users and \napplications interact with the data each day \n \nThe company is moving its Windows workloads to AWS. As the company continues this process, the \ncompany requires access to AWS and on-premises file storage with minimum latency The company needs a \nsolution that minimizes operational overhead and requires no significant changes to the existing file access \npatterns. The company uses an AWS Site-to-Site VPN connection for connectivity to AWS \n \nWhat should a solutions architect do to meet these requirements? \n ","Option":["A. Deploy and configure Amazon FSx for Windows File Server on AWS. Move the on-premises file data \nto FSx for Windows File Server. Reconfigure the workloads to use FSx for Windows File Server on \nAWS. ","B. Deploy and configure an Amazon S3 File Gateway on premises Move the on-premises file data to the \nS3 File Gateway Reconfigure the on-premises workloads and the cloud workloads to use the S3 File \nGateway ","C. Deploy and configure an Amazon S3 File Gateway on premises Move the on-premises file data to \nAmazon S3 Reconfigure the workloads to use either Amazon S3 directly or the S3 File Gateway, \ndepending on each workload\u0027s location ","D. Deploy and configure Amazon FSx for Windows File Server on AWS Deploy and configure an Amazon \nFSx File Gateway on premises Move the on-premises file data to the FSx File Gateway Configure the \ncloud workloads to use FSx for Windows File Server on AWS Configure the on-premises workloads to \nuse the FSx File Gateway "],"Explanation":"Answer: D \n \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":40,"QuestionContent":" \nA development team needs to host a website that will be accessed by other teams. The website contents consist \nof HTML, CSS, client-side JavaScript, and images Which method is the MOST cost-effective for hosting the \nwebsite? \n ","Option":["A. Containerize the website and host it in AWS Fargate. ","B. Create an Amazon S3 bucket and host the website there ","C. Deploy a web server on an Amazon EC2 instance to host the website. ","D. Configure an Application Loa d Balancer with an AWS Lambda target that uses the Express js \nframework. "],"Explanation":"Answer: B \n \nExplanation \nIn Static Websites, Web pages are returned by the server which are prebuilt. \n \n 24 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \nThey use simple languages such as HTML, CSS, or JavaScript. \n \nThere is no processing of content on the server (according to the user) in Static Websites. Web pages are \nreturned by the server with no change therefore, static Websites are fast. \n \nThere is no interaction with databases. \n \nAlso, they are less costly as the host does not need to support server-side processing with different languages. \n \n============ \n \nIn Dynamic Websites, Web pages are returned by the server which are processed during runtime means they \nare not prebuilt web pages but they are built during runtime according to the user\u2019s demand. \n \nThese use server-side scripting languages such as PHP, Node.js, ASP.NET and many more supported by the \nserver. \n \nSo, they are slower than static websites but updates and interaction with databases are possible. \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":41,"QuestionContent":" \nA company needs to store its accounting records in Amazon S3. The records must be immediately accessible \nfor 1 year and then must be archived for an additional 9 years. No one at the company, including \nadministrative users and root users, can be able to delete the records during the entire 10-year period. The \nrecords must be stored with maximum resiliency. \n \nWhich solution will meet these requirements? \n ","Option":["A. Store the records in S3 Glacier for the entire 10-year period. Use an access control policy to deny \ndeletion of the records for a period of 10 years. ","B. Store the records by using S3 Intelligent-Tiering. Use an IAM policy to deny deletion of the records. \nAfter 10 years, change the IAM policy to allow deletion. ","C. Use an S3 Lifecycle policy to transition the records from S3 Standard to S3 Glacier Deep Archive after \n1 year. Use S3 Object Lock in compliance mode for a period of 10 years. ","D. Use an S3 Lifecycle policy to transition the records from S3 Standard to S3 One Zone-Infrequent \nAccess (S3 One Zone-IA) after 1 year. Use S3 Object Lock in governance mode for a period of 10 \nyears. "],"Explanation":"Answer: C \n \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":42,"QuestionContent":" \nA company hosts a containerized web application on a fleet of on-premises servers that process incoming \nrequests. The number of requests is growing quickly. The on-premises servers cannot handle the increased \n \n 25 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \nnumber of requests. The company wants to move the application to AWS with minimum code changes and \nminimum development effort. \n \nWhich solution will meet these requirements with the LEAST operational overhead? \n ","Option":["A. Use AWS Fargate on Amazon Elastic Container Service (Amazon ECS) to run the containerized web \napplication with Service Auto Scaling. Use an Application Load Balancer to distribute the incoming \nrequests. ","B. Use two Amazon EC2 instances to host the containerized web application. Use an Application Load \nBalancer to distribute the incoming requests ","C. Use AWS Lambda with a new code that uses one of the supported languages. Create multiple Lambda \nfunctions to support the load. Use Amazon API Gateway as an entry point to the Lambda functions. ","D. Use a high performance computing (HPC) solution such as AWS ParallelClusterto establish an HPC \ncluster that can process the incoming requests at the appropriate scale. "],"Explanation":"Answer: A \n \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":43,"QuestionContent":"A company needs to keep user transaction data in an Amazon DynamoDB table. \nThe company must retain the data for 7 years. \n \nWhat is the MOST operationally efficient solution that meets these requirements? \n ","Option":["A. Use DynamoDB point-in-time recovery to back up the table continuously. ","B. Use AWS Backup to create backup schedules and retention policies for the table. ","C. Create an on-demand backup of the table by using the DynamoDB console. Store the backup in an \nAmazon S3 bucket. Set an S3 Lifecycle configuration for the S3 bucket. ","D. Create an Amazon EventBridge (Amazon CloudWatch Events) rule to invoke an AWS Lambda \nfunction. Configure the Lambda function to back up the table and to store the backup in an Amazon S3 \nbucket. Set an S3 Lifecycle configuration for the S3 bucket. "],"Explanation":"Answer: C \n \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":44,"QuestionContent":" \nA company recently signed a contract with an AWS Managed Service Provider (MSP) Partner for help with an \napplication migration initiative. A solutions architect needs to share an Amazon Machine Image (AMI) from \n \n 26 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \nan existing AWS account with the MSP Partner\u0027s AWS account. The AMI is backed by Amazon Elastic Block \nStore (Amazon EBS) and uses a customer managed customer master key (CMK) to encrypt EBS volume \nsnapshots. \n \nWhat is the MOST secure way for the solutions architect to share the AMI with the MSP Partner\u0027s AWS \naccount? \n ","Option":["A. Make the encrypted AMI and snapshots publicly available. Modify the CMK\u0027s key policy to allow the \nMSP Partner\u0027s AWS account to use the key ","B. Modify the launchPermission property of the AMI. Share the AMI with the MSP Partner\u0027s AWS account \nonly. Modify the CMK\u0027s key policy to allow the MSP Partner\u0027s AWS account to use the key. ","C. Modify the launchPermission property of the AMI Share the AMI with the MSP Partner\u0027s AWS account \nonly. Modify the CMK\u0027s key policy to trust a new CMK that is owned by the MSP Partner for \nencryption. ","D. Export the AMI from the source account to an Amazon S3 bucket in the MSP Partner\u0027s AWS account. \nEncrypt the S3 bucket with a CMK that is owned by the MSP Partner Copy and launch the AMI in the \nMSP Partner\u0027s AWS account. "],"Explanation":"Answer: B \n \nExplanation \nShare the existing KMS key with the MSP external account because it has already been used to encrypt the \nAMI snapshot. \nhttps://docs.aws.amazon.com/kms/latest/developerguide/key-policy-modifying-external-accounts.html \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":45,"QuestionContent":" \nA company runs an online marketplace web application on AWS. The application serves hundreds of \nthousands of users during peak hours. The company needs a scalable, near-real-time solution to share the \ndetails of millions of financial transactions with several other internal applications Transactions also need to be \nprocessed to remove sensitive data before being stored in a document database for low-latency retrieval. \n \nWhat should a solutions architect recommend to meet these requirements? \n ","Option":["A. Store the transactions data into Amazon DynamoDB Set up a rule in DynamoDB to remove sensitive \ndata from every transaction upon write Use DynamoDB Streams to share the transactions data with \nother applications ","B. Stream the transactions data into Amazon Kinesis Data Firehose to store data in Amazon DynamoDB \nand Amazon S3 Use AWS Lambda integration with Kinesis Data Firehose to remove sensitive data. \nOther applications can consume the data stored in Amazon S3 ","C. Stream the transactions data into Amazon Kinesis Data Streams Use AWS Lambda integration to \nremove sensitive data from every transaction and then store the transactions data in Amazon \nDynamoDB Other applications can consume the transactions data off the Kinesis data stream. \n 27 of 230  Practice Test Amazon Web Services - SAA-C03 ","D. Store the batched transactions data in Amazon S3 as files. Use AWS Lambda to process every file and \nremove sensitive data before updating the files in Amazon S3 The Lambda function then stores the data \nin Amazon DynamoDB Other applications can consume transaction files stored in Amazon S3. "],"Explanation":"Answer: C \nExplanation \nThe destination of your Kinesis Data Firehose delivery stream. Kinesis Data Firehose can send data records to \nvarious destinations, including Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Amazon \nOpenSearch Service, and any HTTP endpoint that is owned by you or any of your third-party service \nproviders. The following are the supported destinations: \n \n* Amazon OpenSearch Service \n \n* Amazon S3 \n \n* Datadog \n \n* Dynatrace \n \n* Honeycomb \n \n* HTTP Endpoint \n \n* Logic Monitor \n \n* MongoDB Cloud \n \n* New Relic \n \n* Splunk \n \n* Sumo Logic \nhttps://docs.aws.amazon.com/firehose/latest/dev/create-name.html \nhttps://aws.amazon.com/kinesis/data-streams/ \nAmazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. \nKDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as \nwebsite clickstreams, database event streams, financial transactions, social media feeds, IT logs, and \nlocation-tracking events. \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":46,"QuestionContent":" \nA company recently launched a variety of new workloads on Amazon EC2 instances in its AWS account. The \ncompany needs to create a strategy to access and administer the instances remotely and securely. The company \nneeds to implement a repeatable process that works with native AWS services and follows the AWS \nWell-Architected Framework. \n \n 28 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \nWhich solution will meet these requirements with the LEAST operational overhead? \n ","Option":["A. Use the EC2 serial console to directly access the terminal interface of each instance for administration. ","B. Attach the appropriate IAM role to each existing instance and new instance. Use AWS Systems \nManager Session Manager to establish a remote SSH session. ","C. Create an administrative SSH key pair. Load the public key into each EC2 instance. Deploy a bastion \nhost in a public subnet to provide a tunnel for administration of each instance. ","D. Establish an AWS Site-to-Site VPN connection. Instruct administrators to use their local on-premises \nmachines to connect directly to the instances by using SSH keys across the VPN tunnel. "],"Explanation":"Answer: B \n \nExplanation \nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/setup-launch-managed-instance.html \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":47,"QuestionContent":" \nA company is implementing a new business application. The application runs on two Amazon EC2 instances \nand uses an Amazon S3 bucket for document storage. A solutions architect needs to ensure that the EC2 \ninstances can access the S3 bucket. \n \nWhat should the solutions architect do to meet this requirement? \n ","Option":["A. Create an IAM role that grants access to the S3 bucket. Attach the role to the EC2 instances. ","B. Create an IAM policy that grants access to the S3 bucket. Attach the policy to the EC2 instances. ","C. Create an IAM group that grants access to the S3 bucket. Attach the group to the EC2 instances. ","D. Create an IAM user that grants access to the S3 bucket. Attach the user account to the EC2 instances. "],"Explanation":"Answer: A \nExplanation \nhttps://aws.amazon.com/premiumsupport/knowledge-center/ec2-instance-access-s3-bucket/ \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":48,"QuestionContent":" \nA development team runs monthly resource-intensive tests on its general purpose Amazon RDS for MySQL \nDB instance with Performance Insights enabled. The testing lasts for 48 hours once a month and is the only \nprocess that uses the database. The team wants to reduce the cost of running the tests without reducing the \ncompute and memory attributes of the DB instance. \n \n 29 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \nWhich solution meets these requirements MOST cost-effectively? \n ","Option":["A. Stop the DB instance when tests are completed. Restart the DB instance when required. ","B. Use an Auto Scaling policy with the DB instance to automatically scale when tests are completed. ","C. Create a snapshot when tests are completed. Terminate the DB instance and restore the snapshot when \nrequired. ","D. Modify the DB instance to a low-capacity instance when tests are completed. Modify the DB instance \nagain when required. "],"Explanation":"Answer: A \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":49,"QuestionContent":" \nA company is designing an application where users upload small files into Amazon S3. After a user uploads a \nfile, the file requires one-time simple processing to transform the data and save the data in JSON format for \nlater analysis. \n \nEach file must be processed as quickly as possible after it is uploaded. Demand will vary. On some days, users \nwill upload a high number of files. On other days, users will upload a few files or no files. \n \nWhich solution meets these requirements with the LEAST operational overhead? \n ","Option":["A. Configure Amazon EMR to read text files from Amazon S3. Run processing scripts to transform the \ndata. Store the resulting JSON file in an Amazon Aurora DB cluster. ","B. Configure Amazon S3 to send an event notification to an Amazon Simple Queue Service (Amazon \nSQS) queue. Use Amazon EC2 instances to read from the queue and process the data. Store the resulting \nJSON file in Amazon DynamoDB. ","C. Configure Amazon S3 to send an event notification to an Amazon Simple Queue Service (Amazon \nSQS) queue. Use an AWS Lambda function to read from the queue and process the data. Store the \nresulting JSON file in Amazon DynamoDB. Most Voted ","D. Configure Amazon EventBridge (Amazon CloudWatch Events) to send an event to Amazon Kinesis \nData Streams when a new file is uploaded. Use an AWS Lambda function to consume the event from \nthe stream and process the data. Store the resulting JSON file in Amazon Aurora DB cluster. "],"Explanation":"Answer: C \n \nExplanation \nAmazon S3 sends event notifications about S3 buckets (for example, object created, object removed, or object \nrestored) to an SNS topic in the same Region. \n \nThe SNS topic publishes the event to an SQS queue in the central Region. \n \nThe SQS queue is configured as the event source for your Lambda function and buffers the event messages for \n \n 30 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \nthe Lambda function. \n \nThe Lambda function polls the SQS queue for messages and processes the Amazon S3 event notifications \naccording to your application\u2019s requirements. \n \nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/subscribe-a-lambda-function-to-event-notific \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":50,"QuestionContent":" \nA company has an Amazon S3 bucket that contains critical data. The company must protect the data from \naccidental deletion. \n \nWhich combination of steps should a solutions architect take to meet these requirements? (Choose two.) \n ","Option":["A. Enable versioning on the S3 bucket. ","B. Enable MFA Delete on the S3 bucket. ","C. Create a bucket policy on the S3 bucket. ","D. Enable default encryption on the S3 bucket. ","E. Create a lifecycle policy for the objects in the S3 bucket. "],"Explanation":"Answer: A B \n \n \n","RightAnswer":["A","B"],"QuestionChoose":[]},{"QuestionNumber":51,"QuestionContent":" \nA company uses NFS to store large video files in on-premises network attached storage. Each video file ranges \nin size from 1MB to 500 GB. The total storage is 70 TB and is no longer growing. The company decides to \nmigrate the video files to Amazon S3. The company must migrate the video files as soon as possible while \nusing the least possible network bandwidth. \n \nWhich solution will meet these requirements? \n ","Option":["A. Create an S3 bucket Create an IAM role that has permissions to write to the S3 bucket. Use the AWS \nCLI to copy all files locally to the S3 bucket. ","B. Create an AWS Snowball Edge job. Receive a Snowball Edge device on premises. Use the Snowball \nEdge client to transfer data to the device. Return the device so that AWS can import the data into \nAmazon S3. ","C. Deploy an S3 File Gateway on premises. Create a public service endpoint to connect to the S3 File \nGateway Create an S3 bucket Create a new NFS file share on the S3 File Gateway Point the new file \nshare to the S3 bucket. Transfer the data from the existing NFS file share to the S3 File Gateway. ","D. Set up an AWS Direct Connect connection between the on-premises network and AWS. Deploy an S3 \nFile Gateway on premises. Create a public virtual interlace (VIF) to connect to the S3 File Gateway. \n 31 of 230  Practice Test Amazon Web Services - SAA-C03 \nCreate an S3 bucket. Create a new NFS file share on the S3 File Gateway. Point the new file share to the \nS3 bucket. Transfer the data from the existing NFS file share to the S3 File Gateway. "],"Explanation":"Answer: B \n \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":52,"QuestionContent":" \nA company runs multiple Windows workloads on AWS. The company\u0027s employees use Windows file shares \nthat are hosted on two Amazon EC2 instances. The file shares synchronize data between themselves and \nmaintain duplicate copies. The company wants a highly available and durable storage solution that preserves \nhow users currently access the files. \n \nWhat should a solutions architect do to meet these requirements? \n ","Option":["A. Migrate all the data to Amazon S3 Set up IAM authentication for users to access files ","B. Set up an Amazon S3 File Gateway. Mount the S3 File Gateway on the existing EC2 Instances. ","C. Extend the file share environment to Amazon FSx for Windows File Server with a Multi-AZ \nconfiguration. Migrate all the data to FSx for Windows File Server. ","D. Extend the file share environment to Amazon Elastic File System (Amazon EFS) with a Multi-AZ \nconfiguration. Migrate all the data to Amazon EFS. "],"Explanation":"Answer: A \n \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":53,"QuestionContent":" \nA company needs to configure a real-time data ingestion architecture for its application. The company needs \nan API, a process that transforms data as the data is streamed, and a storage solution for the data. \n \nWhich solution will meet these requirements with the LEAST operational overhead? \n ","Option":["A. Deploy an Amazon EC2 instance to host an API that sends data to an Amazon Kinesis data stream. \nCreate an Amazon Kinesis Data Firehose delivery stream that uses the Kinesis data stream as a data \nsource. Use AWS Lambda functions to transform the data. Use the Kinesis Data Firehose delivery \nstream to send the data to Amazon S3. ","B. Deploy an Amazon EC2 instance to host an API that sends data to AWS Glue. Stop source/destination \nchecking on the EC2 instance. Use AWS Glue to transform the data and to send the data to Amazon S3. ","C. Configure an Amazon API Gateway API to send data to an Amazon Kinesis data stream. Create an \nAmazon Kinesis Data Firehose delivery stream that uses the Kinesis data stream as a data source. Use \nAWS Lambda functions to transform the data. Use the Kinesis Data Firehose delivery stream to send the \ndata to Amazon S3. ","D. Configure an Amazon API Gateway API to send data to AWS Glue. Use AWS Lambda functions to \n 32 of 230  Practice Test Amazon Web Services - SAA-C03 \ntransform the data. Use AWS Glue to send the data to Amazon S3. "],"Explanation":"Answer: C \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":54,"QuestionContent":" \nA company has several web servers that need to frequently access a common Amazon RDS MySQL Multi-AZ \nDB instance The company wants a secure method for the web servers to connect to the database while meeting \na security requirement to rotate user credentials frequently. \n \nWhich solution meets these requirements? \n ","Option":["A. Store the database user credentials in AWS Secrets Manager Grant the necessary IAM permissions to \nallow the web servers to access AWS Secrets Manager ","B. Store the database user credentials in AWS Systems Manager OpsCenter Grant the necessary IAM \npermissions to allow the web servers to access OpsCenter ","C. Store the database user credentials in a secure Amazon S3 bucket Grant the necessary IAM permissions \nto allow the web servers to retrieve credentials and access the database. ","D. Store the database user credentials in files encrypted with AWS Key Management Service (AWS KMS) \non the web server file system. The web server should be able to decrypt the files and access the database "],"Explanation":"Answer: A \n \nExplanation \nAWS Secrets Manager helps you protect secrets needed to access your applications, services, and IT \nresources. The service enables you to easily rotate, manage, and retrieve database credentials, API keys, and \nother secrets throughout their lifecycle. \n \nhttps://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html \n \nSecrets Manager enables you to replace hardcoded credentials in your code, including passwords, with an API \ncall to Secrets Manager to retrieve the secret programmatically. This helps ensure the secret can\u0027t be \ncompromised by someone examining your code, because the secret no longer exists in the code. Also, you can \nconfigure Secrets Manager to automatically rotate the secret for you according to a specified schedule. This \nenables you to replace long-term secrets with short-term ones, significantly reducing the risk of compromise. \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":55,"QuestionContent":" \nA company uses Amazon S3 to store its confidential audit documents. The S3 bucket uses bucket policies to \nrestrict access to audit team IAM user credentials according to the principle of least privilege. Company \nmanagers are worried about accidental deletion of documents in the S3 bucket and want a more secure \nsolution. \n \nWhat should a solutions architect do to secure the audit documents? \n \n 33 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \n ","Option":["A. Enable the versioning and MFA Delete features on the S3 bucket. ","B. Enable multi-factor authentication (MFA) on the IAM user credentials for each audit team IAM user \naccount. ","C. Add an S3 Lifecycle policy to the audit team\u0027s IAM user accounts to deny the s3:DeleteObject action \nduring audit dates. ","D. Use AWS Key Management Service (AWS KMS) to encrypt the S3 bucket and restrict audit team IAM \nuser accounts from accessing the KMS key. "],"Explanation":"Answer: A \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":56,"QuestionContent":" \nA company has a production web application in which users upload documents through a web interlace or a \nmobile app. According to a new regulatory requirement, new documents cannot be modified or deleted after \nthey are stored. \n \nWhat should a solutions architect do to meet this requirement? \n ","Option":["A. Store the uploaded documents in an Amazon S3 bucket with S3 Versioning and S3 Object Lock enabled ","B. Store the uploaded documents in an Amazon S3 bucket. Configure an S3 Lifecycle policy to archive the \ndocuments periodically. ","C. Store the uploaded documents in an Amazon S3 bucket with S3 Versioning enabled Configure an ACL \nto restrict all access to read-only. ","D. Store the uploaded documents on an Amazon Elastic File System (Amazon EFS) volume. Access the \ndata by mounting the volume in read-only mode. "],"Explanation":"Answer: A \n \nExplanation \nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":57,"QuestionContent":" \nA company uses AWS Organizations to manage multiple AWS accounts for different departments. The \nmanagement account has an Amazon S3 bucket that contains project reports. The company wants to limit \naccess to this S3 bucket to only users of accounts within the organization in AWS Organizations. \n \nWhich solution meets these requirements with the LEAST amount of operational overhead? \n ","Option":["A. Add the aws:PrincipalOrgID global condition key with a reference to the organization ID to the S3 \nbucket policy. \n 34 of 230  Practice Test Amazon Web Services - SAA-C03 ","B. Create an organizational unit (OU) for each department. Add the aws:PrincipalOrgPaths global \ncondition key to the S3 bucket policy. ","C. Use AWS CloudTrail to monitor the CreateAccount, InviteAccountToOrganization, LeaveOrganization, \nand RemoveAccountFromOrganization events. Update the S3 bucket policy accordingly. ","D. Tag each user that needs access to the S3 bucket. Add the aws:PrincipalTag global condition key to the \nS3 bucket policy. "],"Explanation":"Answer: A \n \nExplanation \nhttps://aws.amazon.com/blogs/security/control-access-to-aws-resources-by-using-the-aws-organization-of-iam-p \n \nThe aws:PrincipalOrgID global key provides an alternative to listing all the account IDs for all AWS accounts \nin an organization. For example, the following Amazon S3 bucket policy allows members of any account in \nthe XXX organization to add an object into the examtopics bucket. \n \n{\u0022Version\u0022: \u00222020-09-10\u0022, \n \n\u0022Statement\u0022: { \n \n\u0022Sid\u0022: \u0022AllowPutObject\u0022, \n\u0022Effect\u0022: \u0022Allow\u0022, \n\u0022Principal\u0022: \u0022*\u0022, \n\u0022Action\u0022: \u0022s3:PutObject\u0022, \n\u0022Resource\u0022: \u0022arn:aws:s3:::examtopics/*\u0022, \n\u0022Condition\u0022: {\u0022StringEquals\u0022: \n{\u0022aws:PrincipalOrgID\u0022:[\u0022XXX\u0022]}}}} \nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":58,"QuestionContent":" \nA company is hosting a web application on AWS using a single Amazon EC2 instance that stores \nuser-uploaded documents in an Amazon EBS volume. For better scalability and availability, the company \nduplicated the architecture and created a second EC2 instance and EBS volume in another Availability Zone \nplacing both behind an Application Load Balancer After completing this change, users reported that, each time \nthey refreshed the website, they could see one subset of their documents or the other, but never all of the \ndocuments at the same time. \n \nWhat should a solutions architect propose to ensure users see all of their documents at once? \n \n 35 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n ","Option":["A. Copy the data so both EBS volumes contain all the documents. ","B. Configure the Application Load Balancer to direct a user to the server with the documents ","C. Copy the data from both EBS volumes to Amazon EFS Modify the application to save new documents \nto Amazon EFS ","D. Configure the Application Load Balancer to send the request to both servers Return each document from \nthe correct server. "],"Explanation":"Answer: A \n \nExplanation \nAmazon EFS provides file storage in the AWS Cloud. With Amazon EFS, you can create a file system, mount \nthe file system on an Amazon EC2 instance, and then read and write data to and from your file system. You \ncan mount an Amazon EFS file system in your VPC, through the Network File System versions 4.0 and \n \n4.1 (NFSv4) protocol. We recommend using a current generation Linux NFSv4.1 client, such as those found in \nthe latest Amazon Linux, Redhat, and Ubuntu \n \nAMIs, in conjunction with the Amazon EFS Mount Helper. For instructions, see Using the amazon-efs-utils \nTools. \n \nFor a list of Amazon EC2 Linux Amazon Machine Images (AMIs) that support this protocol, see NFS \nSupport. For some AMIs, you\u0027ll need to install an NFS client to mount your file system on your Amazon EC2 \ninstance. For instructions, see Installing the NFS Client. \n \nYou can access your Amazon EFS file system concurrently from multiple NFS clients, so applications that \nscale beyond a single connection can access a file system. Amazon EC2 instances running in multiple \nAvailability Zones within the same AWS Region can access the file system, so that many users can access and \nshare a common data source. \n \nhttps://docs.aws.amazon.com/efs/latest/ug/how-it-works.html#how-it-works-ec2 \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":59,"QuestionContent":" \nA company is migrating applications to AWS. The applications are deployed in different accounts. The \ncompany manages the accounts centrally by using AWS Organizations. The company\u0027s security team needs a \nsingle sign-on (SSO) solution across all the company\u0027s accounts. The company must continue managing the \nusers and groups in its on-premises self-managed Microsoft Active Directory. \n \nWhich solution will meet these requirements? \n ","Option":["A. Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console. Create a one-way forest trust or \na one-way domain trust to connect the company\u0027s self-managed Microsoft Active Directory with AWS \nSSO by using AWS Directory Service for Microsoft Active Directory. ","B. Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console. Create a two-way forest trust to \nconnect the company\u0027s self-managed Microsoft Active Directory with AWS SSO by using AWS \n 36 of 230  Practice Test Amazon Web Services - SAA-C03 \nDirectory Service for Microsoft Active Directory. ","C. Use AWS Directory Service. Create a two-way trust relationship with the company\u0027s self-managed \nMicrosoft Active Directory. ","D. Deploy an identity provider (IdP) on premises. Enable AWS Single Sign-On (AWS SSO) from the \nAWS SSO console. "],"Explanation":"Answer: A \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":60,"QuestionContent":" \nA company has a website hosted on AWS. The website is behind an Application Load Balancer (ALB) that is \nconfigured to handle HTTP and HTTPS separately. The company wants to forward all requests to the website \nso that the requests will use HTTPS. \n \nWhat should a solutions architect do to meet this requirement? \n ","Option":["A. Update the ALB\u0027s network ACL to accept only HTTPS traffic ","B. Create a rule that replaces the HTTP in the URL with HTTPS. ","C. Create a listener rule on the ALB to redirect HTTP traffic to HTTPS. ","D. Replace the ALB with a Network Load Balancer configured to use Server Name Indication (SNI). "],"Explanation":"Answer: C \n \nExplanation \nhttps://aws.amazon.com/premiumsupport/knowledge-center/elb-redirect-http-to-https-using-alb/ \n \nHow can I redirect HTTP requests to HTTPS using an Application Load Balancer? Last updated: 2020-10-30 I \nwant to redirect HTTP requests to HTTPS using Application Load Balancer listener rules. How can I do this? \nResolution Reference: \nhttps://aws.amazon.com/premiumsupport/knowledge-center/elb-redirect-http-to-https-using-alb/ \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":61,"QuestionContent":" \nA company that hosts its web application on AWS wants to ensure all Amazon EC2 instances. Amazon RDS \nDB instances. and Amazon Redshift clusters are configured with tags. The company wants to minimize the \neffort of configuring and operating this check. \n \nWhat should a solutions architect do to accomplish this? \n ","Option":["A. Use AWS Config rules to define and detect resources that are not properly tagged. ","B. Use Cost Explorer to display resources that are not properly tagged. Tag those resources manually. \n 37 of 230  Practice Test Amazon Web Services - SAA-C03 ","C. Write API calls to check all resources for proper tag allocation. Periodically run the code on an EC2 \ninstance. ","D. Write API calls to check all resources for proper tag allocation. Schedule an AWS Lambda function \nthrough Amazon CloudWatch to periodically run the code. "],"Explanation":"Answer: A \n \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":62,"QuestionContent":" \nA company is using a SQL database to store movie data that is publicly accessible. The database runs on an \nAmazon RDS Single-AZ DB instance A script runs queries at random intervals each day to record the number \nof new movies that have been added to the database. The script must report a final total during business hours \nThe company\u0027s development team notices that the database performance is inadequate for development tasks \nwhen the script is running. A solutions architect must recommend a solution to resolve this issue. Which \nsolution will meet this requirement with the LEAST operational overhead? \n ","Option":["A. Modify the DB instance to be a Multi-AZ deployment ","B. Create a read replica of the database Configure the script to query only the read replica ","C. Instruct the development team to manually export the entries in the database at the end of each day ","D. Use Amazon ElastiCache to cache the common queries that the script runs against the database "],"Explanation":"Answer: B \n \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":63,"QuestionContent":" \nA social media company allows users to upload images to its website. The website runs on Amazon EC2 \ninstances. During upload requests, the website resizes the images to a standard size and stores the resized \nimages in Amazon S3. Users are experiencing slow upload requests to the website. \n \nThe company needs to reduce coupling within the application and improve website performance. A solutions \narchitect must design the most operationally efficient process for image uploads. \n \nWhich combination of actions should the solutions architect take to meet these requirements? (Choose two.) \n ","Option":["A. Configure the application to upload images to S3 Glacier. ","B. Configure the web server to upload the original images to Amazon S3. ","C. Configure the application to upload images directly from each user\u0027s browser to Amazon S3 through the \nuse of a presigned URL. ","D. Configure S3 Event Notifications to invoke an AWS Lambda function when an image is uploaded. Use \nthe function to resize the image \n 38 of 230  Practice Test Amazon Web Services - SAA-C03 ","E. Create an Amazon EventBridge (Amazon CloudWatch Events) rule that invokes an AWS Lambda \nfunction on a schedule to resize uploaded images. "],"Explanation":"Answer: B D \n \n \n","RightAnswer":["B","D"],"QuestionChoose":[]},{"QuestionNumber":64,"QuestionContent":" \nA company is developing a two-tier web application on AWS. The company\u0027s developers have deployed the \napplication on an Amazon EC2 instance that connects directly to a backend Amazon RDS database. The \ncompany must not hardcode database credentials in the application. The company must also implement a \nsolution to automatically rotate the database credentials on a regular basis. \n \nWhich solution will meet these requirements with the LEAST operational overhead? \n ","Option":["A. Store the database credentials in the instance metadata. Use Amazon EventBridge (Amazon \nCloudWatch Events) rules to run a scheduled AWS Lambda function that updates the RDS credentials \nand instance metadata at the same time. ","B. Store the database credentials in a configuration file in an encrypted Amazon S3 bucket. Use Amazon \nEventBridge (Amazon CloudWatch Events) rules to run a scheduled AWS Lambda function that \nupdates the RDS credentials and the credentials in the configuration file at the same time. Use S3 \nVersioning to ensure the ability to fall back to previous values. ","C. Store the database credentials as a secret in AWS Secrets Manager. Turn on automatic rotation for the \nsecret. Attach the required permission to the EC2 role to grant access to the secret. ","D. Store the database credentials as encrypted parameters in AWS Systems Manager Parameter Store. Turn \non automatic rotation for the encrypted parameters. Attach the required permission to the EC2 role to \ngrant access to the encrypted parameters. "],"Explanation":"Answer: C \n \nExplanation \nhttps://docs.aws.amazon.com/secretsmanager/latest/userguide/create_database_secret.html \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":65,"QuestionContent":" \nA company is launching a new application and will display application metrics on an Amazon CloudWatch \ndashboard. The company\u2019s product manager needs to access this dashboard periodically. The product manager \ndoes not have an AWS account. A solution architect must provide access to the product manager by following \nthe principle of least privilege. \n \nWhich solution will meet these requirements? \n ","Option":["A. Share the dashboard from the CloudWatch console. Enter the product manager\u2019s email address, and \ncomplete the sharing steps. Provide a shareable link for the dashboard to the product manager. \n 39 of 230  Practice Test Amazon Web Services - SAA-C03 ","B. Create an IAM user specifically for the product manager. Attach the CloudWatch Read Only Access \nmanaged policy to the user. Share the new login credential with the product manager. Share the browser \nURL of the correct dashboard with the product manager. ","C. Create an IAM user for the company\u2019s employees, Attach the View Only Access AWS managed policy \nto the IAM user. Share the new login credentials with the product manager. Ask the product manager to \nnavigate to the CloudWatch console and locate the dashboard by name in the Dashboards section. ","D. Deploy a bastion server in a public subnet. When the product manager requires access to the dashboard, \nstart the server and share the RDP credentials. On the bastion server, ensure that the browser is \nconfigured to open the dashboard URL with cached AWS credentials that have appropriate permissions \nto view the dashboard. "],"Explanation":"Answer: B \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":66,"QuestionContent":" \nA company has an on-premises application that generates a large amount of time-sensitive data that is backed \nup to Amazon S3. The application has grown and there are user complaints about internet bandwidth \nlimitations. A solutions architect needs to design a long-term solution that allows for both timely backups to \nAmazon S3 and with minimal impact on internet connectivity for internal users. \n \nWhich solution meets these requirements? \n ","Option":["A. Establish AWS VPN connections and proxy all traffic through a VPC gateway endpoint ","B. Establish a new AWS Direct Connect connection and direct backup traffic through this new connection. ","C. Order daily AWS Snowball devices Load the data onto the Snowball devices and return the devices to \nAWS each day. ","D. Submit a support ticket through the AWS Management Console Request the removal of S3 service \nlimits from the account. "],"Explanation":"Answer: B \n \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":67,"QuestionContent":" \nA company has a large Microsoft SharePoint deployment running on-premises that requires Microsoft \nWindows shared file storage. The company wants to migrate this workload to the AWS Cloud and is \nconsidering various storage options. The storage solution must be highly available and integrated with Active \nDirectory for access control. \n \nWhich solution will satisfy these requirements? \n ","Option":["A. Configure Amazon EFS storage and set the Active Directory domain for authentication ","B. Create an SMB Me share on an AWS Storage Gateway tile gateway in two Availability Zones \n 40 of 230  Practice Test Amazon Web Services - SAA-C03 ","C. Create an Amazon S3 bucket and configure Microsoft Windows Server to mount it as a volume ","D. Create an Amazon FSx for Windows File Server file system on AWS and set the Active Directory \ndomain for authentication "],"Explanation":"Answer: D \n \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":68,"QuestionContent":" \nA company is migrating a distributed application to AWS The application serves variable workloads The \nlegacy platform consists of a primary server trial coordinates jobs across multiple compute nodes The \ncompany wants to modernize the application with a solution that maximizes resiliency and scalability. \n \nHow should a solutions architect design the architecture to meet these requirements? \n ","Option":["A. Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination for the jobs \nImplement the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. \nConfigure EC2 Auto Scaling to use scheduled scaling ","B. Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination for the jobs \nImplement the compute nodes with Amazon EC2 Instances that are managed in an Auto Scaling group \nConfigure EC2 Auto Scaling based on the size of the queue ","C. Implement the primary server and the compute nodes with Amazon EC2 instances that are managed In \nan Auto Scaling group. Configure AWS CloudTrail as a destination for the fobs Configure EC2 Auto \nScaling based on the load on the primary server ","D. implement the primary server and the compute nodes with Amazon EC2 instances that are managed in \nan Auto Scaling group Configure Amazon EventBridge (Amazon CloudWatch Events) as a destination \nfor the jobs Configure EC2 Auto Scaling based on the load on the compute nodes "],"Explanation":"Answer: B \n \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":69,"QuestionContent":" \nA company runs a shopping application that uses Amazon DynamoDB to store customer information. In case \nof data corruption, a solutions architect needs to design a solution that meets a recovery point objective (RPO) \nof 15 minutes and a recovery time objective (RTO) of 1 hour. \n \nWhat should the solutions architect recommend to meet these requirements? \n ","Option":["A. Configure DynamoDB global tables. For RPO recovery, point the application to a different AWS \nRegion. ","B. Configure DynamoDB point-in-time recovery. For RPO recovery, restore to the desired point in time. \n 41 of 230  Practice Test Amazon Web Services - SAA-C03 ","C. Export the DynamoDB data to Amazon S3 Glacier on a daily basis. For RPO recovery, import the data \nfrom S3 Glacier to DynamoDB. ","D. Schedule Amazon Elastic Block Store (Amazon EBS) snapshots for the DynamoDB table every 15 \nminutes. For RPO recovery, restore the DynamoDB table by using the EBS snapshot. "],"Explanation":"Answer: B \n \nExplanation \nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/PointInTimeRecovery.html \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":70,"QuestionContent":" \nA company is implementing a shared storage solution for a media application that is hosted m the AWS Cloud \nThe company needs the ability to use SMB clients to access data The solution must he fully managed. \n \nWhich AWS solution meets these requirements? \n ","Option":["A. Create an AWS Storage Gateway volume gateway. Create a file share that uses the required client \nprotocol Connect the application server to the file share. ","B. Create an AWS Storage Gateway tape gateway Configure (apes to use Amazon S3 Connect the \napplication server lo the tape gateway ","C. Create an Amazon EC2 Windows instance Install and configure a Windows file share role on the \ninstance. Connect the application server to the file share. ","D. Create an Amazon FSx for Windows File Server tile system Attach the fie system to the origin server. \nConnect the application server to the file system "],"Explanation":"Answer: D \n \nExplanation \nhttps://aws.amazon.com/fsx/lustre/ \n \nAmazon FSx has native support for Windows file system features and for the industry-standard Server \nMessage Block (SMB) protocol to access file storage over a network. \nhttps://docs.aws.amazon.com/fsx/latest/WindowsGuide/what-is.html \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":71,"QuestionContent":" \nA company collects temperature, humidity, and atmospheric pressure data in cities across multiple continents. \nThe average volume of data collected per site each day is 500 GB. Each site has a high-speed internet \nconnection. The company\u0027s weather forecasting applications are based in a single Region and analyze the data \ndaily. \n \nWhat is the FASTEST way to aggregate data from all of these global sites? \n \n 42 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \n ","Option":["A. Enable Amazon S3 Transfer Acceleration on the destination bucket. Use multipart uploads to directly \nupload site data to the destination bucket. ","B. Upload site data to an Amazon S3 bucket in the closest AWS Region. Use S3 cross-Region replication \nto copy objects to the destination bucket. ","C. Schedule AWS Snowball jobs daily to transfer data to the closest AWS Region. Use S3 cross-Region \nreplication to copy objects to the destination bucket. ","D. Upload the data to an Amazon EC2 instance in the closest Region. Store the data in an Amazon Elastic \nBlock Store (Amazon EBS) volume. Once a day take an EBS snapshot and copy it to the centralized \nRegion. Restore the EBS volume in the centralized Region and run an analysis on the data daily. "],"Explanation":"Answer: A \n \nExplanation \nYou might want to use Transfer Acceleration on a bucket for various reasons, including the following: \nYou have customers that upload to a centralized bucket from all over the world. \nYou transfer gigabytes to terabytes of data on a regular basis across continents. \nYou are unable to utilize all of your available bandwidth over the Internet when uploading to Amazon S3. \nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html \nhttps://aws.amazon.com/s3/transfer-acceleration/#:~:text=S3%20Transfer%20Acceleration%20(S3TA)%20redu \n\u0022Amazon S3 Transfer Acceleration can speed up content transfers to and from Amazon S3 by as much as \n50-500% for long-distance transfer of larger objects. Customers who have either web or mobile applications \nwith widespread users or applications hosted far away from their S3 bucket can experience long and variable \nupload and download speeds over the Internet\u0022 \n \nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html \n\u0022Improved throughput - You can upload parts in parallel to improve throughput.\u0022 \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":72,"QuestionContent":" \nA company has registered its domain name with Amazon Route 53. The company uses Amazon API Gateway \nin the ca-central-1 Region as a public interface for its backend microservice APIs. Third-party services \nconsume the APIs securely. The company wants to design its API Gateway URL with the company\u0027s domain \nname and corresponding certificate so that the third-party services can use HTTPS. \n \nWhich solution will meet these requirements? \n ","Option":["A. Create stage variables in API Gateway with Name=\u0022Endpoint-URL\u0022 and Value=\u0022Company Domain \nName\u0022 to overwrite the default URL. Import the public certificate associated with the company\u0027s domain \n 43 of 230  Practice Test Amazon Web Services - SAA-C03 \nname into AWS Certificate Manager (ACM). ","B. Create Route 53 DNS records with the company\u0027s domain name. Point the alias record to the Regional \nAPI Gateway stage endpoint. Import the public certificate associated with the company\u0027s domain name \ninto AWS Certificate Manager (ACM) in the us-east-1 Region. ","C. Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company\u0027s \ndomain name. Import the public certificate associated with the company\u0027s domain name into AWS \nCertificate Manager (ACM) in the same Region. Attach the certificate to the API Gateway endpoint. \nConfigure Route 53 to route traffic to the API Gateway endpoint. ","D. Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company\u0027s \ndomain name. Import the public certificate associated with the company\u0027s domain name into AWS \nCertificate Manager (ACM) in the us-east-1 Region. Attach the certificate to the API Gateway APIs. \nCreate Route 53 DNS records with the company\u0027s domain name. Point an A record to the company\u0027s \ndomain name. "],"Explanation":"Answer: D \n \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":73,"QuestionContent":" \nA company observes an increase in Amazon EC2 costs in its most recent bill The billing team notices \nunwanted vertical scaling of instance types for a couple of EC2 instances A solutions architect needs to create \na graph comparing the last 2 months of EC2 costs and perform an in-depth analysis to identify the root cause \nof the vertical scaling \n \nHow should the solutions architect generate the information with the LEAST operational overhead? \n ","Option":["A. Use AWS Budgets to create a budget report and compare EC2 costs based on instance types ","B. Use Cost Explorer\u0027s granular filtering feature to perform an in-depth analysis of EC2 costs based on \ninstance types ","C. Use graphs from the AWS Billing and Cost Management dashboard to compare EC2 costs based on \ninstance types for the last 2 months ","D. Use AWS Cost and Usage Reports to create a report and send it to an Amazon S3 bucket Use Amazon \nQuickSight with Amazon S3 as a source to generate an interactive graph based on instance types. "],"Explanation":"Answer: B \n \nExplanation \nAWS Cost Explorer is a tool that enables you to view and analyze your costs and usage. You can explore your \nusage and costs using the main graph, the Cost Explorer cost and usage reports, or the Cost Explorer RI \nreports. You can view data for up to the last 12 months, forecast how much you\u0027re likely to spend for the next \n12 months, and get recommendations for what Reserved Instances to purchase. You can use Cost Explorer to \nidentify areas that need further inquiry and see trends that you can use to understand your costs. \nhttps://docs.aws.amazon.com/cost-management/latest/userguide/ce-what-is.html \n \n 44 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \n \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":74,"QuestionContent":" \nA company has a three-tier web application that is deployed on AWS. The web servers are deployed in a \npublic subnet in a VPC. The application servers and database servers are deployed in private subnets in the \nsame VPC. The company has deployed a third-party virtual firewall appliance from AWS Marketplace in an \ninspection VPC. The appliance is configured with an IP interface that can accept IP packets. \n \nA solutions architect needs to Integrate the web application with the appliance to inspect all traffic to the \napplication before the traffic teaches the web server. Which solution will moot these requirements with the \nLEAST operational overhead? \n ","Option":["A. Create a Network Load Balancer the public subnet of the application\u0027s VPC to route the traffic lo the \nappliance for packet inspection ","B. Create an Application Load Balancer in the public subnet of the application\u0027s VPC to route the traffic to \nthe appliance for packet inspection ","C. Deploy a transit gateway m the inspection VPC Configure route tables to route the incoming pockets \nthrough the transit gateway ","D. Deploy a Gateway Load Balancer in the inspection VPC Create a Gateway Load Balancer endpoint to \nreceive the incoming packets and forward the packets to the appliance "],"Explanation":"Answer: D \n \nExplanation \nhttps://aws.amazon.com/blogs/networking-and-content-delivery/scaling-network-traffic-inspection-using-aws-ga \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":75,"QuestionContent":" \nA survey company has gathered data for several years from areas m\\ the United States. The company hosts the \ndata in an Amazon S3 bucket that is 3 TB m size and growing. The company has started to share the data with \na European marketing firm that has S3 buckets The company wants to ensure that its data transfer costs remain \nas low as possible \n \nWhich solution will meet these requirements? \n ","Option":["A. Configure the Requester Pays feature on the company\u0027s S3 bucket ","B. Configure S3 Cross-Region Replication from the company\u2019s S3 bucket to one of the marketing firm\u0027s S3 \nbuckets. ","C. Configure cross-account access for the marketing firm so that the marketing firm has access to the \ncompany\u2019s S3 bucket. ","D. Configure the company\u2019s S3 bucket to use S3 Intelligent-Tiering Sync the S3 bucket to one of the \nmarketing firm\u2019s S3 buckets \n 45 of 230  Practice Test Amazon Web Services - SAA-C03 "],"Explanation":"Answer: A \n \nExplanation \n\u0022Typically, you configure buckets to be Requester Pays buckets when you want to share data but not incur \ncharges associated with others accessing the data. For example, you might use Requester Pays buckets when \nmaking available large datasets, such as zip code directories, reference data, geospatial information, or web \ncrawling data.\u0022 https://docs.aws.amazon.com/AmazonS3/latest/userguide/RequesterPaysBuckets.html \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":76,"QuestionContent":" \nA solutions architect is using Amazon S3 to design the storage architecture of a new digital media application. \nThe media files must be resilient to the loss of an Availability Zone Some files are accessed frequently while \nother files are rarely accessed in an unpredictable pattern. The solutions architect must minimize the costs of \nstoring and retrieving the media files. \n \nWhich storage option meets these requirements? \n ","Option":["A. S3 Standard ","B. S3 Intelligent-Tiering ","C. S3 Standard-Infrequent Access {S3 Standard-IA) ","D. S3 One Zone-Infrequent Access (S3 One Zone-IA) "],"Explanation":"Answer: B \n \nExplanation \nS3 Intelligent-Tiering - Perfect use case when you don\u0027t know the frequency of access or irregular patterns of \nusage. \n \nAmazon S3 offers a range of storage classes designed for different use cases. These include S3 Standard for \ngeneral-purpose storage of frequently accessed data; S3 Intelligent-Tiering for data with unknown or changing \naccess patterns; S3 Standard-Infrequent Access (S3 Standard-IA) and S3 One Zone-Infrequent Access (S3 One \nZone-IA) for long-lived, but less frequently accessed data; and Amazon S3 Glacier (S3 Glacier) and Amazon \nS3 Glacier Deep Archive (S3 Glacier Deep Archive) for long-term archive and digital preservation. If you \nhave data residency requirements that can\u2019t be met by an existing AWS Region, you can use the S3 Outposts \nstorage class to store your S3 data on-premises. Amazon S3 also offers capabilities to manage your data \nthroughout its lifecycle. Once an S3 Lifecycle policy is set, your data will automatically transfer to a different \nstorage class without any changes to your application. \n \nhttps://aws.amazon.com/getting-started/hands-on/getting-started-using-amazon-s3-intelligent-tiering/?nc1=h_ls \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":77,"QuestionContent":"A company is preparing to store confidential data in Amazon S3 For compliance reasons the data must be \n \n 46 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \nencrypted at rest Encryption key usage must be logged tor auditing purposes. Keys must be rotated every year. \nWhich solution meets these requirements and \u00ABthe MOST operationally efferent? ","Option":["A. Server-side encryption with customer-provided keys (SSE-C) ","B. Server-side encryption with Amazon S3 managed keys (SSE-S3) ","C. Server-side encryption with AWS KMS (SSE-KMS) customer master keys (CMKs) with manual \nrotation ","D. Server-side encryption with AWS KMS (SSE-KMS) customer master keys (CMKs) with automate \nrotation "],"Explanation":"Answer: D \n \nExplanation \nhttps://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html \n \nWhen you enable automatic key rotation for a customer managed key, AWS KMS generates new \ncryptographic material for the KMS key every year. AWS KMS also saves the KMS key\u0027s older cryptographic \nmaterial in perpetuity so it can be used to decrypt data that the KMS key encrypted. \n \nKey rotation in AWS KMS is a cryptographic best practice that is designed to be transparent and easy to use. \nAWS KMS supports optional automatic key rotation only for customer managed CMKs. Enable and disable \nkey rotation. Automatic key rotation is disabled by default on customer managed CMKs. When you enable (or \nre-enable) key rotation, AWS KMS automatically rotates the CMK 365 days after the enable date and every \n365 days thereafter. \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":78,"QuestionContent":" \nA company receives 10 TB of instrumentation data each day from several machines located at a single factory. \nThe data consists of JSON files stored on a storage area network (SAN) in an on-premises data center located \nwithin the factory. The company wants to send this data to Amazon S3 where it can be accessed by several \nadditional systems that provide critical near-real-lime analytics. A secure transfer is important because the data \nis considered sensitive. \n \nWhich solution offers the MOST reliable data transfer? \n ","Option":["A. AWS DataSync over public internet ","B. AWS DataSync over AWS Direct Connect ","C. AWS Database Migration Service (AWS DMS) over public internet ","D. AWS Database Migration Service (AWS DMS) over AWS Direct Connect "],"Explanation":"Answer: B \n \n 47 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \nExplanation \nThese are some of the main use cases for AWS DataSync: \u2022 Data migration \u2013 Move active datasets rapidly \nover the network into Amazon S3, Amazon EFS, or FSx for Windows File Server. DataSync includes \nautomatic encryption and data integrity validation to help make sure that your data arrives securely, intact, and \nready to use. \n \n\u0022DataSync includes encryption and integrity validation to help make sure your data arrives securely, intact, \nand ready to use.\u0022 https://aws.amazon.com/datasync/faqs/ \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":79,"QuestionContent":" \nA company runs a highly available image-processing application on Amazon EC2 instances in a single VPC \nThe EC2 instances run inside several subnets across multiple Availability Zones. The EC2 instances do not \ncommunicate with each other However, the EC2 instances download images from Amazon S3 and upload \nimages to Amazon S3 through a single NAT gateway The company is concerned about data transfer charges \n \nWhat is the MOST cost-effective way for the company to avoid Regional data transfer charges? \n ","Option":["A. Launch the NAT gateway in each Availability Zone ","B. Replace the NAT gateway with a NAT instance ","C. Deploy a gateway VPC endpoint for Amazon S3 ","D. Provision an EC2 Dedicated Host to run the EC2 instances "],"Explanation":"Answer: A \n \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":80,"QuestionContent":" \nA company is planning to use an Amazon DynamoDB table for data storage. The company is concerned about \ncost optimization. The table will not be used on most mornings. In the evenings, the read and write traffic will \noften be unpredictable. When traffic spikes occur, they will happen very quickly. \n \nWhat should a solutions architect recommend? \n ","Option":["A. Create a DynamoDB table in on-demand capacity mode. ","B. Create a DynamoDB table with a global secondary index. ","C. Create a DynamoDB table with provisioned capacity and auto scaling. ","D. Create a DynamoDB table in provisioned capacity mode, and configure it as a global table. "],"Explanation":"Answer: A \n \n 48 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":81,"QuestionContent":" \nA company wants to reduce the cost of its existing three-tier web architecture. The web, application, and \ndatabase servers are running on Amazon EC2 instances for the development, test, and production \nenvironments. The EC2 instances average 30% CPU utilization during peak hours and 10% CPU utilization \nduring non-peak hours. \n \nThe production EC2 instances run 24 hours a day. The development and test EC2 instances run for at least 8 \nhours each day. The company plans to implement automation to stop the development and test EC2 instances \nwhen they are not in use. \n \nWhich EC2 instance purchasing solution will meet the company\u0027s requirements MOST cost-effectively? \n ","Option":["A. Use Spot Instances for the production EC2 instances. Use Reserved Instances for the development and \ntest EC2 instances. ","B. Use Reserved Instances for the production EC2 instances. Use On-Demand Instances for the \ndevelopment and test EC2 instances. ","C. Use Spot blocks for the production EC2 instances. Use Reserved Instances for the development and test \nEC2 instances. ","D. Use On-Demand Instances for the production EC2 instances. Use Spot blocks for the development and \ntest EC2 instances. "],"Explanation":"Answer: B \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":82,"QuestionContent":" \nA solutions architect is designing a two-tier web application The application consists of a public-facing web \ntier hosted on Amazon EC2 in public subnets The database tier consists of Microsoft SQL Server running on \nAmazon EC2 in a private subnet Security is a high priority for the company \n \nHow should security groups be configured in this situation? (Select TWO ) \n ","Option":["A. Configure the security group for the web tier to allow inbound traffic on port 443 from 0.0.0.0/0. ","B. Configure the security group for the web tier to allow outbound traffic on port 443 from 0.0.0.0/0. ","C. Configure the security group for the database tier to allow inbound traffic on port 1433 from the security \ngroup for the web tier. ","D. Configure the security group for the database tier to allow outbound traffic on ports 443 and 1433 to the \nsecurity group for the web tier. ","E. Configure the security group for the database tier to allow inbound traffic on ports 443 and 1433 from \nthe security group for the web tier. "],"Explanation":"Answer: A C \n \n 49 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \nExplanation \n\u0022Security groups create an outbound rule for every inbound rule.\u0022 Not completely right. Statefull does NOT \nmean that if you create an inbound (or outbound) rule, it will create an outbound (or inbound) rule. What it \ndoes mean is: suppose you create an inbound rule on port 443 for the X ip. When a request enters on port 443 \nfrom X ip, it will allow traffic out for that request in the port 443. However, if you look at the outbound rules, \nthere will not be any outbound rule on port 443 unless explicitly create it. In ACLs, which are stateless, you \nwould have to create an inbound rule to allow incoming requests and an outbound rule to allow your \napplication responds to those incoming requests. \n \nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html#SecurityGroupRules \n \n","RightAnswer":["A","C"],"QuestionChoose":[]},{"QuestionNumber":83,"QuestionContent":" \nA company wants to improve its ability to clone large amounts of production data into a test environment in \nthe same AWS Region. The data is stored in Amazon EC2 instances on Amazon Elastic Block Store (Amazon \nEBS) volumes. Modifications to the cloned data must not affect the production environment. The software that \naccesses this data requires consistently high I/O performance. \n \nA solutions architect needs to minimize the time that is required to clone the production data into the test \nenvironment. \n \nWhich solution will meet these requirements? \n ","Option":["A. Take EBS snapshots of the production EBS volumes. Restore the snapshots onto EC2 instance store \nvolumes in the test environment. ","B. Configure the production EBS volumes to use the EBS Multi-Attach feature. Take EBS snapshots of the \nproduction EBS volumes. Attach the production EBS volumes to the EC2 instances in the test \nenvironment. ","C. Take EBS snapshots of the production EBS volumes. Create and initialize new EBS volumes. Attach the \nnew EBS volumes to EC2 instances in the test environment before restoring the volumes from the \nproduction EBS snapshots. ","D. Take EBS snapshots of the production EBS volumes. Turn on the EBS fast snapshot restore feature on \nthe EBS snapshots. Restore the snapshots into new EBS volumes. Attach the new EBS volumes to EC2 \ninstances in the test environment. "],"Explanation":"Answer: C \n \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":84,"QuestionContent":" \nA company provides a Voice over Internet Protocol (VoIP) service that uses UDP connections. The service \nconsists of Amazon EC2 instances that run in an Auto Scaling group. The company has deployments across \nmultiple AWS Regions. \n \nThe company needs to route users to the Region with the lowest latency. The company also needs automated \n \n 50 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \nfailover between Regions. \n \nWhich solution will meet these requirements? \n ","Option":["A. Deploy a Network Load Balancer (NLB) and an associated target group. Associate the target group with \nthe Auto Scaling group. Use the NLB as an AWS Global Accelerator endpoint in each Region. ","B. Deploy an Application Load Balancer (ALB) and an associated target group. Associate the target group \nwith the Auto Scaling group. Use the ALB as an AWS Global Accelerator endpoint in each Region. ","C. Deploy a Network Load Balancer (NLB) and an associated target group. Associate the target group with \nthe Auto Scaling group. Create an Amazon Route 53 latency record that points to aliases for each NLB. \nCreate an Amazon CloudFront distribution that uses the latency record as an origin. ","D. Deploy an Application Load Balancer (ALB) and an associated target group. Associate the target group \nwith the Auto Scaling group. Create an Amazon Route 53 weighted record that points to aliases for each \nALB. Deploy an Amazon CloudFront distribution that uses the weighted record as an origin. "],"Explanation":"Answer: D \n \nExplanation \nhttps://aws.amazon.com/global-accelerator/faqs/ \n \nHTTP /HTTPS - ALB ; TCP and UDP - NLB; Lowest latency routing and more throughput. Also supports \nfailover, uses Anycast Ip addressing - Global Accelerator Caching at Egde Locations \u2013 Cloutfront \n \nWS Global Accelerator automatically checks the health of your applications and routes user traffic only to \nhealthy application endpoints. If the health status changes or you make configuration updates, AWS Global \nAccelerator reacts instantaneously to route your users to the next available endpoint.. \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":85,"QuestionContent":" \nAn ecommerce company wants to launch a one-deal-a-day website on AWS. Each day will feature exactly one \nproduct on sale for a period of 24 hours. The company wants to be able to handle millions of requests each \nhour with millisecond latency during peak hours. \n \nWhich solution will meet these requirements with the LEAST operational overhead? \n ","Option":["A. Use Amazon S3 to host the full website in different S3 buckets Add Amazon CloudFront distributions \nSet the S3 buckets as origins for the distributions Store the order data in Amazon S3 ","B. Deploy the full website on Amazon EC2 instances that run in Auto Scaling groups across multiple \nAvailability Zones Add an Application Load Balancer (ALB) to distribute the website traffic Add \nanother ALB for the backend APIs Store the data in Amazon RDS for MySQL ","C. Migrate the full application to run in containers Host the containers on Amazon Elastic Kubernetes \nService (Amazon EKS) Use the Kubernetes Cluster Autoscaler to increase and decrease the number of \npods to process bursts in traffic Store the data in Amazon RDS for MySQL \n 51 of 230  Practice Test Amazon Web Services - SAA-C03 ","D. Use an Amazon S3 bucket to host the website\u0027s static content Deploy an Amazon CloudFront \ndistribution. Set the S3 bucket as the origin Use Amazon API Gateway and AWS Lambda functions for \nthe backend APIs Store the data in Amazon DynamoDB "],"Explanation":"Answer: D \n \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":86,"QuestionContent":" \nA solutions architect is designing a new hybrid architecture to extend a company s on-premises infrastructure \nto AWS The company requires a highly available connection with consistent low latency to an AWS Region. \nThe company needs to minimize costs and is willing to accept slower traffic if the primary connection fails. \n \nWhat should the solutions architect do to meet these requirements? \n ","Option":["A. Provision an AWS Direct Connect connection to a Region Provision a VPN connection as a backup if \nthe primary Direct Connect connection fails. ","B. Provision a VPN tunnel connection to a Region for private connectivity. Provision a second VPN tunnel \nfor private connectivity and as a backup if the primary VPN connection fails. ","C. Provision an AWS Direct Connect connection to a Region Provision a second Direct Connect \nconnection to the same Region as a backup if the primary Direct Connect connection fails. ","D. Provision an AWS Direct Connect connection to a Region Use the Direct Connect failover attribute \nfrom the AWS CLI to automatically create a backup connection if the primary Direct Connect \nconnection fails. "],"Explanation":"Answer: A \n \nExplanation \n\u0022In some cases, this connection alone is not enough. It is always better to guarantee a fallback connection as \nthe backup of DX. There are several options, but implementing it with an AWS Site-To-Site VPN is a real \ncost-effective solution that can be exploited to reduce costs or, in the meantime, wait for the setup of a second \nDX.\u0022 \n \nhttps://www.proud2becloud.com/hybrid-cloud-networking-backup-aws-direct-connect-network-connection-with \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":87,"QuestionContent":" \nA company has thousands of edge devices that collectively generate 1 TB of status alerts each day. Each alert \nis approximately 2 KB in size. A solutions architect needs to implement a solution to ingest and store the alerts \nfor future analysis. \n \nThe company wants a highly available solution. However, the company needs to minimize costs and does not \nwant to manage additional infrastructure. Ad ditionally, the company wants to keep 14 days of data available \nfor immediate analysis and archive any data older than 14 days. \n \n 52 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \nWhat is the MOST operationally efficient solution that meets these requirements? \n ","Option":["A. Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts Configure the Kinesis Data \nFirehose stream to deliver the alerts to an Amazon S3 bucket Set up an S3 Lifecycle configuration to \ntransition data to Amazon S3 Glacier after 14 days ","B. Launch Amazon EC2 instances across two Availability Zones and place them behind an Elastic Load \nBalancer to ingest the alerts Create a script on the EC2 instances that will store tne alerts m an Amazon \nS3 bucket Set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days ","C. Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts Configure the Kinesis Data \nFirehose stream to deliver the alerts to an Amazon Elasticsearch Service (Amazon ES) duster Set up the \nAmazon ES cluster to take manual snapshots every day and delete data from the duster that is older than \n14 days ","D. Create an Amazon Simple Queue Service (Amazon SQS i standard queue to ingest the alerts and set the \nmessage retention period to 14 days Configure consumers to poll the SQS queue check the age of the \nmessage and analyze the message data as needed If the message is 14 days old the consumer should \ncopy the message to an Amazon S3 bucket and delete the message from the SQS queue "],"Explanation":"Answer: A \nExplanation \nhttps://aws.amazon.com/kinesis/data-firehose/features/?nc=sn\u0026loc=2#:~:text=into%20Amazon%20S3%2C%20 \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":88,"QuestionContent":" \nA global company hosts its web application on Amazon EC2 instances behind an Application Load Balancer \n(ALB). The web application has static data and dynamic data. The company stores its static data in an Amazon \nS3 bucket. The company wants to improve performance and reduce latency for the static data and dynamic \ndata. The company is using its own domain name registered with Amazon Route 53. \n \nWhat should a solutions architect do to meet these requirements? \n ","Option":["A. Create an Amazon CloudFront distribution that has the S3 bucket and the ALB as origins Configure \nRoute 53 to route traffic to the CloudFront distribution. ","B. Create an Amazon CloudFront distribution that has the ALB as an origin Create an AWS Global \nAccelerator standard accelerator that has the S3 bucket as an endpoint. Configure Route 53 to route \ntraffic to the CloudFront distribution. ","C. Create an Amazon CloudFront distribution that has the S3 bucket as an origin Create an AWS Global \nAccelerator standard accelerator that has the ALB and the CloudFront distribution as endpoints Create a \ncustom domain name that points to the accelerator DNS name Use the custom domain name as an \nendpoint for the web application. ","D. Create an Amazon CloudFront distribution that has the ALB as an origin C. Create an AWS Global \nAccelerator standard accelerator that has the S3 bucket as an endpoint Create two domain names. Point \none domain name to the CloudFront DNS name for dynamic content, Point the other domain name to \n 53 of 230  Practice Test Amazon Web Services - SAA-C03 \nthe accelerator DNS name for static content Use the domain names as endpoints for the web application. "],"Explanation":"Answer: C \n \nExplanation \nStatic content can be cached at Cloud front Edge locations from S3 and dynamic content EC2 behind the ALB \nwhose performance can be improved by Global Accelerator whose one endpoint is ALB and other Cloud front. \nSo with regards to custom domain name endpoint is web application is R53 alias records for the custom \ndomain point to web application \nhttps://aws.amazon.com/blogs/networking-and-content-delivery/improving-availability-and-performance-for-ap \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":89,"QuestionContent":" \nA company has an automobile sales website that stores its listings in a database on Amazon RDS When an \nautomobile is sold the listing needs to be removed from the website and the data must be sent to multiple \ntarget systems. \n \nWhich design should a solutions architect recommend? \n ","Option":["A. Create an AWS Lambda function triggered when the database on Amazon RDS is updated to send the \ninformation to an Amazon Simple Queue Service (Amazon SQS\u003E queue for the targets to consume ","B. Create an AWS Lambda function triggered when the database on Amazon RDS is updated to send the \ninformation to an Amazon Simple Queue Service (Amazon SQS) FIFO queue for the targets to consume ","C. Subscribe to an RDS event notification and send an Amazon Simple Queue Service (Amazon SQS) \nqueue fanned out to multiple Amazon Simple Notification Service (Amazon SNS) topics Use AWS \nLambda functions to update the targets ","D. Subscribe to an RDS event notification and send an Amazon Simple Notification Service (Amazon \nSNS) topic fanned out to multiple Amazon Simple Queue Service (Amazon SQS) queues Use AWS \nLambda functions to update the targets "],"Explanation":"Answer: D \n \nExplanation \nhttps://docs.aws.amazon.com/lambda/latest/dg/services-rds.html \nhttps://docs.aws.amazon.com/lambda/latest/dg/with-sns.html \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":90,"QuestionContent":" \nA company\u0027s HTTP application is behind a Network Load Balancer (NLB). The NLB\u0027s target group is \nconfigured to use an Amazon EC2 Auto Scaling group with multiple EC2 instances that run the web service. \n \nThe company notices that the NLB is not detecting HTTP errors for the application. These errors require a \nmanual restart of the EC2 instances that run the web service. The company needs to improve the application\u0027s \navailability without writing custom scripts or code. \n \n 54 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \n \nWhat should a solutions architect do to meet these requirements? \n ","Option":["A. Enable HTTP health checks on the NLB. supplying the URL of the company\u0027s application. ","B. Add a cron job to the EC2 instances to check the local application\u0027s logs once each minute. If HTTP \nerrors are detected, the application will restart. ","C. Replace the NLB with an Application Load Balancer. Enable HTTP health checks by supplying the \nURL of the company\u0027s application. Configure an Auto Scaling action to replace unhealthy instances. ","D. Create an Amazon Cloud Watch alarm that monitors the UnhealthyHostCount metric for the NLB. \nConfigure an Auto Scaling action to replace unhealthy instances when the alarm is in the ALARM state. "],"Explanation":"Answer: C \n \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":91,"QuestionContent":" \nA solutions architect is designing the cloud architecture for a new application being deployed on AWS. The \nprocess should run in parallel while adding and removing application nodes as needed based on the number of \njobs to be processed. The processor application is stateless. The solutions architect must ensure that the \napplication is loosely coupled and the job items are durably stored. \n \nWhich design should the solutions architect use? \n ","Option":["A. Create an Amazon SNS topic to send the jobs that need to be processed Create an Amazon Machine \nImage (AMI) that consists of the processor application Create a launch configuration that uses the AMI \nCreate an Auto Scaling group using the launch configuration Set the scaling policy for the Auto Scaling \ngroup to add and remove nodes based on CPU usage ","B. Create an Amazon SQS queue to hold the jobs that need to be processed Create an Amazon Machine \nimage (AMI) that consists of the processor application Create a launch configuration that uses the AM\u0027 \nCreate an Auto Scaling group using the launch configuration Set the scaling policy for the Auto Scaling \ngroup to add and remove nodes based on network usage ","C. Create an Amazon SQS queue to hold the jobs that needs to be processed Create an Amazon Machine \nimage (AMI) that consists of the processor application Create a launch template that uses the AMI \nCreate an Auto Scaling group using the launch template Set the scaling policy for the Auto Scaling \ngroup to add and remove nodes based on the number of items in the SQS queue ","D. Create an Amazon SNS topic to send the jobs that need to be processed Create an Amazon Machine \nImage (AMI) that consists of the processor application Create a launch template that uses the AMI \nCreate an Auto Scaling group using the launch template Set the scaling policy for the Auto Scaling \ngroup to add and remove nodes based on the number of messages published to the SNS topic "],"Explanation":"Answer: C \n \nExplanation \n \n 55 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \n\u0022Create an Amazon SQS queue to hold the jobs that needs to be processed. Create an Amazon EC2 Auto \nScaling group for the compute application. Set the scaling policy for the Auto Scaling group to add and \nremove nodes based on the number of items in the SQS queue\u0022 \n \nIn this case we need to find a durable and loosely coupled solution for storing jobs. Amazon SQS is ideal for \nthis use case and can be configured to use dynamic scaling based on the number of jobs waiting in the \nqueue.To configure this scaling you can use the backlog per instance metric with the target value being the \nacceptable backlog per instance to maintain. You can calculate these numbers as follows: Backlog per \ninstance: To calculate your backlog per instance, start with the ApproximateNumberOfMessages queue \nattribute to determine the length of the SQS queue \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":92,"QuestionContent":" \nA company needs to store data in Amazon S3 and must prevent the data from being changed. The company \nwants new objects that are uploaded to Amazon S3 to remain unchangeable for a nonspecific amount of time \nuntil the company decides to modify the objects. Only specific users in the company\u2019s AWS account can have \nthe ability to delete the objects. What should a solutions architect do to meet these requirements? \n ","Option":["A. Create an S3 Glacier vault Apply a write-once, read-many (WORM) vault lock policy to the objects ","B. Create an S3 bucket with S3 Object Lock enabled Enable versioning Set a retention period of 100 years \nUse governance mode as the S3 bucket\u0027s default retention mode for new objects ","C. Create an S3 bucket Use AWS CloudTrail to (rack any S3 API events that modify the objects Upon \nnotification, restore the modified objects from any backup versions that the company has ","D. Create an S3 bucket with S3 Object Lock enabled Enable versioning Add a legal hold to the objects Add \nthe s3 PutObjectLegalHold permission to the IAM policies of users who need to delete the objects "],"Explanation":"Answer: D \n \nExplanation \n\u0022The Object Lock legal hold operation enables you to place a legal hold on an object version. Like setting a \nretention period, a legal hold prevents an object version from being overwritten or deleted. However, a legal \nhold doesn\u0027t have an associated retention period and remains in effect until removed.\u0022 \nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/batch-ops-legal-hold.html \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":93,"QuestionContent":" \nA company is running an SMB file server in its data center. The file server stores large files that are accessed \nfrequently for the first few days after the files are created. After 7 days the files are rarely accessed. \n \nThe total data size is increasing and is close to the company\u0027s total storage capacity. A solutions architect must \nincrease the company\u0027s available storage space without losing low-latency access to the most recently accessed \nfiles. The solutions architect must also provide file lifecycle management to avoid future storage issues. \n \nWhich solution will meet these requirements? \n \n 56 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n ","Option":["A. Use AWS DataSync to copy data that is older than 7 days from the SMB file server to AWS. ","B. Create an Amazon S3 File Gateway to extend the company\u0027s storage space. Create an S3 Lifecycle \npolicy to transition the data to S3 Glacier Deep Archive after 7 days. ","C. Create an Amazon FSx for Windows File Server file system to extend the company\u0027s storage space. ","D. Install a utility on each user\u0027s computer to access Amazon S3. Create an S3 Lifecycle policy to transition \nthe data to S3 Glacier Flexible Retrieval after 7 days. "],"Explanation":"Answer: A \n \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":94,"QuestionContent":" \nA company has an application that runs on Amazon EC2 instances and uses an Amazon Aurora database. The \nEC2 instances connect to the database by using user names and passwords that are stored locally in a file. The \ncompany wants to minimize the operational overhead of credential management. \n \nWhat should a solutions architect do to accomplish this goal? \n ","Option":["A. Use AWS Secrets Manager. Turn on automatic rotation. ","B. Use AWS Systems Manager Parameter Store. Turn on automatic rotation. ","C. Create an Amazon S3 bucket lo store objects that are encrypted with an AWS Key C. Management \nService (AWS KMS) encryption key. Migrate the credential file to the S3 bucket. Point the application \nto the S3 bucket. ","D. Create an encrypted Amazon Elastic Block Store (Amazon EBS) volume (or each EC2 instance. Attach \nthe new EBS volume to each EC2 instance. Migrate the credential file to the new EBS volume. Point the \napplication to the new EBS volume. "],"Explanation":"Answer: A \n \nExplanation \nhttps://aws.amazon.com/cn/blogs/security/how-to-connect-to-aws-secrets-manager-service-within-a-virtual-priv \nhttps://aws.amazon.com/blogs/security/rotate-amazon-rds-database-credentials-automatically-with-aws-secrets- \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":95,"QuestionContent":" \nA company is storing sensitive user information in an Amazon S3 bucket The company wants to provide \nsecure access to this bucket from the application tier running on Ama2on EC2 instances inside a VPC. \n \nWhich combination of steps should a solutions architect take to accomplish this? (Select TWO.) \n ","Option":["A. Configure a VPC gateway endpoint for Amazon S3 within the VPC \n 57 of 230  Practice Test Amazon Web Services - SAA-C03 ","B. Create a bucket policy to make the objects to the S3 bucket public ","C. Create a bucket policy that limits access to only the application tier running in the VPC ","D. Create an IAM user with an S3 access policy and copy the IAM credentials to the EC2 instance ","E. Create a NAT instance and have the EC2 instances use the NAT instance to access the S3 bucket "],"Explanation":"Answer: A C \n \nExplanation \nhttps://aws.amazon.com/premiumsupport/knowledge-center/s3-private-connection-no-authentication/ \n \n","RightAnswer":["A","C"],"QuestionChoose":[]},{"QuestionNumber":96,"QuestionContent":" \nA company hosts its multi-tier applications on AWS. For compliance, governance, auditing, and security, the \ncompany must track configuration changes on its AWS resources and record a history of API calls made to \nthese resources. \n \nWhat should a solutions architect do to meet these requirements? \n ","Option":["A. Use AWS CloudTrail to track configuration changes and AWS Config to record API calls ","B. Use AWS Config to track configuration changes and AWS CloudTrail to record API calls ","C. Use AWS Config to track configuration changes and Amazon CloudWatch to record API calls ","D. Use AWS CloudTrail to track configuration changes and Amazon CloudWatch to record API calls "],"Explanation":"Answer: B \n \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":97,"QuestionContent":" \nA company\u0027s application integrates with multiple software-as-a-service (SaaS) sources for data collection. The \ncompany runs Amazon EC2 instances to receive the data and to upload the data to an Amazon S3 bucket for \nanalysis. The same EC2 instance that receives and uploads the data also sends a notification to the user when \nan upload is complete. The company has noticed slow application performance and wants to improve the \nperformance as much as possible. \n \nWhich solution will meet these requirements with the LEAST operational overhead? \n ","Option":["A. Create an Auto Scaling group so that EC2 instances can scale out. Configure an S3 event notification to \nsend events to an Amazon Simple Notification Service (Amazon SNS) topic when the upload to the S3 \nbucket is complete. ","B. Create an Amazon AppFlow flow to transfer data between each SaaS source and the S3 bucket. \n 58 of 230  Practice Test Amazon Web Services - SAA-C03 \nConfigure an S3 event notification to send events to an Amazon Simple Notification Service (Amazon \nSNS) topic when the upload to the S3 bucket is complete. ","C. Create an Amazon EventBridge (Amazon CloudWatch Events) rule for each SaaS source to send output \ndata. Configure the S3 bucket as the rule\u0027s target. Create a second EventBridge (CloudWatch Events) \nrule to send events when the upload to the S3 bucket is complete. Configure an Amazon Simple \nNotification Service (Amazon SNS) topic as the second rule\u0027s target. ","D. Create a Docker container to use instead of an EC2 instance. Host the containerized application on \nAmazon Elastic Container Service (Amazon ECS). Configure Amazon CloudWatch Container Insights \nto send events to an Amazon Simple Notification Service (Amazon SNS) topic when the upload to the \nS3 bucket is complete. "],"Explanation":"Answer: B \n \nExplanation \nAmazon AppFlow is a fully managed integration service that enables you to securely transfer data between \nSoftware-as-a-Service (SaaS) applications like Salesforce, SAP, Zendesk, Slack, and ServiceNow, and AWS \nservices like Amazon S3 and Amazon Redshift, in just a few clicks. https://aws.amazon.com/appflow/ \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":98,"QuestionContent":" \nA company is running a business-critical web application on Amazon EC2 instances behind an Application \nLoad Balancer. The EC2 instances are in an Auto Scaling group. The application uses an Amazon Aurora \nPostgreSQL database that is deployed in a single Availability Zone. The company wants the application to be \nhighly available with minimum downtime and minimum loss of data. \n \nWhich solution will meet these requirements with the LEAST operational effort? \n ","Option":["A. Place the EC2 instances in different AWS Regions. Use Amazon Route 53 health checks to redirect \ntraffic. Use Aurora PostgreSQL Cross-Region Replication. ","B. Configure the Auto Scaling group to use multiple Availability Zones. Configure the database as \nMulti-AZ. Configure an Amazon RDS Proxy instance for the database. ","C. Configure the Auto Scaling group to use one Availability Zone. Generate hourly snapshots of the \ndatabase. Recover the database from the snapshots in the event of a failure. ","D. Configure the Auto Scaling group to use multiple AWS Regions. Write the data from the application to \nAmazon S3. Use S3 Event Notifications to launch an AWS Lambda function to write the data to the \ndatabase. "],"Explanation":"Answer: B \n \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":99,"QuestionContent":"A company wants to migrate an on-premises data center to AWS. The data canter hosts an SFTP server that \n \n 59 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \nstores its data on an NFS-based file system. The server holds 200 GB of data that needs to be transferred. The \nserver must be hosted on an Amazon EC2 instance that uses an Amazon Elastic File System (Amazon EFS) \nfile system \n \nWhen combination of steps should a solutions architect take to automate this task? (Select TWO ) \n ","Option":["A. Launch the EC2 instance into the same Avalability Zone as the EFS fie system ","B. install an AWS DataSync agent m the on-premises data center ","C. Create a secondary Amazon Elastic Block Store (Amazon EBS) volume on the EC2 instance tor the data ","D. Manually use an operating system copy command to push the data to the EC2 instance ","E. Use AWS DataSync to create a suitable location configuration for the onprermises SFTP server "],"Explanation":"Answer: A B \n \n \n","RightAnswer":["A","B"],"QuestionChoose":[]},{"QuestionNumber":100,"QuestionContent":" \nA company hosts its web applications in the AWS Cloud. The company configures Elastic Load Balancers to \nuse certificate that are imported into AWS Certificate Manager (ACM). The company\u2019s security team must be \nnotified 30 days before the expiration of each certificate. \n \nWhat should a solutions architect recommend to meet the requirement? \n ","Option":["A. Add a rule m ACM to publish a custom message to an Amazon Simple Notification Service (Amazon \nSNS) topic every day beginning 30 days before any certificate will expire. ","B. Create an AWS Config rule that checks for certificates that will expire within 30 days. Configure \nAmazon EventBridge (Amazon CloudWatch Events) to invoke a custom alert by way of Amazon \nSimple Notification Service (Amazon SNS) when AWS Config reports a noncompliant resource ","C. Use AWS trusted Advisor to check for certificates that will expire within to days. Create an Amazon \nCloudWatch alarm that is based on Trusted Advisor metrics for check status changes Configure the \nalarm to send a custom alert by way of Amazon Simple rectification Service (Amazon SNS) ","D. Create an Amazon EventBridge (Amazon CloudWatch Events) rule to detect any certificates that will \nexpire within 30 days. Configure the rule to invoke an AWS Lambda function. Configure the Lambda \nfunction to send a custom alert by way of Amazon Simple Notification Service (Amazon SNS). "],"Explanation":"Answer: B \n \nExplanation \nhttps://aws.amazon.com/premiumsupport/knowledge-center/acm-certificate-expiration/ \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":101,"QuestionContent":" \n 60 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \n \nA company performs monthly maintenance on its AWS infrastructure. During these maintenance activities, the \ncompany needs to rotate the credentials tor its Amazon ROS tor MySQL databases across multiple AWS \nRegions \n \nWhich solution will meet these requirements with the LEAST operational overhead? \n ","Option":["A. Store the credentials as secrets in AWS Secrets Manager. Use multi-Region secret replication for the \nrequired Regions Configure Secrets Manager to rotate the secrets on a schedule ","B. Store the credentials as secrets in AWS Systems Manager by creating a secure string parameter Use \nmulti-Region secret replication for the required Regions Configure Systems Manager to rotate the \nsecrets on a schedule ","C. Store the credentials in an Amazon S3 bucket that has server-side encryption (SSE) enabled Use \nAmazon EventBridge (Amazon CloudWatch Events) to invoke an AWS Lambda function to rotate the \ncredentials ","D. Encrypt the credentials as secrets by using AWS Key Management Service (AWS KMS) multi-Region \ncustomer managed keys Store the secrets in an Amazon DynamoDB global table Use an AWS Lambda \nfunction to retrieve the secrets from DynamoDB Use the RDS API to rotate the secrets. "],"Explanation":"Answer: A \n \nExplanation \nhttps://aws.amazon.com/blogs/security/how-to-replicate-secrets-aws-secrets-manager-multiple-regions/ \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":102,"QuestionContent":" \nA company runs a photo processing application that needs to frequently upload and download pictures from \nAmazon S3 buckets that are located in the same AWS Region. A solutions architect has noticed an increased \ncost in data transfer fees and needs to implement a solution to reduce these costs. \n \nHow can the solutions architect meet this requirement? \n ","Option":["A. Deploy Amazon API Gateway into a public subnet and adjust the route table to route S3 calls through It. ","B. Deploy a NAT gateway into a public subnet and attach an end point policy that allows access to the S3 \nbuckets. ","C. Deploy the application Into a public subnet and allow it to route through an internet gateway to access \nthe S3 Buckets ","D. Deploy an S3 VPC gateway endpoint into the VPC and attach an endpoint policy that allows access to \nthe S3 buckets. "],"Explanation":"Answer: D \n \n 61 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":103,"QuestionContent":" \nA company has an application that generates a large number of files, each approximately 5 MB in size. The \nfiles are stored in Amazon S3. Company policy requires the files to be stored for 4 years before they can be \ndeleted Immediate accessibility is always required as the files contain critical business data that is not easy to \nreproduce. The files are frequently accessed in the first 30 days of the object creation but are rarely accessed \nafter the first 30 days \n \nWhich storage solution is MOST cost-effective? \n ","Option":["A. Create an S3 bucket lifecycle policy to move Mm from S3 Standard to S3 Glacier 30 days from object \ncreation Delete the Tiles 4 years after object creation ","B. Create an S3 bucket lifecycle policy to move tiles from S3 Standard to S3 One Zone-infrequent Access \n(S3 One Zone-IA] 30 days from object creation. Delete the fees 4 years after object creation ","C. Create an S3 bucket lifecycle policy to move files from S3 Standard-infrequent Access (S3 Standard \n-lA) 30 from object creation. Delete the ties 4 years after object creation ","D. Create an S3 bucket Lifecycle policy to move files from S3 Standard to S3 Standard-Infrequent Access \n(S3 Standard-IA) 30 days from object creation Move the files to S3 Glacier 4 years after object carton. "],"Explanation":"Answer: B \n \nExplanation \nhttps://aws.amazon.com/s3/storage-classes/?trk=66264cd8-3b73-416c-9693-ea7cf4fe846a\u0026sc_channel=ps\u0026s_k \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":104,"QuestionContent":" \nA company needs to review its AWS Cloud deployment to ensure that its Amazon S3 buckets do not have \nunauthorized configuration changes. \n \nWhat should a solutions architect do to accomplish this goal? \n ","Option":["A. Turn on AWS Config with the appropriate rules. ","B. Turn on AWS Trusted Advisor with the appropriate checks. ","C. Turn on Amazon Inspector with the appropriate assessment template. ","D. Turn on Amazon S3 server access logging. Configure Amazon EventBridge (Amazon Cloud Watch \nEvents). "],"Explanation":"Answer: D \n \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":105,"QuestionContent":" \n 62 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \nAn image-processing company has a web application that users use to upload images. The application uploads \nthe images into an Amazon S3 bucket. The company has set up S3 event notifications to publish the object \ncreation events to an Amazon Simple Queue Service (Amazon SQS) standard queue. The SQS queue serves as \nthe event source for an AWS Lambda function that processes the images and sends the results to users through \nemail. \n \nUsers report that they are receiving multiple email messages for every uploaded image. A solutions architect \ndetermines that SQS messages are invoking the Lambda function more than once, resulting in multiple email \nmessages. \n \nWhat should the solutions architect do to resolve this issue with the LEAST operational overhead? \n ","Option":["A. Set up long polling in the SQS queue by increasing the ReceiveMessage wait time to 30 seconds. ","B. Change the SQS standard queue to an SQS FIFO queue. Use the message deduplication ID to discard \nduplicate messages. ","C. Increase the visibility timeout in the SQS queue to a value that is greater than the total of the function \ntimeout and the batch window timeout. ","D. Modify the Lambda function to delete each message from the SQS queue immediately after the message \nis read before processing. "],"Explanation":"Answer: C \n \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":106,"QuestionContent":" \nA company recently migrated to AWS and wants to implement a solution to protect the traffic that flows in \nand out of the production VPC. The company had an inspection server in its on-premises data center. The \ninspection server performed specific operations such as traffic flow inspection and traffic filtering. The \ncompany wants to have the same functionalities in the AWS Cloud. \n \nWhich solution will meet these requirements? \n ","Option":["A. Use Amazon GuardDuty for traffic inspection and traffic filtering in the production VPC ","B. Use Traffic Mirroring to mirror traffic from the production VPC for traffic inspection and filtering. ","C. Use AWS Network Firewall to create the required rules for traffic inspection and traffic filtering for the \nproduction VPC. ","D. Use AWS Firewall Manager to create the required rules for traffic inspection and traffic filtering for the \nproduction VPC. "],"Explanation":"Answer: C \n \nExplanation \nAWS Network Firewall supports both inspection and filtering as required \n \n 63 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \n \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":107,"QuestionContent":" \nA company wants to move a multi-tiered application from on premises to the AWS Cloud to improve the \napplication\u0027s performance. The application consists of application tiers that communicate with each other by \nway of RESTful services. Transactions are dropped when one tier becomes overloaded. A solutions architect \nmust design a solution that resolves these issues and modernizes the application. \n \nWhich solution meets these requirements and is the MOST operationally efficient? \n ","Option":["A. Use Amazon API Gateway and direct transactions to the AWS Lambda functions as the application \nlayer. Use Amazon Simple Queue Service (Amazon SQS) as the communication layer between \napplication services. ","B. Use Amazon CloudWatch metrics to analyze the application performance history to determine the \nserver\u0027s peak utilization during the performance failures. Increase the size of the application server\u0027s \nAmazon EC2 instances to meet the peak requirements. ","C. Use Amazon Simple Notification Service (Amazon SNS) to handle the messaging between application \nservers running on Amazon EC2 in an Auto Scaling group. Use Amazon CloudWatch to monitor the \nSNS queue length and scale up and down as required. ","D. Use Amazon Simple Queue Service (Amazon SQS) to handle the messaging between application \nservers running on Amazon EC2 in an Auto Scaling group. Use Amazon CloudWatch to monitor the \nSQS queue length and scale up when communication failures are detected. "],"Explanation":"Answer: A \n \nExplanation \nhttps://aws.amazon.com/getting-started/hands-on/build-serverless-web-app-lambda-apigateway-s3-dynamodb-c \n \nBuild a Serverless Web Application with AWS Lambda, Amazon API Gateway, AWS Amplify, Amazon \nDynamoDB, and Amazon Cognito. This example showed similar setup as question: Build a Serverless Web \nApplication with AWS Lambda, Amazon API Gateway, AWS Amplify, Amazon DynamoDB, and Amazon \nCognito \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":108,"QuestionContent":" \nA company wants to run its critical applications in containers to meet requirements tor scalability and \navailability The company prefers to focus on maintenance of the critical applications The company does not \nwant to be responsible for provisioning and managing the underlying infrastructure that runs the containerized \nworkload \n \nWhat should a solutions architect do to meet those requirements? \n ","Option":["A. Use Amazon EC2 Instances, and Install Docker on the Instances ","B. Use Amazon Elastic Container Service (Amazon ECS) on Amazon EC2 worker nodes \n 64 of 230  Practice Test Amazon Web Services - SAA-C03 ","C. Use Amazon Elastic Container Service (Amazon ECS) on AWS Fargate ","D. Use Amazon EC2 instances from an Amazon Elastic Container Service (Amazon ECS)-op6mized \nAmazon Machine Image (AMI). "],"Explanation":"Answer: C \n \nExplanation \nusing AWS ECS on AWS Fargate since they requirements are for scalability and availability without having to \nprovision and manage the underlying infrastructure to run the containerized workload. \nhttps://docs.aws.amazon.com/AmazonECS/latest/userguide/what-is-fargate.html \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":109,"QuestionContent":" \nA solutions architect is designing a VPC with public and private subnets. The VPC and subnets use IPv4 \nCIDR blocks. There is one public subnet and one private subnet in each of three Availability Zones (AZs) for \nhigh availability. An internet gateway is used to provide internet access for the public subnets. The private \nsubnets require access to the internet to allow Amazon EC2 instances to download software updates. \n \nWhat should the solutions architect do to enable Internet access for the private subnets? \n ","Option":["A. Create three NAT gateways, one for each public subnet in each AZ. Create a private route table for each \nAZ that forwards non-VPC traffic to the NAT gateway in its AZ. ","B. Create three NAT instances, one for each private subnet in each AZ. Create a private route table for each \nAZ that forwards non-VPC traffic to the NAT instance in its AZ. ","C. Create a second internet gateway on one of the private subnets. Update the route table for the private \nsubnets that forward non-VPC traffic to the private internet gateway. ","D. Create an egress-only internet gateway on one of the public subnets. Update the route table for the \nprivate subnets that forward non-VPC traffic to the egress- only internet gateway. "],"Explanation":"Answer: A \n \nExplanation \nhttps://aws.amazon.com/about-aws/whats-new/2018/03/introducing-amazon-vpc-nat-gateway-in-the-aws-govclo \nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":110,"QuestionContent":" \nA company\u0027s containerized application runs on an Amazon EC2 instance. The application needs to download \nsecurity certificates before it can communicate with other business applications. The company wants a highly \nsecure solution to encrypt and decrypt the certificates in near real time. The solution also needs to store data in \nhighly available storage after the data is encrypted. \n \n 65 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \nWhich solution will meet these requirements with the LEAST operational overhead? \n ","Option":["A. Create AWS Secrets Manager secrets for encrypted certificates. Manually update the certificates as \nneeded. Control access to the data by using fine-grained IAM access. ","B. Create an AWS Lambda function that uses the Python cryptography library to receive and perform \nencryption operations. Store the function in an Amazon S3 bucket. ","C. Create an AWS Key Management Service (AWS KMS) customer managed key. Allow the EC2 role to \nuse the KMS key for encryption operations. Store the encrypted data on Amazon S3. ","D. Create an AWS Key Management Service (AWS KMS) customer managed key. Allow the EC2 role to \nuse the KMS key for encryption operations. Store the encrypted data on Amazon Elastic Block Store \n(Amazon EBS) volumes. "],"Explanation":"Answer: D \n \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":111,"QuestionContent":" \nA company hosts a data lake on AWS. The data lake consists of data in Amazon S3 and Amazon RDS for \nPostgreSQL. The company needs a reporting solution that provides data visualization and includes all the data \nsources within the data lake. Only the company\u0027s management team should have full access to all the \nvisualizations. The rest of the company should have only limited access. \n \nWhich solution will meet these requirements? \n ","Option":["A. Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish \ndashboards to visualize the data. Share the dashboards with the appropriate IAM roles. ","B. Create an analysis in Amazon OuickSighl. Connect all the data sources and create new datasets. Publish \ndashboards to visualize the data. Share the dashboards with the appropriate users and groups. ","C. Create an AWS Glue table and crawler for the data in Amazon S3. Create an AWS Glue extract, \ntransform, and load (ETL) job to produce reports. Publish the reports to Amazon S3. Use S3 bucket \npolicies to limit access to the reports. ","D. Create an AWS Glue table and crawler for the data in Amazon S3. Use Amazon Athena Federated \nQuery to access data within Amazon RDS for PoslgreSQL. Generate reports by using Amazon Athena. \nPublish the reports to Amazon S3. Use S3 bucket policies to limit access to the reports. "],"Explanation":"Answer: A \n \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":112,"QuestionContent":" \nAn application runs on an Amazon EC2 instance in a VPC. The application processes logs that are stored in an \nAmazon S3 bucket. The EC2 instance needs to access the S3 bucket without connectivity to the internet. \n \n 66 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \nWhich solution will provide private network connectivity to Amazon S3? \n ","Option":["A. Create a gateway VPC endpoint to the S3 bucket. ","B. Stream the logs to Amazon CloudWatch Logs. Export the logs to the S3 bucket. ","C. Create an instance profile on Amazon EC2 to allow S3 access. ","D. Create an Amazon API Gateway API with a private link to access the S3 endpoint. "],"Explanation":"Answer: A \nExplanation \nVPC endpoint allows you to connect to AWS services using a private network instead of using the public \nInternet \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":113,"QuestionContent":" \nAn Amazon EC2 administrator created the following policy associated with an IAM group containing several \nusers \n \n 67 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nWhat is the effect of this policy? \n ","Option":["A. Users can terminate an EC2 instance in any AWS Region except us-east-1. ","B. Users can terminate an EC2 instance with the IP address 10 100 100 1 in the us-east-1 Region ","C. Users can terminate an EC2 instance in the us-east-1 Region when the user\u0027s source IP is \n10.100.100.254. ","D. Users cannot terminate an EC2 instance in the us-east-1 Region when the user\u0027s source IP is 10.100 100 \n254 "],"Explanation":"Answer: C \n \nExplanation \nas the policy prevents anyone from doing any EC2 action on any region except us-east-1 and allows only users \nwith source ip 10.100.100.0/24 to terminate instances. So user with source ip 10.100.100.254 can terminate \ninstances in us-east-1 region. \n \n 68 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":114,"QuestionContent":"A company has a data ingestion workflow that consists the following: \nAn Amazon Simple Notification Service (Amazon SNS) topic for notifications about new data \ndeliveries \n \nAn AWS Lambda function to process the data and record metadata \n \nThe company observes that the ingestion workflow fails occasionally because of network connectivity issues. \nWhen such a failure occurs, the Lambda function does not ingest the corresponding data unless the company \nmanually reruns the job. \n \nWhich combination of actions should a solutions architect take to ensure that the Lambda function ingests all \ndata in the future? (Select TWO.) \n ","Option":["A. Configure the Lambda function In multiple Availability Zones. ","B. Create an Amazon Simple Queue Service (Amazon SQS) queue, and subscribe It to me SNS topic. ","C. Increase the CPU and memory that are allocated to the Lambda function. ","D. Increase provisioned throughput for the Lambda function. ","E. Modify the Lambda function to read from an Amazon Simple Queue Service (Amazon SQS) queue "],"Explanation":"Answer: B E \n \n 69 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \n \nTopic 2, Exam Pool B \n","RightAnswer":["B","E"],"QuestionChoose":[]},{"QuestionNumber":1,"QuestionContent":" \nOrganizers for a global event want to put daily reports online as static HTML pages. The pages are expected to \ngenerate millions of views from users around the world. The files are stored In an Amazon S3 bucket. A \nsolutions architect has been asked to design an efficient and effective solution. \n \nWhich action should the solutions architect take to accomplish this? \n ","Option":["A. Generate presigned URLs for the files. ","B. Use cross-Region replication to all Regions. ","C. Use the geoproximtty feature of Amazon Route 53. ","D. Use Amazon CloudFront with the S3 bucket as its origin. "],"Explanation":"Answer: D \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":2,"QuestionContent":"A company has a highly dynamic batch processing job that uses many Amazon EC2 instances to complete it. \nThe job is stateless in nature, can be started and stopped at any given time with no negative impact, and \ntypically takes upwards of 60 minutes total to complete. The company has asked a solutions architect to design \na scalable and cost-effective solution that meets the requirements of the job. \n \nWhat should the solutions architect recommend? \n ","Option":["A. Implement EC2 Spot Instances ","B. Purchase EC2 Reserved Instances ","C. Implement EC2 On-Demand Instances ","D. Implement the processing on AWS Lambda "],"Explanation":"Answer: A \n \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":3,"QuestionContent":" \nA medical records company is hosting an application on Amazon EC2 instances. The application processes \ncustomer data files that are stored on Amazon S3. The EC2 instances are hosted in public subnets. The EC2 \ninstances access Amazon S3 over the internet, but they do not require any other network access. \n \nA new requirement mandates that the network traffic for file transfers take a private route and not be sent over \n \n 70 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \nthe internet. \n \nWhich change to the network architecture should a solutions architect recommend to meet this requirement? \n ","Option":["A. Create a NAT gateway. Configure the route table for the public subnets to send traffic to Amazon S3 \nthrough the NAT gateway. ","B. Configure the security group for the EC2 instances to restrict outbound traffic so that only traffic to the \nS3 prefix list is permitted. ","C. Move the EC2 instances to private subnets. Create a VPC endpoint for Amazon S3, and link the \nendpoint to the route table for the private subnets ","D. Remove the internet gateway from the VPC. Set up an AWS Direct Connect connection, and route \ntraffic to Amazon S3 over the Direct Connect connection. "],"Explanation":"Answer: C \n \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":4,"QuestionContent":" \nA company is building a web-based application running on Amazon EC2 instances in multiple Availability \nZones. The web application will provide access to a repository of text documents totaling about 900 TB in \nsize. The company anticipates that the web application will experience periods of high demand. A solutions \narchitect must ensure that the storage component for the text documents can scale to meet the demand of the \napplication at all times. The company is concerned about the overall cost of the solution. \n \nWhich storage solution meets these requirements MOST cost-effectively? \n ","Option":["A. Amazon Elastic Block Store (Amazon EBS) ","B. Amazon Elastic File System (Amazon EFS) ","C. Amazon Elasticsearch Service (Amazon ES) ","D. Amazon S3 "],"Explanation":"Answer: D \n \nExplanation \nAmazon S3 is cheapest and can be accessed from anywhere. \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":5,"QuestionContent":" \nA company has a dynamic web application hosted on two Amazon EC2 instances. The company has its own \nSSL certificate, which is on each instance to perform SSL termination. \n \nThere has been an increase in traffic recently, and the operations team determined that SSL encryption and \n \n 71 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \ndecryption is causing the compute capacity of the web servers to reach their maximum limit. \nWhat should a solutions architect do to increase the application\u0027s performance? ","Option":["A. Create a new SSL certificate using AWS Certificate Manager (ACM) install the ACM certificate on \neach instance ","B. Create an Amazon S3 bucket Migrate the SSL certificate to the S3 bucket Configure the EC2 instances \nto reference the bucket for SSL termination ","C. Create another EC2 instance as a proxy server Migrate the SSL certificate to the new instance and \nconfigure it to direct connections to the existing EC2 instances ","D. Import the SSL certificate into AWS Certificate Manager (ACM) Create an Application Load Balancer \nwith an HTTPS listener that uses the SSL certificate from ACM "],"Explanation":"Answer: D \n \nExplanation \nhttps://aws.amazon.com/certificate-manager/: \n \n\u0022With AWS Certificate Manager, you can quickly request a certificate, deploy it on ACM-integrated AWS \nresources, such as Elastic Load Balancers, Amazon CloudFront distributions, and APIs on API Gateway, and \nlet AWS Certificate Manager handle certificate renewals. It also enables you to create private certificates for \nyour internal resources and manage the certificate lifecycle centrally.\u0022 \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":6,"QuestionContent":" \nA company wants to manage Amazon Machine Images (AMIs). The company currently copies AMIs to the \nsame AWS Region where the AMIs were created. The company needs to design an application that captures \nAWS API calls and sends alerts whenever the Amazon EC2 Createlmage API operation is called within the \ncompany\u0027s account. \n \nWhich solution will meet these requirements with the LEAST operational overhead? \n ","Option":["A. Create an AWS Lambda function to query AWS CloudTrail logs and to send an alert when a \nCreatelmage API call is detected. ","B. Configure AWS CloudTrail with an Amazon Simple Notification Service {Amazon SNS) notification \nthat occurs when updated logs are sent to Amazon S3. Use Amazon Athena to create a new table and to \nquery on Createlmage when an API call is detected. ","C. Create an Amazon EventBridge (Amazon CloudWatch Events) rule for the Createlmage API call. \nConfigure the target as an Amazon Simple Notification Service (Amazon SNS) topic to send an alert \nwhen a Createlmage API call is detected. ","D. Configure an Amazon Simple Queue Service (Amazon SQS) FIFO queue as a target for AWS \nCloudTrail logs. Create an AWS Lambda function to send an alert to an Amazon Simple Notification \nService (Amazon SNS) topic when a Createlmage API call is detected. \n 72 of 230  Practice Test Amazon Web Services - SAA-C03 "],"Explanation":"Answer: B \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":7,"QuestionContent":" \nA media company is evaluating the possibility ot moving rts systems to the AWS Cloud The company needs at \nleast 10 TB of storage with the maximum possible I/O performance for video processing. 300 TB of very \ndurable storage for storing media content, and 900 TB of storage to meet requirements for archival media that \nis not in use anymore \n \nWhich set of services should a solutions architect recommend to meet these requirements? \n ","Option":["A. Amazon EBS for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier \nfor archival storage ","B. Amazon EBS for maximum performance, Amazon EFS for durable data storage and Amazon S3 Glacier \nfor archival storage ","C. Amazon EC2 instance store for maximum performance. Amazon EFS for durable data storage and \nAmazon S3 for archival storage ","D. Amazon EC2 Instance store for maximum performance. Amazon S3 for durable data storage, and \nAmazon S3 Glacier for archival storage "],"Explanation":"Answer: A \n \nExplanation \nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":8,"QuestionContent":" \nA company wants to move its application to a serverless solution. The serverless solution needs to analyze \nexisting and new data by using SL. The company stores the data in an Amazon S3 bucket. The data requires \nencryption and must be replicated to a different AWS Region. \n \nWhich solution will meet these requirements with the LEAST operational overhead? \n ","Option":["A. Create a new S3 bucket. Load the data into the new S3 bucket. Use S3 Cross-Region Replication (CRR) \nto replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption with AWS \nKMS multi-Region kays (SSE-KMS). Use Amazon Athena to query the data. ","B. Create a new S3 bucket. Load the data into the new S3 bucket. Use S3 Cross-Region Replication (CRR) \nto replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption with AWS \nKMS multi-Region keys (SSE-KMS). Use Amazon RDS to query the data. ","C. Load the data into the existing S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate \nencrypted objects to an S3 bucket in another Region. Use server-side encryption with Amazon S3 \n 73 of 230  Practice Test Amazon Web Services - SAA-C03 \nmanaged encryption keys (SSE-S3). Use Amazon Athena to query the data. ","D. Load the data into the existing S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate \nencrypted objects to an S3 bucket in another Region. Use server-side encryption with Amazon S3 \nmanaged encryption keys (SSE-S3). Use Amazon RDS to query the data. "],"Explanation":"Answer: A \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":9,"QuestionContent":" \nA solutions architect needs to securely store a database user name and password that an application uses to \naccess an Amazon RDS DB instance. The application that accesses the database runs on an Amazon EC2 \ninstance. The solutions architect wants to create a secure parameter in AWS Systems Manager Parameter \nStore. \n \nWhat should the solutions architect do to meet this requirement? \n ","Option":["A. Create an IAM role that has read access to the Parameter Store parameter. Allow Decrypt access to an \nAWS Key Management Service (AWS KMS) key that is used to encrypt the parameter. Assign this \nIAM role to the EC2 instance. ","B. Create an IAM policy that allows read access to the Parameter Store parameter. Allow Decrypt access to \nan AWS Key Management Service (AWS KMS) key that is used to encrypt the parameter. Assign this \nIAM policy to the EC2 instance. ","C. Create an IAM trust relationship between the Parameter Store parameter and the EC2 instance. Specify \nAmazon RDS as a principal in the trust policy. ","D. Create an IAM trust relationship between the DB instance and the EC2 instance. Specify Systems \nManager as a principal in the trust policy. "],"Explanation":"Answer: B \n \nExplanation \nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/reference_aws-services-that-work-with-iam.html \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":10,"QuestionContent":" \nA company wants to run a gaming application on Amazon EC2 instances that are part of an Auto Scaling \ngroup in the AWS Cloud. The application will transmit data by using UDP packets. The company wants to \nensure that the application can scale out and in as traffic increases and decreases. \n \nWhat should a solutions architect do to meet these requirements? \n ","Option":["A. Attach a Network Load Balancer to the Auto Scaling group ","B. Attach an Application Load Balancer to the Auto Scaling group. \n 74 of 230  Practice Test Amazon Web Services - SAA-C03 ","C. Deploy an Amazon Route 53 record set with a weighted policy to route traffic appropriately ","D. Deploy a NAT instance that is configured with port forwarding to the EC2 instances in the Auto Scaling \ngroup. "],"Explanation":"Answer: B \n \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":11,"QuestionContent":" \nA reporting team receives files each day in an Amazon S3 bucket. The reporting team manually reviews and \ncopies the files from this initial S3 bucket to an analysis S3 bucket each day at the same time to use with \nAmazon QuickSight. Additional teams are starting to send more files in larger sizes to the initial S3 bucket. \n \nThe reporting team wants to move the files automatically analysis S3 bucket as the files enter the initial S3 \nbucket. The reporting team also wants to use AWS Lambda functions to run pattern-matching code on the \ncopied data. In addition, the reporting team wants to send the data files to a pipeline in Amazon SageMaker \nPipelines. \n \nWhat should a solutions architect do to meet these requirements with the LEAST operational overhead? \n ","Option":["A. Create a Lambda function to copy the files to the analysis S3 bucket. Create an S3 event notification for \nthe analysis S3 bucket. Configure Lambda and SageMaker Pipelines as destinations of the event \nnotification. Configure s30bjectCreated:Put as the event type. ","B. Create a Lambda function to copy the files to the analysis S3 bucket. Configure the analysis S3 bucket \nto send event notifications to Amazon EventBridge (Amazon CloudWatch Events). Configure an \nObjectCreated rule in EventBridge (CloudWatch Events). Configure Lambda and SageMaker Pipelines \nas targets for the rule. ","C. Configure S3 replication between the S3 buckets. Create an S3 event notification for the analysis S3 \nbucket. Configure Lambda and SageMaker Pipelines as destinations of the event notification. Configure \ns30bjectCreated:Put as the event type. ","D. Configure S3 replication between the S3 buckets. Configure the analysis S3 bucket to send event \nnotifications to Amazon EventBridge (Amazon CloudWatch Events). Configure an ObjectCreated rule \nin EventBridge (CloudWatch Events). Configure Lambda and SageMaker Pipelines as targets for the \nrule. "],"Explanation":"Answer: A \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":12,"QuestionContent":" \nA new employee has joined a company as a deployment engineer. The deployment engineer will be using \nAWS CloudFormation templates to create multiple AWS resources. A solutions architect wants the \ndeployment engineer to perform job activities while following the principle of least privilege. \n \nWhich steps should the solutions architect do in conjunction to reach this goal? (Select two.) \n \n 75 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n ","Option":["A. Have the deployment engineer use AWS account roof user credentials for performing AWS \nCloudFormation stack operations. ","B. Create a new IAM user for the deployment engineer and add the IAM user to a group that has the \nPowerUsers IAM policy attached. ","C. Create a new IAM user for the deployment engineer and add the IAM user to a group that has the \nAdministrate/Access IAM policy attached. ","D. Create a new IAM User for the deployment engineer and add the IAM user to a group that has an IAM \npolicy that allows AWS CloudFormation actions only. ","E. Create an IAM role for the deployment engineer to explicitly define the permissions specific to the \nAWS CloudFormation stack and launch stacks using Dial IAM role. "],"Explanation":"Answer: D E \n \nExplanation \nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html \nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_users.html \n \n","RightAnswer":["D","E"],"QuestionChoose":[]},{"QuestionNumber":13,"QuestionContent":" \nA gaming company hosts a browser-based application on AWS. The users of the application consume a large \nnumber of videos and images that are stored in Amazon S3. This content is the same for all users. \n \nThe application has increased in popularity, and millions of users worldwide are accessing these media files. \nThe company wants to provide the files to the users while reducing the load on the origin. \n \nWhich solution meets these requirements MOST cost-effectively? \n ","Option":["A. Deploy an AWS Global Accelerator accelerator in front of the web servers. ","B. Deploy an Amazon CloudFront web distribution in front of the S3 bucket. ","C. Deploy an Amazon ElastiCache for Redis instance in front of the web servers. ","D. Deploy an Amazon ElastiCache for Memcached instance in front of the web servers. "],"Explanation":"Answer: B \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":14,"QuestionContent":" \nA solutions architect is creating a new Amazon CloudFront distribution for an application. Some of the \ninformation submitted by users is sensitive. The application uses HTTPS but needs another layer of security. \nThe sensitive information should be protected throughout the entire application stack, and access to the \ninformation should be restricted to certain applications. \n \n 76 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \nWhich action should the solutions architect take? \n ","Option":["A. Configure a CloudFront signed URL. ","B. Configure a CloudFront signed cookie. ","C. Configure a CloudFront field-level encryption profile. ","D. Configure CloudFront and set the Origin Protocol Policy setting to HTTPS Only for the Viewer Protocol \nPolicy. "],"Explanation":"Answer: C \n \nExplanation \nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html \n \n\u0022With Amazon CloudFront, you can enforce secure end-to-end connections to origin servers by using HTTPS. \nField-level encryption adds an additional layer of security that lets you protect specific data throughout system \nprocessing so that only certain applications can see it.\u0022 \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":15,"QuestionContent":" \nA company recently started using Amazon Aurora as the data store for its global ecommerce application When \nlarge reports are run developers report that the ecommerce application is performing poorly After reviewing \nmetrics in Amazon CloudWatch, a solutions architect finds that the ReadlOPS and CPUUtilization metrics are \nspiking when monthly reports run. \n \nWhat is the MOST cost-effective solution? \n ","Option":["A. Migrate the monthly reporting to Amazon Redshift. ","B. Migrate the monthly reporting to an Aurora Replica ","C. Migrate the Aurora database to a larger instance class ","D. Increase the Provisioned IOPS on the Aurora instance "],"Explanation":"Answer: B \n \nExplanation \nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html \n \n#Aurora.Replication.Replicas Aurora Replicas have two main purposes. You can issue queries to them to scale \nthe read operations for your application. You typically do so by connecting to the reader endpoint of the \ncluster. That way, Aurora can spread the load for read-only connections across as many Aurora Replicas as \nyou have in the cluster. Aurora Replicas also help to increase availability. If the writer instance in a cluster \nbecomes unavailable, Aurora automatically promotes one of the reader instances to take its place as the new \nwriter. \n \n 77 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \n \nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.html \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":16,"QuestionContent":" \nA security team wants to limit access to specific services or actions in all of the team\u0027s AWS accounts. All \naccounts belong to a large organization in AWS Organizations. The solution must be scalable and there must \nbe a single point where permissions can be maintained. \n \nWhat should a solutions architect do to accomplish this? \n ","Option":["A. Create an ACL to provide access to the services or actions. ","B. Create a security group to allow accounts and attach it to user groups. ","C. Create cross-account roles in each account to deny access to the services or actions. ","D. Create a service control policy in the root organizational unit to deny access to the services or actions. "],"Explanation":"Answer: D \n \nExplanation \nService control policies (SCPs) are one type of policy that you can use to manage your organization. SCPs \noffer central control over the maximum available permissions for all accounts in your organization, allowing \nyou to ensure your accounts stay within your organization\u0027s access control guidelines. See \nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html. \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":17,"QuestionContent":"A company wants to migrate its existing on-premises monolithic application to AWS. \nThe company wants to keep as much of the front- end code and the backend code as possible. However, the \ncompany wants to break the application into smaller applications. A different team will manage each \napplication. The company needs a highly scalable solution that minimizes operational overhead. \n \nWhich solution will meet these requirements? \n ","Option":["A. Host the application on AWS Lambda Integrate the application with Amazon API Gateway. ","B. Host the application with AWS Amplify. Connect the application to an Amazon API Gateway API that \nis integrated with AWS Lambda. ","C. Host the application on Amazon EC2 instances. Set up an Application Load Balancer with EC2 \ninstances in an Auto Scaling group as targets. ","D. Host the application on Amazon Elastic Container Service (Amazon ECS) Set up an Application Load \nBalancer with Amazon ECS as the target. \n 78 of 230  Practice Test Amazon Web Services - SAA-C03 "],"Explanation":"Answer: D \n \nExplanation \nhttps://aws.amazon.com/blogs/compute/microservice-delivery-with-amazon-ecs-and-application-load-balancers/ \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":18,"QuestionContent":" \nA company is migrating an application from on-premises servers to Amazon EC2 instances. As part of the \nmigration design requirements, a solutions architect must implement infrastructure metric alarms. The \ncompany does not need to take action if CPU utilization increases to more than 50% for a short burst of time. \nHowever, if the CPU utilization increases to more than 50% and read IOPS on the disk are high at the same \ntime, the company needs to act as soon as possible. The solutions architect also must reduce false alarms. \n \nWhat should the solutions architect do to meet these requirements? \n ","Option":["A. Create Amazon CloudWatch composite alarms where possible. ","B. Create Amazon CloudWatch dashboards to visualize the metrics and react to issues quickly. ","C. Create Amazon CloudWatch Synthetics canaries to monitor the application and raise an alarm. ","D. Create single Amazon CloudWatch metric alarms with multiple metric thresholds where possible. "],"Explanation":"Answer: A \n \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":19,"QuestionContent":" \nA company runs an application using Amazon ECS. The application creates esi/ed versions of an original \nimage and then makes Amazon S3 API calls to store the resized images in Amazon S3. \n \nHow can a solutions architect ensure that the application has permission to access Amazon S3? \n ","Option":["A. Update the S3 role in AWS IAM to allow read/write access from Amazon ECS, and then relaunch the \ncontainer. ","B. Create an IAM role with S3 permissions, and then specify that role as the taskRoleAm in the task \ndefinition. ","C. Create a security group that allows access from Amazon ECS to Amazon S3, and update the launch \nconfiguration used by the ECS cluster. ","D. Create an IAM user with S3 permissions, and then relaunch the Amazon EC2 instances for the ECS \ncluster while logged in as this account. "],"Explanation":"Answer: B \n \n 79 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":20,"QuestionContent":" \nA solutions architect needs to help a company optimize the cost of running an application on AWS. The \napplication will use Amazon EC2 instances, AWS Fargate, and AWS Lambda for compute within the \narchitecture. \n \nThe EC2 instances will run the data ingestion layer of the application. EC2 usage will be sporadic and \nunpredictable. Workloads that run on EC2 instances can be interrupted at any time. The application front end \nwill run on Fargate, and Lambda will serve the API layer. The front-end utilization and API layer utilization \nwill be predictable over the course of the next year. \n \nWhich combination of purchasing options will provide the MOST cost-effective solution for hosting this \napplication? (Choose two.) \n ","Option":["A. Use Spot Instances for the data ingestion layer ","B. Use On-Demand Instances for the data ingestion layer ","C. Purchase a 1-year Compute Savings Plan for the front end and API layer. ","D. Purchase 1-year All Upfront Reserved instances for the data ingestion layer. ","E. Purchase a 1-year EC2 instance Savings Plan for the front end and API layer. "],"Explanation":"Answer: A C \n \n \n","RightAnswer":["A","C"],"QuestionChoose":[]},{"QuestionNumber":21,"QuestionContent":" \nA solutions architect must design a solution that uses Amazon CloudFront with an Amazon S3 origin to store a \nstatic website. The company\u0027s security policy requires that all website traffic be inspected by AWS WAR \n \nHow should the solutions architect comply with these requirements? \n ","Option":["A. Configure an S3 bucket policy lo accept requests coming from the AWS WAF Amazon Resource Name \n(ARN) only. ","B. Configure Amazon CloudFront to forward all incoming requests to AWS WAF before requesting \ncontent from the S3 origin. ","C. Configure a security group that allows Amazon CloudFront IP addresses to access Amazon S3 only. \nAssociate AWS WAF to CloudFront. ","D. Configure Amazon CloudFront and Amazon S3 to use an origin access identity (OAI) to restrict access \nto the S3 bucket. Enable AWS WAF on the distribution. "],"Explanation":"Answer: D \n \nExplanation \n \n 80 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3 \nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-awswaf.html \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":22,"QuestionContent":" \nA company needs to move data from an Amazon EC2 instance to an Amazon S3 bucket. The company must \nensure that no API calls and no data are routed through public internet routes. Only the EC2 instance can have \naccess to upload data to the S3 bucket. \n \nWhich solution will meet these requirements? \n ","Option":["A. Create an interface VPC endpoint for Amazon S3 in the subnet where the EC2 instance is located. \nAttach a resource policy to the S3 bucket to only allow the EC2 instance\u0027s IAM role for access. ","B. Create a gateway VPC endpoint for Amazon S3 in the Availability Zone where the EC2 instance is \nlocated. Attach appropriate security groups to the endpoint. Attach a resource policy lo the S3 bucket to \nonly allow the EC2 instance\u0027s IAM role for access. ","C. Run the nslookup tool from inside the EC2 instance to obtain the private IP address of the S3 bucket\u0027s \nservice API endpoint. Create a route in the VPC route table to provide the EC2 instance with access to \nthe S3 bucket. Attach a resource policy to the S3 bucket to only allow the EC2 instance\u0027s IAM role for \naccess. ","D. Use the AWS provided, publicly available ip-ranges.json tile to obtain the private IP address of the S3 \nbucket\u0027s service API endpoint. Create a route in the VPC route table to provide the EC2 instance with \naccess to the S3 bucket. Attach a resource policy to the S3 bucket to only allow the EC2 instance\u0027s IAM \nrole for access. "],"Explanation":"Answer: A \n \nExplanation \n(https://aws.amazon.com/blogs/security/how-to-restrict-amazon-s3-bucket-access-to-a-specific-iam-role/) \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":23,"QuestionContent":" \nA gaming company is designing a highly available architecture. The application runs on a modified Linux \nkernel and supports only UDP-based traffic. The company needs the front-end tier to provide the best possible \nuser experience. That tier must have low latency, route traffic to the nearest edge location, and provide static \nIP addresses for entry into the application endpoints. \n \nWhat should a solutions architect do to meet these requirements? \n ","Option":["A. Configure Amazon Route 53 to forward requests to an Application Load Balancer. Use AWS Lambda \nfor the application in AWS Application Auto Scaling. ","B. Configure Amazon CloudFront to forward requests to a Network Load Balancer. Use AWS Lambda for \nthe application in an AWS Application Auto Scaling group. \n 81 of 230  Practice Test Amazon Web Services - SAA-C03 ","C. Configure AWS Global Accelerator to forward requests to a Network Load Balancer. Use Amazon EC2 \ninstances for the application in an EC2 Auto Scaling group. ","D. Configure Amazon API Gateway to forward requests to an Application Load Balancer. Use Amazon \nEC2 instances for the application in an EC2 Auto Scaling group. "],"Explanation":"Answer: C \n \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":24,"QuestionContent":" \nA company runs its two-tier ecommerce website on AWS. The web tier consists of a load balancer that sends \ntraffic to Amazon EC2 instances. The database tier uses an Amazon RDS DB instance. The EC2 instances and \nthe RDS DB instance should not be exposed to the public internet. The EC2 instances require internet access \nto complete payment processing of orders through a third-party web service. The application must be highly \navailable. \n \nWhich combination of configuration options will meet these requirements? (Choose two.) \n ","Option":["A. Use an Auto Scaling group to launch the EC2 instances in private subnets. Deploy an RDS Multi-AZ \nDB instance in private subnets. ","B. Configure a VPC with two private subnets and two NAT gateways across two Availability Zones. \nDeploy an Application Load Balancer in the private subnets. ","C. Use an Auto Scaling group to launch the EC2 instances in public subnets across two Availability Zones. \nDeploy an RDS Multi-AZ DB instance in private subnets. ","D. Configure a VPC with one public subnet, one private subnet, and two NAT gateways across two \nAvailability Zones. Deploy an Application Load Balancer in the public subnet. ","E. Configure a VPC with two public subnets, two private subnets, and two NAT gateways across two \nAvailability Zones. Deploy an Application Load Balancer in the public subnets. "],"Explanation":"Answer: A E \nExplanation \nBefore you begin: Decide which two Availability Zones you will use for your EC2 instances. Configure your \nvirtual private cloud (VPC) with at least one public subnet in each of these Availability Zones. These public \nsubnets are used to configure the load balancer. You can launch your EC2 instances in other subnets of these \nAvailability Zones instead. \n \n","RightAnswer":["A","E"],"QuestionChoose":[]},{"QuestionNumber":25,"QuestionContent":" \nA company stores its application logs in an Amazon CloudWatch Logs log group. A new policy requires the \ncompany to store all application logs in Amazon OpenSearch Service (Amazon Elasticsearch Service) in \nnear-real time. \n \n 82 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \nWhich solution will meet this requirement with the LEAST operational overhead? \n ","Option":["A. Configure a CloudWatch Logs subscription to stream the logs to Amazon OpenSearch Service (Amazon \nElasticsearch Service). ","B. Create an AWS Lambda function. Use the log group to invoke the function to write the logs to Amazon \nOpenSearch Service (Amazon Elasticsearch Service). ","C. Create an Amazon Kinesis Data Firehose delivery stream. Configure the log group as the delivery \nstream\u0027s source. Configure Amazon OpenSearch Service (Amazon Elasticsearch Service) as the delivery \nstream\u0027s destination. ","D. Install and configure Amazon Kinesis Agent on each application server to deliver the logs to Amazon \nKinesis Data Streams. Configure Kinesis Data Streams to deliver the logs to Amazon OpenSearch \nService (Amazon Elasticsearch Service) "],"Explanation":"Answer: B \n \nExplanation \nhttps://computingforgeeks.com/stream-logs-in-aws-from-cloudwatch-to-elasticsearch/ \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":26,"QuestionContent":" \nAn application runs on Amazon EC2 instances across multiple Availability Zones The instances run in an \nAmazon EC2 Auto Scaling group behind an Application Load Balancer The application performs best when \nthe CPU utilization of the EC2 instances is at or near 40%. \n \nWhat should a solutions architect do to maintain the desired performance across all instances in the group? \n ","Option":["A. Use a simple scaling policy to dynamically scale the Auto Scaling group ","B. Use a target tracking policy to dynamically scale the Auto Scaling group ","C. Use an AWS Lambda function to update the desired Auto Scaling group capacity. ","D. Use scheduled scaling actions to scale up and scale down the Auto Scaling group "],"Explanation":"Answer: B \n \nExplanation \nhttps://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-target-tracking.html \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":27,"QuestionContent":" \nA company runs a high performance computing (HPC) workload on AWS. The workload required low-latency \nnetwork performance and high network throughput with tightly coupled node-to-node communication. The \n \n 83 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \nAmazon EC2 instances are properly sized for compute and storage capacity, and are launched using default \noptions. \n \nWhat should a solutions architect propose to improve the performance of the workload? \n ","Option":["A. Choose a cluster placement group while launching Amazon EC2 instances. ","B. Choose dedicated instance tenancy while launching Amazon EC2 instances. ","C. Choose an Elastic Inference accelerator while launching Amazon EC2 instances. ","D. Choose the required capacity reservation while launching Amazon EC2 instances. "],"Explanation":"Answer: A \n \nExplanation \nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-ec2-placementgroup.html \n \n\u0022A cluster placement group is a logical grouping of instances within a single Availability Zone that benefit \nfrom low network latency, high network throughput\u0022 \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":28,"QuestionContent":" \nA company uses a popular content management system (CMS) for its corporate website. However, the \nrequired patching and maintenance are burdensome. The company is redesigning its website and wants anew \nsolution. The website will be updated four times a year and does not need to have any dynamic content \navailable. The solution must provide high scalability and enhanced security. \n \nWhich combination of changes will meet these requirements with the LEAST operational overhead? (Choose \ntwo.) \n ","Option":["A. Deploy an AWS WAF web ACL in front of the website to provide HTTPS functionality ","B. Create and deploy an AWS Lambda function to manage and serve the website content ","C. Create the new website and an Amazon S3 bucket Deploy the website on the S3 bucket with static \nwebsite hosting enabled ","D. Create the new website. Deploy the website by using an Auto Scaling group of Amazon EC2 instances \nbehind an Application Load Balancer. "],"Explanation":"Answer: A D \n \n \n","RightAnswer":["A","D"],"QuestionChoose":[]},{"QuestionNumber":29,"QuestionContent":" \nA company is migrating its on-premises PostgreSQL database to Amazon Aurora PostgreSQL. The \non-premises database must remain online and accessible during the migration. The Aurora database must \n \n 84 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \nremain synchronized with the on-premises database. \n \nWhich combination of actions must a solutions architect take to meet these requirements? (Choose two.) \n ","Option":["A. Create an ongoing replication task. ","B. Create a database backup of the on-premises database ","C. Create an AWS Database Migration Service (AWS DMS) replication server ","D. Convert the database schema by using the AWS Schema Conversion Tool (AWS SCT). ","E. Create an Amazon EventBridge (Amazon CloudWatch Events) rule to monitor the database \nsynchronization "],"Explanation":"Answer: C D \n \n","RightAnswer":["C","D"],"QuestionChoose":[]},{"QuestionNumber":30,"QuestionContent":" \nA company is running several business applications in three separate VPCs within me us-east-1 Region. The \napplications must be able to communicate between VPCs. The applications also must be able to consistently \nsend hundreds to gigabytes of data each day to a latency-sensitive application that runs in a single on-premises \ndata center. \n \nA solutions architect needs to design a network connectivity solution that maximizes cost-effectiveness \nWhich solution moots those requirements? ","Option":["A. Configure three AWS Site-to-Site VPN connections from the data center to AWS Establish connectivity \nby configuring one VPN connection for each VPC ","B. Launch a third-party virtual network appliance in each VPC Establish an iPsec VPN tunnel between the \nData center and each virtual appliance ","C. Set up three AWS Direct Connect connections from the data center to a Direct Connect gateway in \nus-east-1 Establish connectivity by configuring each VPC to use one of the Direct Connect connections ","D. Set up one AWS Direct Connect connection from the data center to AWS. Create a transit gateway, and \nattach each VPC to the transit gateway. Establish connectivity between the Direct Connect connection \nand the transit gateway. "],"Explanation":"Answer: D \n \nExplanation \nhttps://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-aws-transit-g \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":31,"QuestionContent":" \n 85 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \nA company is planning to move its data to an Amazon S3 bucket. The data must be encrypted when it is stored \nin the S3 bucket. Additionally, the encryption key must be automatically rotated every year. \n \nWhich solution will meet these requirements with the LEAST operational overhead? \n ","Option":["A. Move the data to the S3 bucket. Use server-side encryption with Amazon S3 managed encryption keys \n(SSE-S3). Use the built-in key rotation behavior of SSE-S3 encryption keys. ","B. Create an AWS Key Management Service {AWS KMS) customer managed key. Enable automatic key \nrotation. Set the S3 bucket\u0027s default encryption behavior to use the customer managed KMS key. Move \nthe data to the S3 bucket. ","C. Create an AWS Key Management Service (AWS KMS) customer managed key. Set the S3 bucket\u0027s \ndefault encryption behavior to use the customer managed KMS key. Move the data to the S3 bucket. \nManually rotate the KMS key every year. ","D. Encrypt the data with customer key material before moving the data to the S3 bucket. Create an AWS \nKey Management Service (AWS KMS) key without key material. Import the customer key material into \nthe KMS key. Enable automatic key rotation. "],"Explanation":"Answer: C \n \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":32,"QuestionContent":" \nA company runs a web-based portal that provides users with global breaking news, local alerts, and weather \nupdates. The portal delivers each user a personalized view by using mixture of static and dynamic content. \nContent is served over HTTPS through an API server running on an Amazon EC2 instance behind an \nApplication Load Balancer (ALB). The company wants the portal to provide this content to its users across the \nworld as quickly as possible. \n \nHow should a solutions architect design the application to ensure the LEAST amount of latency for all users? \n ","Option":["A. Deploy the application stack in a single AWS Region. Use Amazon CloudFront to serve all static and \ndynamic content by specifying the ALB as an origin. ","B. Deploy the application stack in two AWS Regions. Use an Amazon Route 53 latency routing policy to \nserve all content from the ALB in the closest Region. ","C. Deploy the application stack in a single AWS Region. Use Amazon CloudFront to serve the static \ncontent. Serve the dynamic content directly from the ALB. ","D. Deploy the application stack in two AWS Regions. Use an Amazon Route 53 geolocation routing policy \nto serve all content from the ALB in the closest Region. "],"Explanation":"Answer: A \n \nExplanation \nhttps://aws.amazon.com/blogs/networking-and-content-delivery/deliver-your-apps-dynamic-content-using-amaz \n \n 86 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \n \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":33,"QuestionContent":" \nA company has a legacy data processing application that runs on Amazon EC2 instances. Data is processed \nsequentially, but the order of results does not matter. The application uses a monolithic architecture. The only \nway that the company can scale the application to meet increased demand is to increase the size of the \ninstances. \n \nThe company\u0027s developers have decided to rewrite the application to use a microservices architecture on \nAmazon Elastic Container Service (Amazon ECS). \n \nWhat should a solutions architect recommend for communication between the microservices? \n ","Option":["A. Create an Amazon Simple Queue Service (Amazon SQS) queue. Add code to the data producers, and \nsend data to the queue. Add code to the data consumers to process data from the queue. ","B. Create an Amazon Simple Notification Service (Amazon SNS) topic. Add code to the data producers, \nand publish notifications to the topic. Add code to the data consumers to subscribe to the topic. ","C. Create an AWS Lambda function to pass messages. Add code to the data producers to call the Lambda \nfunction with a data object. Add code to the data consumers to receive a data object that is passed from \nthe Lambda function. ","D. Create an Amazon DynamoDB table. Enable DynamoDB Streams. Add code to the data producers to \ninsert data into the table. Add code to the data consumers to use the DynamoDB Streams API to detect \nnew table entries and retrieve the data. "],"Explanation":"Answer: A \n \nExplanation \nQueue has Limited throughput (300 msg/s without batching, 3000 msg/s with batching whereby up-to 10 msg \nper batch operation; Msg duplicates not allowed in the queue (exactly-once delivery); Msg order is preserved \n(FIFO); Queue name must end with .fifo \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":34,"QuestionContent":" \nA company\u0027s web application is running on Amazon EC2 instances behind an Application Load Balancer. The \ncompany recently changed its policy, which now requires the application to be accessed from one specific \ncountry only. \n \nWhich configuration will meet this requirement? \n ","Option":["A. Configure the security group for the EC2 instances. ","B. Configure the security group on the Application Load Balancer. ","C. Configure AWS WAF on the Application Load Balancer in a VPC. \n 87 of 230  Practice Test Amazon Web Services - SAA-C03 ","D. Configure the network ACL for the subnet that contains the EC2 instances. "],"Explanation":"Answer: C \n \nExplanation \nhttps://aws.amazon.com/about-aws/whats-new/2017/10/aws-waf-now-supports-geographic-match/ \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":35,"QuestionContent":" \nA company\u2019s website provides users with downloadable historical performance reports. The website needs a \nsolution that will scale to meet the company\u2019s website demands globally. The solution should be \ncost-effective, limit the provisioning of infrastructure resources, and provide the fastest possible response time. \nWhich combination should a solutions architect recommend to meet these requirements? ","Option":["A. Amazon CloudFront and Amazon S3 ","B. AWS Lambda and Amazon DynamoDB ","C. Application Load Balancer with Amazon EC2 Auto Scaling ","D. Amazon Route 53 with internal Application Load Balancers "],"Explanation":"Answer: A \n \nExplanation \nCloudfront for rapid response and s3 to minimize infrastructure. \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":36,"QuestionContent":"A company runs workloads on AWS. The company needs to connect to a service from an external provider. \nThe service is hosted in the provider\u0027s VPC. According to the company\u2019s security team, the connectivity must \nbe private and must be restricted to the target service. The connection must be initiated only from the \ncompany\u2019s VPC. \n \nWhich solution will mast these requirements? \n ","Option":["A. Create a VPC peering connection between the company\u0027s VPC and the provider\u0027s VPC. Update the route \ntable to connect to the target service. ","B. Ask the provider to create a virtual private gateway in its VPC. Use AWS PrivateLink to connect to the \ntarget service. ","C. Create a NAT gateway in a public subnet of the company\u0027s VPC. Update the route table to connect to \nthe target service. ","D. Ask the provider to create a VPC endpoint for the target service. Use AWS PrivateLink to connect to the \n 88 of 230  Practice Test Amazon Web Services - SAA-C03 \ntarget service. "],"Explanation":"Answer: D \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":37,"QuestionContent":" \nA gaming company has a web application that displays scores. The application runs on Amazon EC2 instances \nbehind an Application Load Balancer. The application stores data in an Amazon RDS for MySQL database. \nUsers are starting to experience long delays and interruptions that are caused by database read performance. \nThe company wants to improve the user experience while minimizing changes to the application\u0027s architecture. \n \nWhat should a solutions architect do to meet these requirements? \n ","Option":["A. Use Amazon ElastiCache in front of the database. ","B. Use RDS Proxy between the application and the database. ","C. Migrate the application from EC2 instances to AWS Lambda. ","D. Migrate the database from Amazon RDS for MySQL to Amazon DynamoDB. "],"Explanation":"Answer: A \n \nExplanation \nElastiCache can help speed up the read performance of the database by caching frequently accessed data, \nreducing latency and allowing the application to access the data more quickly. This solution requires minimal \nmodifications to the current architecture, as ElastiCache can be used in conjunction with the existing Amazon \nRDS for MySQL database. \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":38,"QuestionContent":" \nA company is running a multi-tier web application on premises. The web application is containerized and runs \non a number of Linux hosts connected to a PostgreSQL database that contains user records. The operational \noverhead of maintaining the infrastructure and capacity planning is limiting the company\u0027s growth. A solutions \narchitect must improve the application\u0027s infrastructure. \n \nWhich combination of actions should the solutions architect take to accomplish this? (Choose two.) \n ","Option":["A. Migrate the PostgreSQL database to Amazon Aurora ","B. Migrate the web application to be hosted on Amazon EC2 instances. ","C. Set up an Amazon CloudFront distribution for the web application content. ","D. Set up Amazon ElastiCache between the web application and the PostgreSQL database. ","E. Migrate the web application to be hosted on AWS Fargate with Amazon Elastic Container Service \n(Amazon ECS). \n 89 of 230  Practice Test Amazon Web Services - SAA-C03 "],"Explanation":"Answer: A E \n \n \n","RightAnswer":["A","E"],"QuestionChoose":[]},{"QuestionNumber":39,"QuestionContent":" \nAn online retail company has more than 50 million active customers and receives more than 25,000 orders \neach day. The company collects purchase data for customers and stores this data in Amazon S3. Additional \ncustomer data is stored in Amazon RDS. \n \nThe company wants to make all the data available to various teams so that the teams can perform analytics. \nThe solution must provide the ability to manage fine-grained permissions for the data and must minimize \noperational overhead. \n \nWhich solution will meet these requirements? \n ","Option":["A. Migrate the purchase data to write directly to Amazon RDS. Use RDS access controls to limit access. ","B. Schedule an AWS Lambda function to periodically copy data from Amazon RDS to Amazon S3. Create \nan AWS Glue crawler. Use Amazon Athena to query the data. Use S3 policies to limit access. ","C. Create a data lake by using AWS Lake Formation. Create an AWS Glue JDBC connection to Amazon \nRDS. Register (he S3 bucket in Lake Formation. Use Lake Formation access controls to limit access. ","D. Create an Amazon Redshift cluster. Schedule an AWS Lambda function to periodically copy data from \nAmazon S3 and Amazon RDS to Amazon Redshift. Use Amazon Redshift access controls to limit \naccess. "],"Explanation":"Answer: D \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":40,"QuestionContent":" \nA global company is using Amazon API Gateway to design REST APIs for its loyalty club users in the \nus-east-1 Region and the ap-southeast-2 Region. A solutions architect must design a solution to protect these \nAPI Gateway managed REST APIs across multiple accounts from SQL injection and cross-site scripting \nattacks. \n \nWhich solution will meet these requirements with the LEAST amount of administrative effort? \n ","Option":["A. Set up AWS WAF in both Regions. Associate Regional web ACLs with an API stage. ","B. Set up AWS Firewall Manager in both Regions. Centrally configure AWS WAF rules. ","C. Set up AWS Shield in bath Regions. Associate Regional web ACLs with an API stage. ","D. Set up AWS Shield in one of the Regions. Associate Regional web ACLs with an API stage. "],"Explanation":"Answer: A \n \n 90 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \nExplanation \nUsing AWS WAF has several benefits. Additional protection against web attacks using criteria that you \nspecify. You can define criteria using characteristics of web requests such as the following: Presence of SQL \ncode that is likely to be malicious (known as SQL injection). Presence of a script that is likely to be malicious \n(known as cross-site scripting). AWS Firewall Manager simplifies your administration and maintenance tasks \nacross multiple accounts and resources for a variety of protections. \nhttps://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":41,"QuestionContent":" \nA company has an event-driven application that invokes AWS Lambda functions up to 800 times each minute \nwith varying runtimes. The Lambda functions access data that is stored in an Amazon Aurora MySQL OB \ncluster. The company is noticing connection timeouts as user activity increases The database shows no signs of \nbeing overloaded. CPU. memory, and disk access metrics are all low. \n \nWhich solution will resolve this issue with the LEAST operational overhead? \n ","Option":["A. Adjust the size of the Aurora MySQL nodes to handle more connections. Configure retry logic in the \nLambda functions for attempts to connect to the database ","B. Set up Amazon ElastiCache tor Redls to cache commonly read items from the database. Configure the \nLambda functions to connect to ElastiCache for reads. ","C. Add an Aurora Replica as a reader node. Configure the Lambda functions to connect to the reader \nendpoint of the OB cluster rather than lo the writer endpoint. ","D. Use Amazon ROS Proxy to create a proxy. Set the DB cluster as the target database Configure the \nLambda functions lo connect to the proxy rather than to the DB cluster. "],"Explanation":"Answer: D \n \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":42,"QuestionContent":" \nA business\u0027s backup data totals 700 terabytes (TB) and is kept in network attached storage (NAS) at its data \ncenter. This backup data must be available in the event of occasional regulatory inquiries and preserved for a \nperiod of seven years. The organization has chosen to relocate its backup data from its on-premises data center \nto Amazon Web Services (AWS). Within one month, the migration must be completed. The company\u0027s public \ninternet connection provides 500 Mbps of dedicated capacity for data transport. \n \nWhat should a solutions architect do to ensure that data is migrated and stored at the LOWEST possible cost? \n ","Option":["A. Order AWS Snowball devices to transfer the data. Use a lifecycle policy to transition the files to \nAmazon S3 Glacier Deep Archive. ","B. Deploy a VPN connection between the data center and Amazon VPC. Use the AWS CLI to copy the \ndata from on premises to Amazon S3 Glacier. \n 91 of 230  Practice Test Amazon Web Services - SAA-C03 ","C. Provision a 500 Mbps AWS Direct Connect connection and transfer the data to Amazon S3. Use a \nlifecycle policy to transition the files to Amazon S3 Glacier Deep Archive. ","D. Use AWS DataSync to transfer the data and deploy a DataSync agent on premises. Use the DataSync \ntask to copy files from the on-premises NAS storage to Amazon S3 Glacier. "],"Explanation":"Answer: A \n \nExplanation \nhttps://www.omnicalculator.com/other/data-transfer \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":43,"QuestionContent":" \nA company wants to measure the effectiveness of its recent marketing campaigns. The company performs \nbatch processing on csv files of sales data and stores the results \u00ABi an Amazon S3 bucket once every hour. The \nS3 bi petabytes of objects. The company runs one-time queries in Amazon Athena to determine which \nproducts are most popular on a particular date for a particular region Queries sometimes fail or take longer \nthan expected to finish. \n \nWhich actions should a solutions architect take to improve the query performance and reliability? (Select \nTWO.) \n ","Option":["A. Reduce the S3 object sizes to less than 126 MB ","B. Partition the data by date and region n Amazon S3 ","C. Store the files as large, single objects in Amazon S3. ","D. Use Amazon Kinosis Data Analytics to run the Queries as pan of the batch processing operation ","E. Use an AWS duo extract, transform, and load (ETL) process to convert the csv files into Apache Parquet \nformat. "],"Explanation":"Answer: C E \n \n \n","RightAnswer":["C","E"],"QuestionChoose":[]},{"QuestionNumber":44,"QuestionContent":" \nA company is planning to build a high performance computing (HPC) workload as a service solution that Is \nhosted on AWS A group of 16 AmazonEC2Ltnux Instances requires the lowest possible latency for \nnode-to-node communication. The instances also need a shared block device volume for high-performing \nstorage. \n \nWhich solution will meet these requirements? \n ","Option":["A. Use a duster placement group. Attach a single Provisioned IOPS SSD Amazon Elastic Block Store \n(Amazon E BS) volume to all the instances by using Amazon EBS Multi-Attach \n 92 of 230  Practice Test Amazon Web Services - SAA-C03 ","B. Use a cluster placement group. Create shared \u0027lie systems across the instances by using Amazon Elastic \nFile System (Amazon EFS) ","C. Use a partition placement group. Create shared tile systems across the instances by using Amazon \nElastic File System (Amazon EFS). ","D. Use a spread placement group. Attach a single Provisioned IOPS SSD Amazon Elastic Block Store \n(Amazon EBS) volume to all the instances by using Amazon EBS Multi-Attach "],"Explanation":"Answer: A \n \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":45,"QuestionContent":" \nA solutions architect is designing a customer-facing application for a company. The application\u0027s database will \nhave a clearly defined access pattern throughout the year and will have a variable number of reads and writes \nthat depend on the time of year. The company must retain audit records for the database for 7 days. The \nrecovery point objective (RPO) must be less than 5 hours. \n \nWhich solution meets these requirements? \n ","Option":["A. Use Amazon DynamoDB with auto scaling Use on-demand backups and Amazon DynamoDB Streams ","B. Use Amazon Redshift. Configure concurrency scaling. Activate audit logging. Perform database \nsnapshots every 4 hours. ","C. Use Amazon RDS with Provisioned IOPS Activate the database auditing parameter Perform database \nsnapshots every 5 hours ","D. Use Amazon Aurora MySQL with auto scaling. Activate the database auditing parameter "],"Explanation":"Answer: B \n \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":46,"QuestionContent":" \nA company produces batch data that comes from different databases. The company also produces live stream \ndata from network sensors and application APIs. The company needs to consolidate all the data into one place \nfor business analytics. The company needs to process the incoming data and then stage the data in different \nAmazon S3 buckets. Teams will later run one-time queries and import the data into a business intelligence tool \nto show key performance indicators (KPIs). \n \nWhich combination of steps will meet these requirements with the LEAST operational overhead? (Choose \ntwo.) \n ","Option":["A. Use Amazon Athena foe one-time queries Use Amazon QuickSight to create dashboards for KPIs ","B. Use Amazon Kinesis Data Analytics for one-time queries Use Amazon QuickSight to create dashboards \nfor KPIs \n 93 of 230  Practice Test Amazon Web Services - SAA-C03 ","C. Create custom AWS Lambda functions to move the individual records from me databases to an Amazon \nRedshift duster ","D. Use an AWS Glue extract transform, and toad (ETL) job to convert the data into JSON format Load the \ndata into multiple Amazon OpenSearch Service (Amazon Elasticsearch Service) dusters ","E. Use blueprints in AWS Lake Formation to identify the data that can be ingested into a data lake Use \nAWS Glue to crawl the source extract the data and load the data into Amazon S3 in Apache Parquet \nformat "],"Explanation":"Answer: C E \n \n \n","RightAnswer":["C","E"],"QuestionChoose":[]},{"QuestionNumber":47,"QuestionContent":" \nAn ecommerce company has an order-processing application that uses Amazon API Gateway and an AWS \nLambda function. The application stores data in an Amazon Aurora PostgreSQL database. During a recent \nsales event, a sudden surge in customer orders occurred. Some customers experienced timeouts and the \napplication did not process the orders of those customers A solutions architect determined that the CPU \nutilization and memory utilization were high on the database because of a large number of open connections \nThe solutions architect needs to prevent the timeout errors while making the least possible changes to the \napplication. \n \nWhich solution will meet these requirements? \n ","Option":["A. Configure provisioned concurrency for the Lambda function Modify the database to be a global \ndatabase in multiple AWS Regions ","B. Use Amazon RDS Proxy to create a proxy for the database Modify the Lambda function to use the RDS \nProxy endpoint instead of the database endpoint ","C. Create a read replica for the database in a different AWS Region Use query string parameters in API \nGateway to route traffic to the read replica ","D. Migrate the data from Aurora PostgreSQL to Amazon DynamoDB by using AWS Database Migration \nService (AWS DMS| Modify the Lambda function to use the OynamoDB table "],"Explanation":"Answer: D \n \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":48,"QuestionContent":" \nA company is developing a file-sharing application that will use an Amazon S3 bucket for storage. The \ncompany wants to serve all the files through an Amazon CloudFront distribution. The company does not want \nthe files to be accessible through direct navigation to the S3 URL. \n \nWhat should a solutions architect do to meet these requirements? \n \n 94 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n ","Option":["A. Write individual policies for each S3 bucket to grant read permission for only CloudFront access. ","B. Create an IAM user. Grant the user read permission to objects in the S3 bucket. Assign the user to \nCloudFront. ","C. Write an S3 bucket policy that assigns the CloudFront distribution ID as the Principal and assigns the \ntarget S3 bucket as the Amazon Resource Name (ARN). ","D. Create an origin access identity (OAI). Assign the OAI to the CloudFront distribution. Configure the S3 \nbucket permissions so that only the OAI has read permission. "],"Explanation":"Answer: D \n \nExplanation \nhttps://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-access-to-amazon-s3/ \nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3 \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":49,"QuestionContent":" \nA company runs its ecommerce application on AWS. Every new order is published as a message in a \nRabbitMQ queue that runs on an Amazon EC2 instance in a single Availability Zone. These messages are \nprocessed by a different application that runs on a separate EC2 instance. This application stores the details in \na PostgreSQL database on another EC2 instance. All the EC2 instances are in the same Availability Zone. \n \nThe company needs to redesign its architecture to provide the highest availability with the least operational \noverhead. \n \nWhat should a solutions architect do to meet these requirements? \n ","Option":["A. Migrate the queue to a redundant pair (active/standby) of RabbitMQ instances on Amazon MQ. Create a \nMulti-AZ Auto Scaling group (or EC2 instances that host the application. Create another Multi-AZ \nAuto Scaling group for EC2 instances that host the PostgreSQL database. ","B. Migrate the queue to a redundant pair (active/standby) of RabbitMQ instances on Amazon MQ. Create a \nMulti-AZ Auto Scaling group for EC2 instances that host the application. Migrate the database to run on \na Multi-AZ deployment of Amazon RDS for PostgreSQL. ","C. Create a Multi-AZ Auto Scaling group for EC2 instances that host the RabbitMQ queue. Create another \nMulti-AZ Auto Scaling group for EC2 instances that host the application. Migrate the database to run \non a Multi-AZ deployment of Amazon RDS fqjPostgreSQL. ","D. Create a Multi-AZ Auto Scaling group for EC2 instances that host the RabbitMQ queue. Create another \nMulti-AZ Auto Scaling group for EC2 instances that host the application. Create a third Multi-AZ Auto \nScaling group for EC2 instances that host the PostgreSQL database. \n 95 of 230  Practice Test Amazon Web Services - SAA-C03 "],"Explanation":"Answer: B \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":50,"QuestionContent":" \nA company wants to use the AWS Cloud to make an existing application highly available and resilient. The \ncurrent version of the application resides in the company\u0027s data center. The application recently experienced \ndata loss after a database server crashed because of an unexpected power outage. \n \nThe company needs a solution that avoids any single points of failure. The solution must give the application \nthe ability to scale to meet user demand. \n \nWhich solution will meet these requirements? \n ","Option":["A. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across \nmultiple Availability Zones. Use an Amazon RDS DB instance in a Multi-AZ configuration. ","B. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group in a single \nAvailability Zone. Deploy the database \non an EC2 instance. Enable EC2 Auto Recovery. ","C. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across \nmultiple Availability Zones. Use an Amazon RDS DB instance with a read replica in a single \nAvailability Zone. Promote the read replica to replace the primary DB instance if the primary DB \ninstance fails. ","D. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across \nmultiple Availability Zones Deploy the primary and secondary database servers on EC2 instances across \nmultiple Availability Zones Use Amazon Elastic Block Store (Amazon EBS) Multi-Attach to create \nshared storage between the instances. "],"Explanation":"Answer: A \n \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":51,"QuestionContent":" \nA corporation has recruited a new cloud engineer who should not have access to the CompanyConfidential \nAmazon S3 bucket. The cloud engineer must have read and write permissions on an S3 bucket named \nAdminTools. \n \nWhich IAM policy will satisfy these criteria? \n \n ","Option":["A. Text, letter Description automatically generated \n 96 of 230  Practice Test Amazon Web Services - SAA-C03 ","B. Text Description automatically generated ","C. Text, application Description automatically generated \n 97 of 230  Practice Test Amazon Web Services - SAA-C03 ","D. Text, application Description automatically generated "],"Explanation":"Answer: A \n \nExplanation \nhttps://docs.amazonaws.cn/en_us/IAM/latest/UserGuide/reference_policies_examples_s3_rw-bucket.html \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":52,"QuestionContent":" \n 98 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \nA company runs a production application on a fleet of Amazon EC2 instances. The application reads the data \nfrom an Amazon SQS queue and processes the messages in parallel. The message volume is unpredictable and \noften has intermittent traffic. This application should continually process messages without any downtime. \n \nWhich solution meets these requirements MOST cost-effectively? \n ","Option":["A. Use Spot Instances exclusively to handle the maximum capacity required. ","B. Use Reserved Instances exclusively to handle the maximum capacity required. ","C. Use Reserved Instances for the baseline capacity and use Spot Instances to handle additional capacity. ","D. Use Reserved Instances for the baseline capacity and use On-Demand Instances to handle additional \ncapacity. "],"Explanation":"Answer: D \n \nExplanation \nWe recommend that you use On-Demand Instances for applications with short-term, irregular workloads that \ncannot be interrupted. \nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-on-demand-instances.html \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":53,"QuestionContent":" \nA company needs to retain application logs files for a critical application for 10 years. The application team \nregularly accesses logs from the past month for troubleshooting, but logs older than 1 month are rarely \naccessed. The application generates more than 10 TB of logs per month. \n \nWhich storage option meets these requirements MOST cost-effectively? \n ","Option":["A. Store the Iogs in Amazon S3 Use AWS Backup lo move logs more than 1 month old to S3 Glacier Deep \nArchive ","B. Store the logs in Amazon S3 Use S3 Lifecycle policies to move logs more than 1 month old to S3 \nGlacier Deep Archive ","C. Store the logs in Amazon CloudWatch Logs Use AWS Backup to move logs more then 1 month old to \nS3 Glacier Deep Archive ","D. Store the logs in Amazon CloudWatch Logs Use Amazon S3 Lifecycle policies to move logs more than \n1 month old to S3 Glacier Deep Archive "],"Explanation":"Answer: B \n \nExplanation \nYou need S3 to be able to archive the logs after one month. Cannot do that with CloudWatch Logs. \n \n 99 of 230  Practice Test Amazon Web Services - SAA-C03 \n \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":54,"QuestionContent":" \nA company is building a containerized application on premises and decides to move the application to AWS. \nThe application will have thousands of users soon after li is deployed. The company Is unsure how to manage \nthe deployment of containers at scale. The company needs to deploy the containerized application in a highly \navailable architecture that minimizes operational overhead. \n \nWhich solution will meet these requirements? \n ","Option":["A. Store container images In an Amazon Elastic Container Registry (Amazon ECR) repository. Use an \nAmazon Elastic Container Service (Amazon ECS) cluster with the AWS Fargate launch type to run the \ncontainers. Use target tracking to scale automatically based on demand. ","B. Store container images in an Amazon Elastic Container Registry (Amazon ECR) repository. Use an \nAmazon Elastic Container Service (Amazon ECS) cluster with the Amazon EC2 launch type to run the \ncontainers. Use target tracking to scale automatically based on demand. ","C. Store container images in a repository that runs on an Amazon EC2 instance. Run the containers on EC2 \ninstances that are spread across multiple Availability Zones. Monitor the average CPU utilization in \nAmazon CloudWatch. Launch new EC2 instances as needed ","D. Create an Amazon EC2 Amazon Machine Image (AMI) that contains the container image Launch EC2 \nInstances in an Auto Scaling group across multiple Availability Zones. Use an Amazon CloudWatch \nalarm to scale out EC2 instances when the average CPU utilization threshold is breached. "],"Explanation":"Answer: A \n \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":55,"QuestionContent":" \nAn entertainment company is using Amazon DynamoDB to store media metadata. The application is read \nintensive and experiencing delays. The company does not have staff to handle additional operational overhead \nand needs to improve the performance efficiency of DynamoDB without reconfiguring the application. \n \nWhat should a solutions architect recommend to meet this requirement? \n ","Option":["A. Use Amazon ElastiCache for Redis. ","B. Use Amazon DynamoDB Accelerator (DAX). ","C. Replicate data by using DynamoDB global tables. ","D. Use Amazon ElastiCache for Memcached with Auto Discovery enabled. "],"Explanation":"Answer: B \n \nExplanation \nhttps://aws.amazon.com/dynamodb/dax/ \n \n100 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":56,"QuestionContent":" \nA company has an AWS account used for software engineering. The AWS account has access to the \ncompany\u0027s on-premises data center through a pair of AWS Direct Connect connections. All non-VPC traffic \nroutes to the virtual private gateway. \n \nA development team recently created an AWS Lambda function through the console. The development team \nneeds to allow the function to access a database that runs in a private subnet in the company\u0027s data center. \n \nWhich solution will meet these requirements? \n ","Option":["A. Configure the Lambda function to run in the VPC with the appropriate security group. ","B. Set up a VPN connection from AWS to the data center. Route the traffic from the Lambda function \nthrough the VPN. ","C. Update the route tables in the VPC to allow the Lambda function to access the on-premises data center \nthrough Direct Connect. ","D. Create an Elastic IP address. Configure the Lambda function to send traffic through the Elastic IP \naddress without an elastic network interface. "],"Explanation":"Answer: A \n \nExplanation \nhttps://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html#vpc-managing-eni \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":57,"QuestionContent":" \nA company has a service that produces event data. The company wants to use AWS to process the event data \nas it is received. The data is written in a specific order that must be maintained throughout processing The \ncompany wants to implement a solution that minimizes operational overhead. \n \nHow should a solutions architect accomplish this? \n ","Option":["A. Create an Amazon Simple Queue Service (Amazon SQS) FIFO queue to hold messages Set up an AWS \nLambda function to process messages from the queue ","B. Create an Amazon Simple Notification Service (Amazon SNS) topic to deliver notifications containing \npayloads to process Configure an AWS Lambda function as a subscriber. ","C. Create an Amazon Simple Queue Service (Amazon SQS) standard queue to hold messages. Set up an \nAWS Lambda function to process messages from the queue independently ","D. Create an Amazon Simple Notification Service (Amazon SNS) topic to deliver notifications containing \npayloads to process. Configure an Amazon Simple Queue Service (Amazon SQS) queue as a subscriber. "],"Explanation":"Answer: A \n \n101 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nExplanation \nThe details are revealed in below url: \nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html \n \nFIFO (First-In-First-Out) queues are designed to enhance messaging between applications when the order of \noperations and events is critical, or where duplicates can\u0027t be tolerated. Examples of situations where you \nmight use FIFO queues include the following: To make sure that user-entered commands are run in the right \norder. To display the correct product price by sending price modifications in the right order. To prevent a \nstudent from enrolling in a course before registering for an account. \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":58,"QuestionContent":" \nA company has a Windows-based application that must be migrated to AWS. The application requires the use \nof a shared Windows file system attached to multiple Amazon EC2 Windows instances that are deployed \nacross multiple Availability Zones. \n \nWhat should a solutions architect do to meet this requirement? \n ","Option":["A. Configure AWS Storage Gateway in volume gateway mode. Mount the volume to each Windows \ninstance. ","B. Configure Amazon FSx for Windows File Server. Mount the Amazon FSx file system to each Windows \ninstance. ","C. Configure a file system by using Amazon Elastic File System (Amazon EFS). Mount the EFS file \nsystem to each Windows instance. ","D. Configure an Amazon Elastic Block Store (Amazon EBS) volume with the required size. Attach each \nEC2 instance to the volume. Mount the file system within the volume to each Windows instance. "],"Explanation":"Answer: B \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":59,"QuestionContent":" \nA hospital wants to create digital copies for its large collection of historical written records. The hospital will \ncontinue to add hundreds of new documents each day. The hospital\u0027s data team will scan the documents and \nwill upload the documents to the AWS Cloud. \n \nA solutions architect must implement a solution to analyze the documents, extract the medical information, \nand store the documents so that an application can run SQL queries on the data. The solution must maximize \nscalability and operational efficiency. \n \nWhich combination of steps should the solutions architect take to meet these requirements? (Select TWO.) \n ","Option":["A. Write the document information to an Amazon EC2 instance that runs a MySQL database. ","B. Write the document information to an Amazon S3 bucket. Use Amazon Athena to query the data. \n102 of 230 Practice Test Amazon Web Services - SAA-C03 ","C. Create an Auto Scaling group of Amazon EC2 instances to run a custom application that processes the \nscanned files and extracts the medical information. ","D. Create an AWS Lambda function that runs when new documents are uploaded. Use Amazon \nRekognition to convert the documents to raw text. Use Amazon Transcribe Medical to detect and extract \nrelevant medical information from the text. ","E. Create an AWS Lambda function that runs when new documents are uploaded. Use Amazon Textract to \nconvert the documents to raw text. Use Amazon Comprehend Medical to detect and extract relevant \nmedical information from the text. "],"Explanation":"Answer: D E \n \n","RightAnswer":["D","E"],"QuestionChoose":[]},{"QuestionNumber":60,"QuestionContent":" \nA company has an ecommerce checkout workflow that writes an order to a database and calls a service to \nprocess the payment. Users are experiencing timeouts during the checkout process. When users resubmit the \ncheckout form, multiple unique orders are created for the same desired transaction. \n \nHow should a solutions architect refactor this workflow to prevent the creation of multiple orders? \n ","Option":["A. Configure the web application to send an order message to Amazon Kinesis Data Firehose. Set the \npayment service to retrieve the message from Kinesis Data Firehose and process the order. ","B. Create a rule in AWS CloudTrail to invoke an AWS Lambda function based on the logged application \npath request Use Lambda to query the database, call the payment service, and pass in the order \ninformation. ","C. Store the order in the database. Send a message that includes the order number to Amazon Simple \nNotification Service (Amazon SNS). Set the payment service to poll Amazon SNS. retrieve the message, \nand process the order. ","D. Store the order in the database. Send a message that includes the order number to an Amazon Simple \nQueue Service (Amazon SQS) FIFO queue. Set the payment service to retrieve the message and process \nthe order. Delete the message from the queue. "],"Explanation":"Answer: D \n \nExplanation \nThis approach ensures that the order creation and payment processing steps are separate and atomic. By \nsending the order information to an SQS FIFO queue, the payment service can process the order one at a time \nand in the order they were received. If the payment service is unable to process an order, it can be retried later, \npreventing the creation of multiple orders. The deletion of the message from the queue after it is processed will \nprevent the same message from being processed multiple times. \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":61,"QuestionContent":"An ecommerce company hosts its analytics application in the AWS Cloud. The application generates about \n \n103 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \n300 MB of data each month. The data is stored in JSON format. The company is evaluating a disaster recovery \nsolution to back up the data. The data must be accessible in milliseconds if it is needed, and the data must be \nkept for 30 days. \n \nWhich solution meets these requirements MOST cost-effectively? \n ","Option":["A. Amazon OpenSearch Service (Amazon Elasticsearch Service) ","B. Amazon S3 Glacier ","C. Amazon S3 Standard ","D. Amazon RDS for PostgreSQL "],"Explanation":"Answer: C \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":62,"QuestionContent":" \nA company uses a three-tier web application to provide training to new employees. The application is accessed \nfor only 12 hours every day. The company is using an Amazon RDS for MySQL DB instance to store \ninformation and wants to minimize costs. \n \nWhat should a solutions architect do to meet these requirements? \n ","Option":["A. Configure an IAM policy for AWS Systems Manager Session Manager. Create an IAM role for the \npolicy. Update the trust relationship of the role. Set up automatic start and stop for the DB instance. ","B. Create an Amazon ElastiCache for Redis cache cluster that gives users the ability to access the data from \nthe cache when the DB instance is stopped. Invalidate the cache after the DB instance is started. ","C. Launch an Amazon EC2 instance. Create an IAM role that grants access to Amazon RDS. Attach the \nrole to the EC2 instance. Configure a cron job to start and stop the EC2 instance on the desired schedule. ","D. Create AWS Lambda functions to start and stop the DB instance. Create Amazon EventBridge (Amazon \nCloudWatch Events) scheduled rules to invoke the Lambda functions. Configure the Lambda functions \nas event targets for the rules "],"Explanation":"Answer: C \n \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":63,"QuestionContent":" \nA large media company hosts a web application on AWS. The company wants to start caching confidential \nmedia files so that users around the world will have reliable access to the files. The content is stored in \nAmazon S3 buckets. The company must deliver the content quickly, regardless of where the requests originate \ngeographically. \n \nWhich solution will meet these requirements? \n \n104 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n ","Option":["A. Use AWS DataSync to connect the S3 buckets to the web application. ","B. Deploy AWS Global Accelerator to connect the S3 buckets to the web application. ","C. Deploy Amazon CloudFront to connect the S3 buckets to CloudFront edge servers. ","D. Use Amazon Simple Queue Service (Amazon SQS) to connect the S3 buckets to the web application. "],"Explanation":"Answer: C \n \nExplanation \nCloudFront uses a local cache to provide the response, AWS Global accelerator proxies requests and connects \nto the application all the time for the response. \n \nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3 \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":64,"QuestionContent":" \nA company sells ringtones created from clips of popular songs. The files containing the ringtones are stored in \nAmazon S3 Standard and are at least 128 KB in size. The company has millions of files, but downloads are \ninfrequent for ringtones older than 90 days. The company needs to save money on storage while keeping the \nmost accessed files readily available for its users. \n \nWhich action should the company take to meet these requirements MOST cost-effectively? \n ","Option":["A. Configure S3 Standard-Infrequent Access (S3 Standard-IA) storage for the initial storage tier of the \nobjects. ","B. Move the files to S3 Intelligent-Tiering and configure it to move objects to a less expensive storage tier \nafter 90 days. ","C. Configure S3 inventory to manage objects and move them to S3 Standard-Infrequent Access (S3 \nStandard-1A) after 90 days. ","D. Implement an S3 Lifecycle policy that moves the objects from S3 Standard to S3 Standard-Infrequent \nAccess (S3 Standard-1A) after 90 days. "],"Explanation":"Answer: D \n \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":65,"QuestionContent":" \nA company uses AWS Organizations to create dedicated AWS accounts for each business unit to manage each \nbusiness unit\u0027s account independently upon request. The root email recipient missed a notification that was \nsent to the root user email address of one account. The company wants to ensure that all future notifications \nare not missed. Future notifications must be limited to account administrators. \n \nWhich solution will meet these requirements? \n \n105 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \n ","Option":["A. Configure the company\u0027s email server to forward notification email messages that are sent to the AWS \naccount root user email address to all users in the organization. ","B. Configure all AWS account root user email addresses as distribution lists that go to a few administrators \nwho can respond to alerts. Configure AWS account alternate contacts in the AWS Organizations console \nor programmatically. ","C. Configure all AWS account root user email messages to be sent to one administrator who is responsible \nfor monitoring alerts and forwarding those alerts to the appropriate groups. ","D. Configure all existing AWS accounts and all newly created accounts to use the same root user email \naddress. Configure AWS account alternate contacts in the AWS Organizations console or \nprogrammatically. "],"Explanation":"Answer: D \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":66,"QuestionContent":" \nA company has two applications: a sender application that sends messages with payloads to be processed and a \nprocessing application intended to receive the messages with payloads. The company wants to implement an \nAWS service to handle messages between the two applications. The sender application can send about 1.000 \nmessages each hour. The messages may take up to 2 days to be processed. If the messages fail to process, they \nmust be retained so that they do not impact the processing of any remaining messages. \n \nWhich solution meets these requirements and is the MOST operationally efficient? \n ","Option":["A. Set up an Amazon EC2 instance running a Redis database. Configure both applications to use the \ninstance. Store, process, and delete the messages, respectively. ","B. Use an Amazon Kinesis data stream to receive the messages from the sender application. Integrate the \nprocessing application with the Kinesis Client Library (KCL). ","C. Integrate the sender and processor applications with an Amazon Simple Queue Service (Amazon SQS) \nqueue. Configure a dead-letter queue to collect the messages that failed to process. ","D. Subscribe the processing application to an Amazon Simple Notification Service (Amazon SNS) topic to \nreceive notifications to process. Integrate the sender application to write to the SNS topic. "],"Explanation":"Answer: C \n \nExplanation \nhttps://aws.amazon.com/blogs/compute/building-loosely-coupled-scalable-c-applications-with-amazon-sqs-and- \nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.htm \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":67,"QuestionContent":" \n106 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nA company wants to migrate its on-premises data center to AWS. According to the company\u0027s compliance \nrequirements, the company can use only the ap-northeast-3 Region. Company administrators are not permitted \nto connect VPCs to the internet. \n \nWhich solutions will meet these requirements? (Choose two.) \n ","Option":["A. Use AWS Control Tower to implement data residency guardrails to deny internet access and deny \naccess to all AWS Regions except ap-northeast-3. ","B. Use rules in AWS WAF to prevent internet access. Deny access to all AWS Regions except \nap-northeast-3 in the AWS account settings. ","C. Use AWS Organizations to configure service control policies (SCPS) that prevent VPCs from gaining \ninternet access. Deny access to all AWS Regions except ap-northeast-3. ","D. Create an outbound rule for the network ACL in each VPC to deny all traffic from 0.0.0.0/0. Create an \nIAM policy for each user to prevent the use of any AWS Region other than ap-northeast-3. ","E. Use AWS Config to activate managed rules to detect and alert for internet gateways and to detect and \nalert for new resources deployed outside of ap-northeast-3. "],"Explanation":"Answer: A C \n \n \n","RightAnswer":["A","C"],"QuestionChoose":[]},{"QuestionNumber":68,"QuestionContent":" \nA company owns an asynchronous API that is used to ingest user requests and, based on the request type, \ndispatch requests to the appropriate microservice for processing. The company is using Amazon API Gateway \nto deploy the API front end, and an AWS Lambda function that invokes Amazon DynamoDB to store user \nrequests before dispatching them to the processing microservices. \n \nThe company provisioned as much DynamoDB throughput as its budget allows, but the company is still \nexperiencing availability issues and is losing user requests. \n \nWhat should a solutions architect do to address this issue without impacting existing users? \n ","Option":["A. Add throttling on the API Gateway with server-side throttling limits. ","B. Use DynamoDB Accelerator (DAX) and Lambda to buffer writes to DynamoDB. ","C. Create a secondary index in DynamoDB for the table with the user requests. ","D. Use the Amazon Simple Queue Service (Amazon SQS) queue and Lambda to buffer writes to \nDynamoDB. "],"Explanation":"Answer: D \n \nExplanation \nBy using an SQS queue and Lambda, the solutions architect can decouple the API front end from the \n \n107 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nprocessing microservices and improve the overall scalability and availability of the system. The SQS queue \nacts as a buffer, allowing the API front end to continue accepting user requests even if the processing \nmicroservices are experiencing high workloads or are temporarily unavailable. The Lambda function can then \nretrieve requests from the SQS queue and write them to DynamoDB, ensuring that all user requests are stored \nand processed. This approach allows the company to scale the processing microservices independently from \nthe API front end, ensuring that the API remains available to users even during periods of high demand. \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":69,"QuestionContent":" \nA company has implemented a self-managed DNS solution on three Amazon EC2 instances behind a Network \nLoad Balancer (NLB) in the us-west-2 Region. Most of the company\u0027s users are located in the United States \nand Europe. The company wants to improve the performance and availability of the solution. The company \nlaunches and configures three EC2 instances in the eu-west-1 Region and adds the EC2 instances as targets for \na new NLB. \n \nWhich solution can the company use to route traffic to all the EC2 instances? \n ","Option":["A. Create an Amazon Route 53 geolocation routing policy to route requests to one of the two NLBs. Create \nan Amazon CloudFront distribution. Use the Route 53 record as the distribution\u0027s origin. ","B. Create a standard accelerator in AWS Global Accelerator. Create endpoint groups in us-west-2 and \neu-west-1. Add the two NLBs as endpoints for the endpoint groups. ","C. Attach Elastic IP addresses to the six EC2 instances. Create an Amazon Route 53 geolocation routing \npolicy to route requests to one of the six EC2 instances. Create an Amazon CloudFront distribution. Use \nthe Route 53 record as the distribution\u0027s origin. ","D. Replace the two NLBs with two Application Load Balancers (ALBs). Create an Amazon Route 53 \nlatency routing policy to route requests to one of the two ALBs. Create an Amazon CloudFront \ndistribution. Use the Route 53 record as the distribution\u0027s origin. "],"Explanation":"Answer: B \n \nExplanation \nFor standard accelerators, Global Accelerator uses the AWS global network to route traffic to the optimal \nregional endpoint based on health, client location, and policies that you configure, which increases the \navailability of your applications. Endpoints for standard accelerators can be Network Load Balancers, \nApplication Load Balancers, Amazon EC2 instances, or Elastic IP addresses that are located in one AWS \nRegion or multiple Regions. \n \nhttps://docs.aws.amazon.com/global-accelerator/latest/dg/what-is-global-accelerator.html \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":70,"QuestionContent":" \nA company needs to save the results from a medical trial to an Amazon S3 repository. The repository must \nallow a few scientists to add new files and must restrict all other users to read-only access. No users can have \nthe ability to modify or delete any files in the repository. The company must keep every file in the repository \nfor a minimum of 1 year after its creation date. \n \n108 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nWhich solution will meet these requirements? \n ","Option":["A. Use S3 Object Lock In governance mode with a legal hold of 1 year ","B. Use S3 Object Lock in compliance mode with a retention period of 365 days. ","C. Use an IAM role to restrict all users from deleting or changing objects in the S3 bucket Use an S3 \nbucket policy to only allow the IAM role ","D. Configure the S3 bucket to invoke an AWS Lambda function every tune an object is added Configure \nthe function to track the hash of the saved object to that modified objects can be marked accordingly "],"Explanation":"Answer: C \n \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":71,"QuestionContent":" \nA company wants to migrate its MySQL database from on premises to AWS. The company recently \nexperienced a database outage that significantly impacted the business. To ensure this does not happen again, \nthe company wants a reliable database solution on AWS that minimizes data loss and stores every transaction \non at least two nodes. \n \nWhich solution meets these requirements? \n ","Option":["A. Create an Amazon RDS DB instance with synchronous replication to three nodes in three Availability \nZones. ","B. Create an Amazon RDS MySQL DB instance with Multi-AZ functionality enabled to synchronously \nreplicate the data. ","C. Create an Amazon RDS MySQL DB instance and then create a read replica in a separate AWS Region \nthat synchronously replicates the data. ","D. Create an Amazon EC2 instance with a MySQL engine installed that triggers an AWS Lambda function \nto synchronously replicate the data to an Amazon RDS MySQL DB instance. "],"Explanation":"Answer: B \nExplanation \nQ: What does Amazon RDS manage on my behalf? \n \nAmazon RDS manages the work involved in setting up a relational database: from provisioning the \ninfrastructure capacity you request to installing the database software. Once your database is up and running, \nAmazon RDS automates common administrative tasks such as performing backups and patching the software \nthat powers your database. With optional Multi-AZ deployments, Amazon RDS also manages synchronous \ndata replication across Availability Zones with automatic failover. \n \nhttps://aws.amazon.com/rds/faqs/ \n \n109 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":72,"QuestionContent":" \nA company runs a global web application on Amazon EC2 instances behind an Application Load Balancer \nThe application stores data in Amazon Aurora. The company needs to create a disaster recovery solution and \ncan tolerate up to 30 minutes of downtime and potential data loss. The solution does not need to handle the \nload when the primary infrastructure is healthy \n \nWhat should a solutions architect do to meet these requirements? \n ","Option":["A. Deploy the application with the required infrastructure elements in place Use Amazon Route 53 to \nconfigure active-passive failover Create an Aurora Replica in a second AWS Region ","B. Host a scaled-down deployment of the application in a second AWS Region Use Amazon Route 53 to \nconfigure active-active failover Create an Aurora Replica in the second Region ","C. Replicate the primary infrastructure in a second AWS Region Use Amazon Route 53 to configure \nactive-active failover Create an Aurora database that is restored from the latest snapshot ","D. Back up data with AWS Backup Use the backup to create the required infrastructure in a second AWS \nRegion Use Amazon Route 53 to configure active-passive failover Create an Aurora second primary \ninstance in the second Region "],"Explanation":"Answer: A \n \nExplanation \nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":73,"QuestionContent":" \nA company\u0027s application Is having performance issues The application staleful and needs to complete \nm-memory tasks on Amazon EC2 instances. The company used AWS CloudFormation to deploy \ninfrastructure and used the M5 EC2 Instance family As traffic increased, the application performance degraded \nUsers are reporting delays when the users attempt to access the application. \n \nWhich solution will resolve these issues in the MOST operationally efficient way? \n ","Option":["A. Replace the EC2 Instances with T3 EC2 instances that run in an Auto Scaling group. Made the changes \nby using the AWS Management Console. ","B. Modify the CloudFormation templates to run the EC2 instances in an Auto Scaling group. Increase the \ndesired capacity and the maximum capacity of the Auto Scaling group manually when an increase is \nnecessary ","C. Modify the CloudFormation templates. Replace the EC2 instances with R5 EC2 instances. Use Amazon \nCloudWatch built-in EC2 memory metrics to track the application performance for future capacity \nplanning. ","D. Modify the CloudFormation templates. Replace the EC2 instances with R5 EC2 instances. Deploy the \n110 of 230 Practice Test Amazon Web Services - SAA-C03 \nAmazon CloudWatch agent on the EC2 instances to generate custom application latency metrics for \nfuture capacity planning. "],"Explanation":"Answer: D \n \nExplanation \nhttps://aws.amazon.com/premiumsupport/knowledge-center/cloudwatch-memory-metrics-ec2/ \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":74,"QuestionContent":" \nA company runs an Oracle database on premises. As part of the company\u2019s migration to AWS, the company \nwants to upgrade the database to the most recent available version. The company also wants to set up disaster \nrecovery (DR) for the database. The company needs to minimize the operational overhead for normal \noperations and DR setup. The company also needs to maintain access to the database\u0027s underlying operating \nsystem. \n \nWhich solution will meet these requirements? \n ","Option":["A. Migrate the Oracle database to an Amazon EC2 instance. Set up database replication to a different AWS \nRegion. ","B. Migrate the Oracle database to Amazon RDS for Oracle. Activate Cross-Region automated backups to \nreplicate the snapshots to another AWS Region. ","C. Migrate the Oracle database to Amazon RDS Custom for Oracle. Create a read replica for the database \nin another AWS Region. ","D. Migrate the Oracle database to Amazon RDS for Oracle. Create a standby database in another \nAvailability Zone. "],"Explanation":"Answer: C \n \nExplanation \nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-custom.html and \nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/working-with-custom-oracle.html \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":75,"QuestionContent":" \nA company wants to run applications in containers in the AWS Cloud. These applications are stateless and can \ntolerate disruptions within the underlying infrastructure. The company needs a solution that minimizes cost \nand operational overhead. \n \nWhat should a solutions architect do to meet these requirements? \n ","Option":["A. Use Spot Instances in an Amazon EC2 Auto Scaling group to run the application containers. ","B. Use Spot Instances in an Amazon Elastic Kubernetes Service (Amazon EKS) managed node group. \n111 of 230 Practice Test Amazon Web Services - SAA-C03 ","C. Use On-Demand Instances in an Amazon EC2 Auto Scaling group to run the application containers. ","D. Use On-Demand Instances in an Amazon Elastic Kubernetes Service (Amazon EKS) managed node \ngroup. "],"Explanation":"Answer: A \n \nExplanation \nhttps://aws.amazon.com/cn/blogs/compute/cost-optimization-and-resilience-eks-with-spot-instances/ \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":76,"QuestionContent":" \nA company hosts a two-tier application on Amazon EC2 instances and Amazon RDS. The application\u0027s \ndemand varies based on the time of day. The load is minimal after work hours and on weekends. The EC2 \ninstances run in an EC2 Auto Scaling group that is configured with a minimum of two instances and a \nmaximum of five instances. The application must be available at all times, but the company is concerned about \noverall cost. \n \nWhich solution meets the availability requirement MOST cost-effectively? \n ","Option":["A. Use all EC2 Spot Instances. Stop the RDS database when it is not in use. ","B. Purchase EC2 Instance Savings Plans to cover five EC2 instances. Purchase an RDS Reserved DB \nInstance ","C. Purchase two EC2 Reserved Instances Use up to three additional EC2 Spot Instances as needed. Stop the \nRDS database when it is not in use. ","D. Purchase EC2 Instance Savings Plans to cover two EC2 instances. Use up to three additional EC2 \nOn-Demand Instances as needed. Purchase an RDS Reserved DB Instance. "],"Explanation":"Answer: D \n \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":77,"QuestionContent":" \nA company wants to build a scalable key management Infrastructure to support developers who need to \nencrypt data in their applications. \n \nWhat should a solutions architect do to reduce the operational burden? \n ","Option":["A. Use multifactor authentication (MFA) to protect the encryption keys. ","B. Use AWS Key Management Service (AWS KMS) to protect the encryption keys ","C. Use AWS Certificate Manager (ACM) to create, store, and assign the encryption keys \n112 of 230 Practice Test Amazon Web Services - SAA-C03 ","D. Use an IAM policy to limit the scope of users who have access permissions to protect the encryption \nkeys "],"Explanation":"Answer: B \n \nExplanation \nhttps://aws.amazon.com/kms/faqs/#:~:text=If%20you%20are%20a%20developer%20who%20needs%20to%20d \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":78,"QuestionContent":" \nA company has a mulli-tier application that runs six front-end web servers in an Amazon EC2 Auto Scaling \ngroup in a single Availability Zone behind an Application Load Balancer (ALB). A solutions architect needs \nlo modify the infrastructure to be highly available without modifying the application. \n \nWhich architecture should the solutions architect choose that provides high availability? \n ","Option":["A. Create an Auto Scaling group that uses three Instances across each of tv/o Regions. ","B. Modify the Auto Scaling group to use three instances across each of two Availability Zones. ","C. Create an Auto Scaling template that can be used to quickly create more instances in another Region. ","D. Change the ALB in front of the Amazon EC2 instances in a round-robin configuration to balance traffic \nto the web tier. "],"Explanation":"Answer: B \nExplanation \nHigh availability can be enabled for this architecture quite simply by modifying the existing Auto Scaling \ngroup to use multiple availability zones. The ASG will automatically balance the load so you don\u0027t actually \nneed to specify the instances per AZ. \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":79,"QuestionContent":" \nA company is concerned about the security of its public web application due to recent web attacks. The \napplication uses an Application Load Balancer (ALB). A solutions architect must reduce the risk of DDoS \nattacks against the application. \n \nWhat should the solutions architect do to meet this requirement? \n ","Option":["A. Add an Amazon Inspector agent to the ALB. ","B. Configure Amazon Macie to prevent attacks. ","C. Enable AWS Shield Advanced to prevent attacks. ","D. Configure Amazon GuardDuty to monitor the ALB. \n113 of 230 Practice Test Amazon Web Services - SAA-C03 "],"Explanation":"Answer: C \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":80,"QuestionContent":" \nA solutions architect needs to implement a solution to reduce a company\u0027s storage costs. All the company\u0027s \ndata is in the Amazon S3 Standard storage class. The company must keep all data for at least 25 years. Data \nfrom the most recent 2 years must be highly available and immediately retrievable. \n \nWhich solution will meet these requirements? \n ","Option":["A. Set up an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive immediately. ","B. Set up an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive after 2 years. ","C. Use S3 Intelligent-Tiering. Activate the archiving option to ensure that data is archived in S3 Glacier \nDeep Archive. ","D. Set up an S3 Lifecycle policy to transition objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) \nimmediately and to S3 Glacier Deep Archive after 2 years. "],"Explanation":"Answer: B \n \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":81,"QuestionContent":"A company has a data ingestion workflow that includes the following components: \n\u2022 An Amazon Simple Notation Service (Amazon SNS) topic that receives notifications about new data \ndeliveries \n \n\u2022 An AWS Lambda function that processes and stores the data \n \nThe ingestion workflow occasionally fails because of network connectivity issues. When tenure occurs the \ncorresponding data is not ingested unless the company manually reruns the job. What should a solutions \narchitect do to ensure that all notifications are eventually processed? \n ","Option":["A. Configure the Lambda function (or deployment across multiple Availability Zones ","B. Modify me Lambda functions configuration to increase the CPU and memory allocations tor the \n(unction ","C. Configure the SNS topic\u0027s retry strategy to increase both the number of retries and the wait time between \nretries ","D. Configure an Amazon Simple Queue Service (Amazon SQS) queue as the on failure destination Modify \nthe Lambda function to process messages in the queue "],"Explanation":"Answer: A \n \n114 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \n \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":82,"QuestionContent":" \nA solutions architect is optimizing a website for an upcoming musical event. Videos of the performances will \nbe streamed in real time and then will be available on demand. The event is expected to attract a global online \naudience. \n \nWhich service will improve the performance of both the real-lime and on-demand streaming? \n ","Option":["A. Amazon CloudFront ","B. AWS Global Accelerator ","C. Amazon Route 53 ","D. Amazon S3 Transfer Acceleration "],"Explanation":"Answer: A \n \nExplanation \nYou can use CloudFront to deliver video on demand (VOD) or live streaming video using any HTTP origin. \nOne way you can set up video workflows in the cloud is by using CloudFront together with AWS Media \nServices. \nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/on-demand-streaming-video.html \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":83,"QuestionContent":" \nA company is running an online transaction processing (OLTP) workload on AWS. This workload uses an \nunencrypted Amazon RDS DB instance in a Multi-AZ deployment. Daily database snapshots are taken from \nthis instance. \n \nWhat should a solutions architect do to ensure the database and snapshots are always encrypted moving \nforward? \n ","Option":["A. Encrypt a copy of the latest DB snapshot. Replace existing DB instance by restoring the encrypted \nsnapshot ","B. Create a new encrypted Amazon Elastic Block Store (Amazon EBS) volume and copy the snapshots to \nit Enable encryption on the DB instance ","C. Copy the snapshots and enable encryption using AWS Key Management Service (AWS KMS) Restore \nencrypted snapshot to an existing DB instance ","D. Copy the snapshots to an Amazon S3 bucket that is encrypted using server-side encryption with AWS \nKey Management Service (AWS KMS) managed keys (SSE-KMS) "],"Explanation":"Answer: A \n \n115 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nExplanation \nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_RestoreFromSnapshot.html#USER_Restor \n \nUnder \u0022Encrypt unencrypted resources\u0022 - \nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":84,"QuestionContent":" \nA company wants to direct its users to a backup static error page if the company\u0027s primary website is \nunavailable. The primary website\u0027s DNS records are hosted in Amazon Route 53. The domain is pointing to an \nApplication Load Balancer (ALB). The company needs a solution that minimizes changes and infrastructure \noverhead. \n \nWhich solution will meet these requirements? \n ","Option":["A. Update the Route 53 records to use a latency routing policy. Add a static error page that is hosted in an \nAmazon S3 \nbucket to the records so that the traffic is sent to the most responsive endpoints. ","B. Set up a Route 53 active-passive failover configuration. Direct traffic to a static error page that is hosted \nin an \nAmazon S3 bucket when Route 53 health checks determine that the ALB endpoint is unhealthy. ","C. Set up a Route 53 active-active configuration with the ALB and an Amazon EC2 instance that hosts a \nstatic error \npage as endpoints. Configure Route 53 to send requests to the instance only if the health checks fail for \nthe ALB. ","D. Update the Route 53 records to use a multivalue answer routing policy. Create a health check. Direct \ntraffic to the \nwebsite if the health check passes. Direct traffic to a static error page that is hosted in Amazon S3 if the \nhealth check does not pass. "],"Explanation":"Answer: B \n \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":85,"QuestionContent":" \nA company hosts a website analytics application on a single Amazon EC2 On-Demand Instance. The analytics \nsoftware is written in PHP and uses a MySQL database. The analytics software, the web server that provides \nPHP, and the database server are all hosted on the EC2 instance. The application is showing signs of \nperformance degradation during busy times and is presenting 5xx errors. The company needs to make the \napplication scale seamlessly. \n \n116 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nWhich solution will meet these requirements MOST cost-effectively? \n ","Option":["A. Migrate the database to an Amazon RDS for MySQL DB instance. Create an AMI of the web \napplication. Use the AMI to launch a second EC2 On-Demand Instance. Use an Application Load \nBalancer to distribute the load to each EC2 instance. ","B. Migrate the database to an Amazon RDS for MySQL DB instance. Create an AMI of the web \napplication. Use the AMI to launch a second EC2 On-Demand Instance. Use Amazon Route 53 \nweighted routing to distribute the load across the two EC2 instances. ","C. Migrate the database to an Amazon Aurora MySQL DB instance. Create an AWS Lambda function to \nstop the EC2 instance and change the instance type. Create an Amazon CloudWatch alarm to invoke the \nLambda function when CPU utilization surpasses 75%. ","D. Migrate the database to an Amazon Aurora MySQL DB instance. Create an AMI of the web application. \nApply the AMI to a launch template. Create an Auto Scaling group with the launch template Configure \nthe launch template to use a Spot Fleet. Attach an Application Load Balancer to the Auto Scaling group. "],"Explanation":"Answer: D \n \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":86,"QuestionContent":" \nA company runs a stateless web application in production on a group of Amazon EC2 On-Demand Instances \nbehind an Application Load Balancer. The application experiences heavy usage during an 8-hour period each \nbusiness day. Application usage is moderate and steady overnight Application usage is low during weekends. \n \nThe company wants to minimize its EC2 costs without affecting the availability of the application. \nWhich solution will meet these requirements? ","Option":["A. Use Spot Instances for the entire workload. ","B. Use Reserved instances for the baseline level of usage Use Spot Instances for any additional capacity \nthat the application needs. ","C. Use On-Demand Instances for the baseline level of usage. Use Spot Instances for any additional \ncapacity that the application needs ","D. Use Dedicated Instances for the baseline level of usage. Use On-Demand Instances for any additional \ncapacity that the application needs "],"Explanation":"Answer: B \n \n117 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \n \nTopic 3, Exam Pool C \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":1,"QuestionContent":" \nA company has hired an external vendor to perform work in the company\u2019s AWS account. The vendor uses an \nautomated tool that is hosted in an AWS account that the vendor owns. The vendor does not have IAM access \nto the company\u2019s AWS account. \n \nHow should a solutions architect grant this access to the vendor? \n ","Option":["A. Create an IAM role in the company\u2019s account to delegate access to the vendor\u2019s IAM role. Attach the \nappropriate IAM policies to the role for the permissions that the vendor requires. ","B. Create an IAM user in the company\u2019s account with a password that meets the password complexity \nrequirements. Attach the appropriate IAM policies to the user for the permissions that the vendor \nrequires. ","C. Create an IAM group in the company\u2019s account. Add the tool\u2019s IAM user from the vendor account to \nthe group. Attach the appropriate IAM policies to the group for the permissions that the vendor requires. ","D. Create a new identity provider by choosing \u201CAWS account\u201D as the provider type in the IAM console. \nSupply the vendor\u2019s AWS account ID and user name. Attach the appropriate IAM policies to the new \nprovider for the permissions that the vendor requires. "],"Explanation":"Answer: A \n \nExplanation \nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_third-party.html \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":2,"QuestionContent":" \nA payment processing company records all voice communication with its customers and stores the audio files \nin an Amazon S3 bucket. The company needs to capture \n \nthe text from the audio files. The company must remove from the text any personally identifiable information \n(Pll) that belongs to customers. \n \nWhat should a solutions architect do to meet these requirements? \n ","Option":["A. Process the audio files by using Amazon Kinesis Video Streams. Use an AWS Lambda function to scan \nfor known Pll patterns. ","B. When an audio file is uploaded to the S3 bucket, invoke an AWS Lambda function to start an Amazon \nTextract task to analyze the call recordings. ","C. Configure an Amazon Transcribe transcription job with Pll redaction turned on. When an audio file is \nuploaded to the S3 bucket, invoke an AWS Lambda function to start the transcription job. Store the \n118 of 230 Practice Test Amazon Web Services - SAA-C03 \noutput in a separate S3 bucket. ","D. Create an Amazon Connect contact flow that ingests the audio files with transcription turned on. Embed \nan AWS Lambda function to scan for known Pll patterns. Use Amazon EventBridge (Amazon \nCloudWatch Events) to start the contact flow when an audio file is uploaded to the S3 bucket. "],"Explanation":"Answer: C \n \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":3,"QuestionContent":" \nA company has deployed a server less application that invokes an AWS Lambda function when new \ndocuments are uploaded to an Amazon S3 bucket The application uses the Lambda function to process the \ndocuments After a recent marketing campaign the company noticed that the application did not process many \nof The documents \n \nWhat should a solutions architect do to improve the architecture of this application? \n ","Option":["A. Set the Lambda function\u0027s runtime timeout value to 15 minutes ","B. Configure an S3 bucket replication policy Stage the documents m the S3 bucket for later processing ","C. Deploy an additional Lambda function Load balance the processing of the documents across the two \nLambda functions ","D. Create an Amazon Simple Queue Service (Amazon SOS) queue Send the requests to the queue \nConfigure the queue as an event source for Lambda. "],"Explanation":"Answer: B \n \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":4,"QuestionContent":" \nA company has a regional subscription-based streaming service that runs in a single AWS Region. The \narchitecture consists of web servers and application servers on Amazon EC2 instances. The EC2 instances are \nin Auto Scaling groups behind Elastic Load Balancers. The architecture includes an Amazon Aurora database \ncluster that extends across multiple Availability Zones. \n \nThe company wants to expand globally and to ensure that its application has minimal downtime. \n ","Option":["A. Extend the Auto Scaling groups for the web tier and the application tier to deploy instances in \nAvailability Zones in a second Region. Use an Aurora global database to deploy the database in the \nprimary Region and the second Region. Use Amazon Route 53 health checks with a failover routing \npolicy to the second Region. ","B. Deploy the web tier and the application tier to a second Region. Add an Aurora PostgreSQL \ncross-Region Aurara Replica in the second Region. Use Amazon Route 53 health checks with a \nfailovers routing policy to the second Region, Promote the secondary to primary as needed. \n119 of 230 Practice Test Amazon Web Services - SAA-C03 ","C. Deploy the web tier and the applicatin tier to a second Region. Create an Aurora PostSQL database in \nthe second Region. Use AWS Database Migration Service (AWS DMS) to replicate the primary \ndatabase to the second Region. Use Amazon Route 53 health checks with a failover routing policy to the \nsecond Region. ","D. Deploy the web tier and the application tier to a second Region. Use an Amazon Aurora global database \nto deploy the database in the primary Region and the second Region. Use Amazon Route 53 health \nchecks with a failover routing policy to the second Region. Promote the secondary to primary as needed. "],"Explanation":"Answer: A \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":5,"QuestionContent":" \nA solutions architect is designing a two-tiered architecture that includes a public subnet and a database subnet. \nThe web servers in the public subnet must be open to the internet on port 443. The Amazon RDS for MySQL \nD6 instance in the database subnet must be accessible only to the web servers on port 3306. \n \nWhich combination of steps should the solutions architect take to meet these requirements? (Select TWO.) \n ","Option":["A. Create a network ACL for the public subnet Add a rule to deny outbound traffic to 0 0 0 0/0 on port \n3306 ","B. Create a security group for the DB instance Add a rule to allow traffic from the public subnet CIDR \nblock on port 3306 ","C. Create a security group for the web servers in the public subnet Add a rule to allow traffic from 0 0 0 \nO\u0027O on port 443 ","D. Create a security group for the DB instance Add a rule to allow traffic from the web servers\u0027 security \ngroup on port 3306 ","E. Create a security group for the DB instance Add a rule to deny all traffic except traffic from the web \nservers\u0027 security group on port 3306 "],"Explanation":"Answer: C D \n \n","RightAnswer":["C","D"],"QuestionChoose":[]},{"QuestionNumber":6,"QuestionContent":" \nA company hosts multiple production applications. One of the applications consists of resources from Amazon \nEC2, AWS Lambda, Amazon RDS, Amazon Simple Notification Service (Amazon SNS), and Amazon Simple \nQueue Service (Amazon SQS) across multiple AWS Regions. All company resources are tagged with a tag \nname of \u201Capplication\u201D and a value that corresponds to each application. A solutions architect must provide the \nquickest solution for identifying all of the tagged components. \n \nWhich solution meets these requirements? \n ","Option":["A. Use AWS CloudTrail to generate a list of resources with the application tag. ","B. Use the AWS CLI to query each service across all Regions to report the tagged components. \n120 of 230 Practice Test Amazon Web Services - SAA-C03 ","C. Run a query in Amazon CloudWatch Logs Insights to report on the components with the application tag. ","D. Run a query with the AWS Resource Groups Tag Editor to report on the resources globally with the \napplication tag. "],"Explanation":"Answer: D \n \nExplanation \nhttps://docs.aws.amazon.com/tag-editor/latest/userguide/tagging.html \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":7,"QuestionContent":" \nA solutions architect is designing a multi-tier application for a company. The application\u0027s users upload images \nfrom a mobile device. The application generates a thumbnail of each image and returns a message to the user \nto confirm that the image was uploaded successfully. \n \nThe thumbnail generation can take up to 60 seconds, but the company wants to provide a faster response time \nto its users to notify them that the original image was received. The solutions architect must design the \napplication to asynchronously dispatch requests to the different application tiers. \n \nWhat should the solutions architect do to meet these requirements? \n ","Option":["A. Write a custom AWS Lambda function to generate the thumbnail and alert the user. Use the image \nupload process as an event source to invoke the Lambda function. ","B. Create an AWS Step Functions workflow Configure Step Functions to handle the orchestration between \nthe application tiers and alert the user when thumbnail generation is complete ","C. Create an Amazon Simple Queue Service (Amazon SQS) message queue. As images are uploaded, \nplace a message on the SQS queue for thumbnail generation. Alert the user through an application \nmessage that the image was received ","D. Create Amazon Simple Notification Service (Amazon SNS) notification topics and subscriptions Use \none subscription with the application to generate the thumbnail after the image upload is complete. Use \na second subscription to message the user\u0027s mobile app by way of a push notification after thumbnail \ngeneration is complete. "],"Explanation":"Answer: C \n \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":8,"QuestionContent":" \nA gaming company is moving its public scoreboard from a data center to the AWS Cloud. The company uses \nAmazon EC2 Windows Server instances behind an \n \nApplication Load Balancer to host its dynamic application. The company needs a highly available storage \nsolution for the application. The application consists of static files and dynamic server-side code. \n \n121 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nWhich combination of steps should a solutions architect take to meet these requirements? (Select TWO.) \u0027 \nStore the static files on Amazon S3. Use Amazon  ","Option":["A. CloudFront to cache objects at the edge. ","B. Store the static files on Amazon S3. Use Amazon ElastiCache to cache objects at the edge. ","C. Store the server-side code on Amazon Elastic File System (Amazon EFS). Mount the EFS volume on each \nEC2 instance to share the files. ","D. Store the server-side code on Amazon FSx for Windows File Server. Mount the FSx for Windows File \nServer volume on each EC2 instance to share the files. ","E. Store the server-side code on a General Purpose SSD (gp2) Amazon Elastic Block Store (Amazon EBS) \nvolume. Mount the EBS volume on each EC2 instance to share the files. "],"Explanation":"Answer:  A, E \n \n","RightAnswer":["A,","E"],"QuestionChoose":[]},{"QuestionNumber":9,"QuestionContent":" \nAn ecommerce company is running a multi-tier application on AWS. The front-end and backend tiers run on \nAmazon EC2, and the database runs on Amazon RDS for MYSQL. The backend tier communities with the \nRDS instance. There are frequent calls to return identical database from the database that are causing \nperformance slowdowns. \n \nWhich action should be taken to improve the performance of the backend? \n ","Option":["A. Implement Amazon SNS to store the database calls. ","B. Implement Amazon ElasticCache to cache the large database. ","C. Implement an RDS for MySQL read replica to cache database calls. ","D. Implement Amazon Kinesis Data Firehose to stream the calls to the database. "],"Explanation":"Answer: B \n \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":10,"QuestionContent":" \nA company selves a dynamic website from a flee! of Amazon EC2 instances behind an Application Load \nBalancer (ALB) The website needs to support multiple languages to serve customers around the world The \nwebsite\u0027s architecture is running in the us-west-1 Region and is exhibiting high request latency tor users that \nare located in other parts of the world \n \nThe website needs to serve requests quickly and efficiently regardless of a user\u0027s location However the \ncompany does not want to recreate the existing architecture across multiple Regions \n \nWhat should a solutions architect do to meet these requirements? \n ","Option":["A. Replace the existing architecture with a website that is served from an Amazon S3 bucket Configure an \nAmazon CloudFront distribution with the S3 bucket as the origin Set the cache behavior settings to \n122 of 230 Practice Test Amazon Web Services - SAA-C03 \ncache based on the Accept-Language request header ","B. Configure an Amazon CloudFront distribution with the ALB as the origin Set the cache behavior \nsettings to cache based on the Accept-Language request header ","C. Create an Amazon API Gateway API that is integrated with the ALB Configure the API to use the \nHTTP integration type Set up an API Gateway stage to enable the API cache based on the \nAccept-Language request header ","D. Launch an EC2 instance in each additional Region and configure NGINX to act as a cache server for \nthat Region Put all the EC2 instances and the ALB behind an Amazon Route 53 record set with a \ngeolocation routing policy "],"Explanation":"Answer: B \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":11,"QuestionContent":" \nAn ecommerce company stores terabytes of customer data in the AWS Cloud. The data contains personally \nidentifiable information (Pll). The company wants to use the \n \ndata in three applications. Only one of the applications needs to process the Pll. The Pll must be removed \nbefore the other two applications process the data. \n \nWhich solution will meet these requirements with the LEAST operational overhead? \n ","Option":["A. Store the data in an Amazon DynamoDB table. Create a proxy application layer to intercept and process \nthe data that each application requests. ","B. Store the data in an Amazon S3 bucket. Process and transform the data by using S3 Object Lambda \nbefore returning the data to the requesting application. ","C. Process the data and store the transformed data in three separate Amazon S3 buckets so that each \napplication has its own custom dataset. Point each application to its respective S3 bucket. ","D. Process the data and store the transformed data in three separate Amazon DynamoDB tables so that each \napplication has its own custom dataset. Point each application to its respective DynamoDB table. "],"Explanation":"Answer: B \n \nExplanation \nhttps://aws.amazon.com/blogs/aws/introducing-amazon-s3-object-lambda-use-your-code-to-process-data-as-it-is \nS3 Object Lambda is a new feature of Amazon S3 that enables customers to add their own code to process \ndata retrieved from S3 before returning it to the application. By using S3 Object Lambda, the data can be \nprocessed and transformed in real-time, without the need to store multiple copies of the data in separate S3 \nbuckets or DynamoDB tables. \n \nIn this case, the Pll can be removed from the data by the code added to S3 Object Lambda before returning the \ndata to the two applications that do not need to process Pll. The one application that requires Pll can be pointed \n \n123 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nto the original S3 bucket where the Pll is still stored. \n \nUsing S3 Object Lambda is the simplest and most cost-effective solution, as it eliminates the need to maintain \nmultiple copies of the same data in different buckets or tables, which can result in additional storage costs and \noperational overhead. \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":12,"QuestionContent":" \nA company has deployed a Java Spring Boot application as a pod that runs on Amazon Elastic Kubernetes \nService (Amazon EKS) in private subnets. The application needs to write data to an Amazon DynamoDB \ntable. A solutions architect must ensure that the application can interact with the DynamoDB table without \nexposing traffic to the internet. \n \nWhich combination of steps should the solutions architect take to accomplish this goal? (Choose two.) \n ","Option":["A. Attach an IAM role that has sufficient privileges to the EKS pod. ","B. Attach an IAM user that has sufficient privileges to the EKS pod. ","C. Allow outbound connectivity to the DynamoDB table through the private subnets\u2019 network ACLs. ","D. Create a VPC endpoint for DynamoDB. ","E. Embed the access keys in the Java Spring Boot code. "],"Explanation":"Answer: A D \n \nExplanation \nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html \nhttps://aws.amazon.com/about-aws/whats-new/2019/09/amazon-eks-adds-support-to-assign-iam-permissions-to- \n","RightAnswer":["A","D"],"QuestionChoose":[]},{"QuestionNumber":13,"QuestionContent":"A company runs an application on Amazon EC2 Linux instances across multiple Availability Zones. The \napplication needs a storage layer that is highly available and Portable Operating System Interface (POSIX) \ncompliant. The storage layer must provide maximum data durability and must be shareable across the EC2 \ninstances. The data in the storage layer will be accessed frequency for the first 30 days and will be accessed \ninfrequently alter that time. \n \nWhich solution will meet these requirements MOST cost-effectively? \n ","Option":["A. Use the Amazon S3 Standard storage class Create an S3 Lifecycle policy to move infrequently accessed \ndata to S3 Glacier ","B. Use the Amazon S3 Standard storage class. Create an S3 Lifecycle policy to move infrequently accessed \ndata to S3 Standard-Infrequent Access (EF3 Standard-IA). ","C. Use the Amazon Elastic File System (Amazon EFS) Standard storage class. Create a Lifecycle \n124 of 230 Practice Test Amazon Web Services - SAA-C03 \nmanagement policy to move infrequently accessed data to EFS Standard-Infrequent Access (EFS \nStandard-IA) ","D. Use the Amazon Elastic File System (Amazon EFS) One Zone storage class. Create a Lifecycle \nmanagement policy to move infrequently accessed data to EFS One Zone-Infrequent Access (EFS One \nZone-IA). "],"Explanation":"Answer: C \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":14,"QuestionContent":"A company hosts a multi-tier web application that uses an Amazon Aurora MySQL DB cluster for storage. \nThe application tier is hosted on Amazon EC2 instances. The company\u0027s IT security guidelines mandate that \nthe database credentials be encrypted and rotated every 14 days \n \nWhat should a solutions architect do to meet this requirement with the LEAST operational effort? \n ","Option":["A. Create a new AWS Key Management Service (AWS KMS) encryption key Use AWS Secrets Manager \nto create a new \nsecret that uses the KMS key with the appropriate credentials Associate the secret with the Aurora DB \ncluster Configure a custom rotation period of 14 days ","B. Create two parameters in AWS Systems Manager Parameter Store one for the user name as a string \nparameter and one that uses the SecureStnng type for the password Select AWS Key Management \nService (AWS KMS) encryption for the password parameter, and load these parameters in the \napplication tier Implement an AWS Lambda function that rotates the password every 14 days. ","C. Store a file that contains the credentials in an AWS Key Management Service (AWS KMS) encrypted \nAmazon Elastic File System (Amazon EFS) file system Mount the EFS file system in all EC2 instances \nof the application tier. Restrict the access to the file on the file system so that the application can read \nthe file and that only super users can modify the file Implement an AWS Lambda function that rotates \nthe key in Aurora every 14 days and writes new credentials into the file ","D. Store a file that contains the credentials in an AWS Key Management Service (AWS KMS) encrypted \nAmazon S3 bucket that the application uses to load the credentials Download the file to the application \nregularly to ensure that the correct credentials are used Implement an AWS Lambda function that rotates \nthe Aurora credentials every 14 days and uploads these credentials to the file in the S3 bucket "],"Explanation":"Answer: A \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":15,"QuestionContent":" \nAn application runs on Amazon EC2 instances in private subnets. The application needs to access an Amazon \nDynamoDB table. What is the MOST secure way to access the table while ensuring that the traffic does not \nleave the AWS network? \n ","Option":["A. Use a VPC endpoint for DynamoDB. \n125 of 230 Practice Test Amazon Web Services - SAA-C03 ","B. Use a NAT gateway in a public subnet. ","C. Use a NAT instance in a private subnet. ","D. Use the internet gateway attached to the VPC. "],"Explanation":"Answer: A \n \nExplanation \nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html \n \nA VPC endpoint for DynamoDB enables Amazon EC2 instances in your VPC to use their private IP addresses \nto access DynamoDB with no exposure to the public internet. Your EC2 instances do not require public IP \naddresses, and you don\u0027t need an internet gateway, a NAT device, or a virtual private gateway in your VPC. \nYou use endpoint policies to control access to DynamoDB. Traffic between your VPC and the AWS service \ndoes not leave the Amazon network. \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":16,"QuestionContent":" \nA telemarketing company is designing its customer call center functionality on AWS. The company needs a \nsolution that provides multiples speaker recognition and generates transcript files The company wants to query \nthe transcript files to analyze the business patterns The transcript files must be stored for 7 years for auditing \npiloses. \n \nWhich solution will meet these requirements? \n ","Option":["A. Use Amazon Recognition for multiple speaker recognition. Store the transcript files in Amazon S3 Use \nmachine teaming models for transcript file analysis ","B. Use Amazon Transcribe for multiple speaker recognition. Use Amazon Athena tot transcript file \nanalysts ","C. Use Amazon Translate lor multiple speaker recognition. Store the transcript files in Amazon Redshift \nUse SQL queues lor transcript file analysis ","D. Use Amazon Recognition for multiple speaker recognition. Store the transcript files in Amazon S3 Use \nAmazon Textract for transcript file analysis "],"Explanation":"Answer: C \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":17,"QuestionContent":" \nA company has a multi-tier application deployed on several Amazon EC2 instances in an Auto Scaling group. \nAn Amazon RDS for Oracle instance is the application\u2019s data layer that uses Oracle-specific \n \nPL/SQL functions. Traffic to the application has been steadily increasing. This is causing the EC2 instances to \nbecome overloaded and the RDS instance to run out of storage. The Auto Scaling group does not have any \nscaling metrics and defines the minimum healthy instance count only. The company predicts that traffic will \n \n126 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \ncontinue to increase at a steady but unpredictable rate before levelling off. \n \nWhat should a solutions architect do to ensure the system can automatically scale for the increased traffic? \n(Select TWO.) \n ","Option":["A. Configure storage Auto Scaling on the RDS for Oracle Instance. ","B. Migrate the database to Amazon Aurora to use Auto Scaling storage. ","C. Configure an alarm on the RDS for Oracle Instance for low free storage space ","D. Configure the Auto Scaling group to use the average CPU as the scaling metric ","E. Configure the Auto Scaling group to use the average free memory as the seeing metric "],"Explanation":"Answer: A C \n \n \n","RightAnswer":["A","C"],"QuestionChoose":[]},{"QuestionNumber":18,"QuestionContent":" \nA company wants to configure its Amazon CloudFront distribution to use SSL/TLS certificates. The company \ndoes not want to use the default domain name for the distribution. Instead, the company wants to use a \ndifferent domain name for the distribution. \n \nWhich solution will deploy the certificate with icurring any additional costs? \n ","Option":["A. Request an Amazon issued private certificate from AWS Certificate Manager (ACM) in the us-east-1 \nRegion ","B. Request an Amazon issued private certificate from AWS Certificate Manager (ACM) in the us-west-1 \nRegion. ","C. Request an Amazon issued public certificate from AWS Certificate Manager (ACU) in the us-east-1 \nRegion ","D. Request an Amazon issued public certificate from AWS Certificate Manager (ACU) in the us-west-1 \nRegon. "],"Explanation":"Answer: B \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":19,"QuestionContent":" \nA company\u0027s application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The \ninstances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. On the first day of \nevery month at midnight. The application becomes much slower when the month-end financial calcualtion \nbath runs. This causes the CPU utilization of the EC2 instaces to immediately peak to 100%, which disrupts \nthe application. \n \nWhat should a solution architect recommend to ensure the application is able to handle the workload and avoid \n \n127 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \ndowntime? \n ","Option":["A. Configure an Amazon CloudFront distribution in from of the ALB. ","B. Configure an EC2 Auto Scaling simple scaling policy based on CPU utilization. ","C. Configure an EC2 Auto Scaling scheduled scaling policy based on the monthly schedule. ","D. Configure Amazon ElasticCache to remove some of the workload from tha EC2 instances. "],"Explanation":"Answer: A \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":20,"QuestionContent":" \nA company\u2019s security team requests that network traffic be captured in VPC Flow Logs. The logs will be \nfrequently accessed for 90 days and then accessed intermittently. \n \nWhat should a solutions architect do to meet these requirements when configuring the logs? \n ","Option":["A. Use Amazon CloudWatch as the target. Set the CloudWatch log group with an expiration of 90 days ","B. Use Amazon Kinesis as the target. Configure the Kinesis stream to always retain the logs for 90 days. ","C. Use AWS CloudTrail as the target. Configure CloudTrail to save to an Amazon S3 bucket, and enable \nS3 Intelligent-Tiering. ","D. Use Amazon S3 as the target. Enable an S3 Lifecycle policy to transition the logs to S3 \nStandard-Infrequent Access (S3 Standard-IA) after 90 days. "],"Explanation":"Answer: D \n \nExplanation \nThere\u0027s a table here that specifies that VPC Flow logs can go directly to S3. Does not need to go via \nCloudTrail and then to S3. Nor via CW. \nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AWS-logs-and-resource-policy.html#AWS-logs-i \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":21,"QuestionContent":" \nA company wants to use high performance computing (HPC) infrastructure on AWS for financial risk \nmodeling. The company\u0027s HPC workloads run on Linux. Each HPC workflow runs on hundreds of Amazon \nEC2 Spot Instances, is shorl-lived, and generates thousands of output files that are ultimately stored in \npersistent storage for analytics and long-term future use. \n \nThe company seeks a cloud storage solution that permits the copying of on-premises data to long-term \npersistent storage to make data available for processing by all EC2 instances. The solution should also be a \nhigh performance file system that is integrated with persistent storage to read and write datasets and output \nfiles. \n \n128 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nWhich combination of AWS services meets these requirements? \n ","Option":["A. Amazon FSx for Lustre integrated with Amazon S3 ","B. Amazon FSx for Windows File Server integrated with Amazon S3 ","C. Amazon S3 Glacier integrated with Amazon Elastic Block Store (Amazon EBS) ","D. Amazon S3 bucket with a VPC endpoint integrated with an Amazon Elastic Block Store (Amazon EBS) \nGeneral Purpose SSD (gp2) volume "],"Explanation":"Answer: A \n \nExplanation \nhttps://aws.amazon.com/fsx/lustre/ \n \nAmazon FSx for Lustre is a fully managed service that provides cost-effective, high-performance, scalable \nstorage for compute workloads. Many workloads such as machine learning, high performance computing \n(HPC), video rendering, and financial simulations depend on compute instances accessing the same set of data \nthrough high-performance shared storage. \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":22,"QuestionContent":" \nAn ecommerce company has noticed performance degradation of its Amazon RDS based web application. The \nperformance degradation is attributed to an increase in the number of read-only SQL queries triggered by \nbusiness analysts. A solutions architect needs to solve the problem with minimal changes to the existing web \napplication. \n \nWhat should the solutions architect recommend? \n ","Option":["A. Export the data to Amazon DynamoDB and have the business analysts run their queries. ","B. Load the data into Amazon ElastiCache and have the business analysts run their queries. ","C. Create a read replica of the primary database and have the business analysts run their queries. ","D. Copy the data into an Amazon Redshift cluster and have the business analysts run their queries "],"Explanation":"Answer: C \n \nExplanation \nCreating a read replica of the primary RDS database will offload the read-only SQL queries from the primary \ndatabase, which will help to improve the performance of the web application. Read replicas are exact copies of \nthe primary database that can be used to handle read-only traffic, which will reduce the load on the primary \ndatabase and improve the performance of the web application. This solution can be implemented with minimal \nchanges to the existing web application, as the business analysts can continue to run their queries on the read \nreplica without modifying the code. \n \n129 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":23,"QuestionContent":" \nA solution architect is designing a company\u2019s disaster recovery (DR) architecture. The company has a MySQL \ndatabase that runs on an Amazon EC2 instance in a private subnet with scheduled backup. The DR design to \ninclude multiple AWS Regions. \n \nWhich solution will meet these requiements with the LEAST operational overhead? \n ","Option":["A. Migrate the MySQL database to multiple EC2 instances. Configure a standby EC2 instance in the DR \nRegion Turn on replication. ","B. Migrate the MySQL database to Amazon RDS. Use a Multi-AZ deployment. Turn on read replication \nfor the primary DB instance in the different Availability Zones. ","C. Migrate the MySQL database to an Amazon Aurora global database. Host the primary DB cluster in the \nprimary Region. Host the secondary DB cluster in the DR Region. ","D. Store the schedule backup of the MySQL database in an Amazon S3 bucket that is configured for S3 \nCross-Region Replication (CRR). Use the data backup to restore the database in the DR Region. "],"Explanation":"Answer: C \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":24,"QuestionContent":" \nA company has a large dataset for its online advertising business stored in an Amazon RDS for MySQL DB \ninstance in a single Availability Zone. The company wants business reporting queries to run without impacting \nthe write operations to the production DB instance. \n \nWhich solution meets these requirements? \n ","Option":["A. Deploy RDS read replicas to process the business reporting queries. ","B. Scale out the DB instance horizontally by placing it behind an Elastic Load Balancer ","C. Scale up the DB instance to a larger instance type to handle write operations and queries ","D. Deploy the OB distance in multiple Availability Zones to process the business reporting queries "],"Explanation":"Answer: C \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":25,"QuestionContent":" \nA company hostss a three application on Amazon EC2 instances in a single Availability Zone. The web \napplication uses a self-managed MySQL database that is hosted on an EC2 instances to store data in an \nAmazon Elastic Block Store (Amazon EBS) volumn. The MySQL database currently uses a 1 TB Provisioned \nIOPS SSD (io2) EBS volume. The company expects traffic of 1,000 IOPS for both reads and writes at peak \ntraffic. \n \n130 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nThe company wants to minimize any distruptions, stabilize perperformace, and reduce costs while retaining \nthe capacity for double the IOPS. The company wants to more the database tier to a fully managed solution \nthat is highly available and fault tolerant. \n \nWhich solution will meet these requirements MOST cost-effectively? \n ","Option":["A. Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with an io2 Block Express \nEBS volume. ","B. Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with a General Purpose SSD \n(gp2) EBS volume. ","C. Use Amazon S3 Intelligent-Tiering access tiers. ","D. Use two large EC2 instances to host the database in active-passive mode. "],"Explanation":"Answer: A \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":26,"QuestionContent":" \nA company s order system sends requests from clients to Amazon EC2 instances The EC2 instances process \nthe orders and men store the orders in a database on Amazon RDS Users report that they must reprocess orders \nwhen the system fails. The company wants a resilient solution that can process orders automatically it a system \noutage occurs. \n \nWhat should a solutions architect do to meet these requirements? \n ","Option":["A. Move (he EC2 Instances into an Auto Scaling group Create an Amazon EventBridge (Amazon \nCloudWatch Events) rule to target an Amazon Elastic Container Service (Amazon ECS) task ","B. Move the EC2 instances into an Auto Scaling group behind an Application Load Balancer (ALB) \nUpdate the order system to send messages to the ALB endpoint. ","C. Move the EC2 instances into an Auto Scaling group Configure the order system to send messages to an \nAmazon Simple Queue Service (Amazon SQS) queue Configure the EC2 instances to consume \nmessages from the queue ","D. Create an Amazon Simple Notification Service (Amazon SNS) topic Create an AWS Lambda function, \nand subscribe the function to the SNS topic Configure the order system to send messages to the SNS \ntopic Send a command to the EC2 instances to process the messages by using AWS Systems Manager \nRun Command "],"Explanation":"Answer: C \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":27,"QuestionContent":" \nA company runs a public three-Tier web application in a VPC The application runs on Amazon EC2 instances \nacross multiple Availability Zones. The EC2 instances that run in private subnets need to communicate with a \nlicense server over the internet The company needs a managed solution that minimizes operational \n \n131 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nmaintenance \n \nWhich solution meets these requirements\u0027\u0027 \n ","Option":["A. Provision a NAT instance in a public subnet Modify each private subnets route table with a default route \nthat points to the NAT instance ","B. Provision a NAT instance in a private subnet Modify each private subnet\u0027s route table with a default \nroute that points to the NAT instance ","C. Provision a NAT gateway in a public subnet Modify each private subnet\u0027s route table with a default \nroute that points to the NAT gateway ","D. Provision a NAT gateway in a private subnet Modify each private subnet\u0027s route table with a default \nroute that points to the NAT gateway . "],"Explanation":"Answer: C \n \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":28,"QuestionContent":" \nA company runs a web application on Amazon EC2 instances in multiple Availability Zones. The EC2 \ninstances are in private subnets. A solutions architect implements an internet-facing Application Load \nBalancer (ALB) and specifies the EC2 instances as the target group. However, the internet traffic is not \nreaching the EC2 instances. \n \nHow should the solutions architect reconfigure the architecture to resolve this issue? \n ","Option":["A. Replace the ALB with a Network Load Balancer. Configure a NAT gateway in a public subnet to allow \ninternet traffic. ","B. Move the EC2 instances to public subnets. Add a rule to the EC2 instances\u2019 security groups to allow \noutbound traffic to 0.0.0.0/0. ","C. Update the route tables for the EC2 instances\u2019 subnets to send 0.0.0.0/0 traffic through the internet \ngateway route. Add a rule to the EC2 instances\u2019 security groups to allow outbound traffic to 0.0.0.0/0. ","D. Create public subnets in each Availability Zone. Associate the public subnets with the ALB. Update the \nroute tables for the public subnets with a route to the private subnets. "],"Explanation":"Answer: D \n \nExplanation \nhttps://aws.amazon.com/premiumsupport/knowledge-center/public-load-balancer-private-ec2/ \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":29,"QuestionContent":"","Option":["A. Create a DB instance security group that allows all traffic from the public IP address of the application \nserver in VPC A. ","B. Configure a VPC peering connection between VPC A and VPC B. ","C. Make the DB instance publicly accessible. Assign a public IP address to the DB instance. ","D. Launch an EC2 instance with an Elastic IP address into VPC B. Proxy all requests through the new EC2 \ninstance. "],"Explanation":"Answer: B \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":30,"QuestionContent":" \nA company has hundreds of Amazon EC2 Linux-based instances in the AWS Cloud. Systems administrators \nhave used shared SSH keys to manage the instances After a recent audit, the company\u0027s security team is \nmandating the removal of all shared keys. A solutions architect must design a solution that provides secure \naccess to the EC2 instances. \n \nWhich solution will meet this requirement with the LEAST amount of administrative overhead? \n ","Option":["A. Use AWS Systems Manager Session Manager to connect to the EC2 instances. ","B. Use AWS Security Token Service (AWS STS) to generate one-time SSH keys on demand. ","C. Allow shared SSH access to a set of bastion instances. Configure all other instances to allow only SSH \naccess from the bastion instances ","D. Use an Amazon Cognito custom authorizer to authenticate users. Invoke an AWS Lambda function to \ngenerate a temporary SSH key. "],"Explanation":"Answer: B \n \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":31,"QuestionContent":" \nA solutions architect has created two IAM policies: Policy1 and Policy2. Both policies are attached to an IAM \ngroup. \n \n133 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nA cloud engineer is added as an IAM user to the IAM group. Which action will the cloud engineer be able to \nperform? \n ","Option":["A. Deleting IAM users ","B. Deleting directories ","C. Deleting Amazon EC2 instances ","D. Deleting logs from Amazon CloudWatch Logs "],"Explanation":"Answer: C \n \n134 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nExplanation \nhttps://awscli.amazonaws.com/v2/documentation/api/latest/reference/ds/index.html \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":32,"QuestionContent":"A media company hosts its website on AWS. The website application\u0027s architecture includes a fleet of \nAmazon EC2 instances behind an Application Load Balancer (ALB) and a database that is hosted on Amazon \nAurora The company\u0027s cyber security teem reports that the application is vulnerable to SOL injection. \nHow should the company resolve this issue? ","Option":["A. Use AWS WAF in front of the ALB Associate the appropriate web ACLs with AWS WAF. ","B. Create an ALB listener rule to reply to SQL injection with a fixed response ","C. Subscribe to AWS Shield Advanced to block all SQL injection attempts automatically. ","D. Set up Amazon Inspector to block all SOL injection attempts automatically "],"Explanation":"Answer: A \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":33,"QuestionContent":" \nA company previously migrated its data warehouse solution to AWS. The company also has an AWS Direct \nConnect connection. Corporate office users query the data warehouse using a visualization tool. The average \nsize of a query returned by the data warehouse is 50 MB and each webpage sent by the visualization tool is \napproximately 500 KB. Result sets returned by the data warehouse are not cached. \n \nWhich solution provides the LOWEST data transfer egress cost for the company? \n ","Option":["A. Host the visualization tool on premises and query the data warehouse directly over the internet. ","B. Host the visualization tool in the same AWS Region as the data warehouse. Access it over the internet. ","C. Host the visualization tool on premises and query the data warehouse directly over a Direct Connect \nconnection at a location in the same AWS Region. ","D. Host the visualization tool in the same AWS Region as the data warehouse and access it over a Direct \nConnect connection at a location in the same Region. "],"Explanation":"Answer: D \n \nExplanation \nhttps://aws.amazon.com/directconnect/pricing/ \nhttps://aws.amazon.com/blogs/aws/aws-data-transfer-prices-reduced/ \n \n135 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":34,"QuestionContent":" \nA company is hosting a web application from an Amazon S3 bucket. The application uses Amazon Cognito as \nan identity provider lo authenticate users and return a JSON Web Token (JWT) that provides access to \nprotected resources that am restored in another S3 bucket. \n \nUpon deployment of the application, users report errors and are unable to access the protected content. A \nsolutions architect must resolve this issue by providing proper permissions so that users can access the \nprotected content. \n \nWhich solution meets these requirements? \n ","Option":["A. Update the Amazon Cognito identity pool to assume the proper IAM role for access to the protected \nconsent. ","B. Update the S3 ACL to allow the application to access the protected content ","C. Redeploy the application to Amazon 33 to prevent eventually consistent reads m the S3 bucket from \naffecting the ability of users to access the protected content. ","D. Update the Amazon Cognito pool to use custom attribute mappings within tie Identity pool and grant \nusers the proper permissions to access the protected content "],"Explanation":"Answer: B \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":35,"QuestionContent":" \nA solution architect must create a disaster recovery (DR) plan for a high-volume software as a service (SaaS) \nplatform. All data for the platform is stored in an Amazon Aurora MySQL DB cluster. \n \nThe DR plan must replcate data to a secondary AWS Region. \n \nWhich solution will meet these requirements MOST cost-effectively? \nUse MySQL binary log replication to an Aurora cluster ","Option":["A. Use MySQL binary log replication to an Aurora cluster in the secondary Region Provision one DB \ninstance for the Aurora cluster in the secondary Region. ","B. Set up an Aurora global database for the DB cluster. When setup is complete, remove the DB instance \nfrom the secondary Region. ","C. Use AWS Database Migration Service (AWS QMS) to continuously replicate data to an Aurora cluster \nin the secondary Region Remove theDB instance from the secondary Region. ","D. Set up an Aurora global database for the DB cluster Specify a minimum of one DB instance in the \nsecondary Region "],"Explanation":"Answer: D \n \n136 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":36,"QuestionContent":" \nA company wants to implement a disaster recovery plan for its primary on-premises file storage volume. The \nfile storage volume is mounted from an Internet Small Computer Systems Interface (iSCSI) device on a local \nstorage server. The file storage volume holds hundreds of terabytes (TB) of data. \n \nThe company wants to ensure that end users retain immediate access to all file types from the on-premises \nsystems without experiencing latency. \n \nWhich solution will meet these requirements with the LEAST amount of change to the company\u0027s existing \ninfrastructure? \n ","Option":["A. Provision an Amazon S3 File Gateway as a virtual machine (VM) that is hosted on premises. Set the \nlocal cache to 10 TB. Modify existing applications to access the files through the NFS protocol. To \nrecover from a disaster, provision an Amazon EC2 instance and mount the S3 bucket that contains the \nfiles. ","B. Provision an AWS Storage Gateway tape gateway. Use a data backup solution to back up all existing \ndata to a virtual tape library. Configure the data backup solution to run nightly after the initial backup is \ncomplete. To recover from a disaster, provision an Amazon EC2 instance and restore the data to an \nAmazon Elastic Block Store (Amazon EBS) volume from the volumes in the virtual tape library. ","C. Provision an AWS Storage Gateway Volume Gateway cached volume. Set the local cache to 10 TB. \nMount the Volume Gateway cached volume to the existing file server by using iSCSI. and copy all files \nto the storage volume. Configure scheduled snapshots of the storage volume. To recover from a disaster, \nrestore a snapshot to an Amazon Elastic Block Store (Amazon EBS) volume and attach the EBS volume \nto an Amazon EC2 instance. ","D. Provision an AWS Storage Gateway Volume Gateway stored volume with the same amount of disk \nspace as the existing file storage volume. Mount the Volume Gateway stored volume to the existing file \nserver by using iSCSI, and copy all files to the storage volume. Configure scheduled snapshots of the \nstorage volume. To recover from a disaster, restore a snapshot to an Amazon Elastic Block Store \n(Amazon EBS) volume and attach the EBS volume to an Amazon EC2 instance. "],"Explanation":"Answer: C \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":37,"QuestionContent":" \nA company uses Amazon API Gateway to run a private gateway with two REST APIs in the same VPC. The \nBuyStock RESTful web service calls the CheckFunds RESTful \n \nweb service to ensure that enough funds are available before a stock can be purchased. The company has \nnoticed in the VPC flow logs that the BuyStock RESTful web \n \nservice calls the CheckFunds RESTful web service over the internet instead of through the VPC. A solutions \narchitect must implement a solution so that the APIs \n \ncommunicate through the VPC. \n \n137 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nWhich solution will meet these requirements with the FEWEST changes to the code? \n \n(Select Correct Option/s and give detailed explanation from AWS Certified Solutions Architect - Associate \n(SAA-C03) Study Manual or documents) \n ","Option":["A. Add an X-APl-Key header in the HTTP header for authorization. ","B. Use an interface endpoint. ","C. Use a gateway endpoint. ","D. Add an Amazon Simple Queue Service (Amazon SQS) queue between the two REST APIs. "],"Explanation":"Answer: B \n \nExplanation \nUsing an interface endpoint will allow the BuyStock RESTful web service and the CheckFunds RESTful web \nservice to communicate through the VPC without any changes to the code. An interface endpoint creates an \nelastic network interface (ENI) in the customer\u0027s VPC, and then configures the route tables to route traffic \nfrom the APIs to the ENI. This will ensure that the two APIs will communicate through the VPC without any \nchanges to the code. \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":38,"QuestionContent":" \nA company wants to migrate a Windows-based application from on premises to the AWS Cloud. The \napplication has three tiers, a business tier, and a database tier with Microsoft SQL Server. The company wants \nto use specific features of SQL Server such as native backups and Data Quality Services. The company also \nneeds to share files for process between the tiers. \n \nHow should a solution architect design the architecture to meet these requirements? \n ","Option":["A. Host all three on Amazon instances. Use Mmazon FSx File Gateway for file sharing between tiers. ","B. Host all three on Amazon EC2 instances. Use Amazon FSx for Windows file sharing between the tiers. ","C. Host the application tier and the business tier on Amazon EC2 instances. Host the database tier on \nAmazon RDS. Use Amazon Elastic File system (Amazon EFS) for file sharing between the tiers. ","D. Host the application tier and the business tier on Amazon EC2 instances. Host the database tier on \nAmazon RDS. Use a Provisioned IOPS SSD (io2) Amazon Elastic Block Store (Amazon EBS) volume \nfor file sharing between the tiers. "],"Explanation":"Answer: B \n \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":39,"QuestionContent":"A company needs to transfer 600 TB of data from its on-premises network-attached storage (NAS) system to \n \n138 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nthe AWS Cloud. The data transfer must be complete within 2 weeks. The data is sensitive and must be \nencrypted in transit. The company\u0027s internet connection can support an upload speed of 100 Mbps. \n \nWhich solution meets these requirements MOST cost-effectively? \n ","Option":["A. Use Amazon S3 multi-part upload functionality to transfer the fees over HTTPS ","B. Create a VPN connection between the on-premises NAS system and the nearest AWS Region Transfer \nthe data over the VPN connection ","C. Use the AWS Snow Family console to order several AWS Snowball Edge Storage Optimized devices \nUse the devices to transfer the data to Amazon S3. ","D. Set up a 10 Gbps AWS Direct Connect connection between the company location and (he nearest AWS \nRegion Transfer the data over a VPN connection into the Region to store the data in Amazon S3 "],"Explanation":"Answer: D \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":40,"QuestionContent":" \nA solutions architect must secure a VPC network that hosts Amazon EC2 instances The EC2 ^stances contain \nhighly sensitive data and tun n a private subnet According to company policy the EC2 instances mat run m the \nVPC can access only approved third-party software repositories on the internet for software product updates \nthat use the third party\u0027s URL Other internet traffic must be blocked. \n \nWhich solution meets these requirements? \n ","Option":["A. Update the route table for the private subnet to route the outbound traffic to an AWS Network Firewall. \nConfigure domain list rule groups ","B. Set up an AWS WAF web ACL. Create a custom set of rules that filter traffic requests based on source \nand destination IP address range sets. ","C. Implement strict inbound security group roles Configure an outbound rule that allows traffic only to the \nauthorized software repositories on the internet by specifying the URLs ","D. Configure an Application Load Balancer (ALB) in front of the EC2 instances. Direct an outbound traffic \nto the ALB Use a URL-based rule listener in the ALB\u0027s target group for outbound access to the internet "],"Explanation":"Answer: C \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":41,"QuestionContent":" \nA company hosts a multiplayer gaming application on AWS. The company wants the application to read data \nwith sub-millisecond latency and run one-time queries on historical data. \n \nWhich solution will meet these requirements with the LEAST operational overhead? \n ","Option":["A. Use Amazon RDS for data that is frequently accessed. Run a periodic custom script to export the data to \n139 of 230 Practice Test Amazon Web Services - SAA-C03 \nan Amazon S3 bucket. ","B. Store the data directly in an Amazon S3 bucket. Implement an S3 Lifecycle policy to move older data to \nS3 Glacier Deep Archive for long-term storage. Run one-time queries on the data in Amazon S3 by \nusing Amazon Athena ","C. Use Amazon DynamoDB with DynamoDB Accelerator (DAX) for data that is frequently accessed. \nExport the data to an Amazon S3 bucket by using DynamoDB table export. Run one-time queries on the \ndata in Amazon S3 by using Amazon Athena. ","D. Use Amazon DynamoDB for data that is frequently accessed Turn on streaming to Amazon Kinesis \nData Streams. Use Amazon Kinesis Data Firehose to read the data from Kinesis Data Streams. Store the \nrecords in an Amazon S3 bucket. "],"Explanation":"Answer: C \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":42,"QuestionContent":" \nA company\u2019s compliance team needs to move its file shares to AWS. The shares run on a Windows Server \nSMB file share. A self-managed on-premises Active Directory controls access to the files and folders. \n \nThe company wants to use Amazon FSx for Windows File Server as part of the solution. The company must \nensure that the on-premises Active Directory groups restrict access to the FSx for Windows File Server SMB \ncompliance shares, folders, and files after the move to AWS. The company has created an FSx for Windows \nFile Server file system. \n \nWhich solution will meet these requirements? \n ","Option":["A. Create an Active Directory Connector to connect to the Active Directory. Map the Active Directory \ngroups to IAM groups to restrict access. ","B. Assign a tag with a Restrict tag key and a Compliance tag value. Map the Active Directory groups to \nIAM groups to restrict access. ","C. Create an IAM service-linked role that is linked directly to FSx for Windows File Server to restrict \naccess. ","D. Join the file system to the Active Directory to restrict access. "],"Explanation":"Answer: D \n \nExplanation \nJoining the FSx for Windows File Server file system to the on-premises Active Directory will allow the \ncompany to use the existing Active Directory groups to restrict access to the file shares, folders, and files after \nthe move to AWS. This option allows the company to continue using their existing access controls and \nmanagement structure, making the transition to AWS more seamless. \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":43,"QuestionContent":" \n140 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \n \nA meteorological startup company has a custom web application to sell weather data to its users online. The \ncompany uses Amazon DynamoDB to store is data and wants to bu4d a new service that sends an alert to the \nmanagers of four Internal teams every time a new weather event is recorded. The company does not want true \nnew service to affect the performance of the current application \n \nWhat should a solutions architect do to meet these requirement with the LEAST amount of operational \noverhead? \n ","Option":["A. Use DynamoDB transactions to write new event data to the table Configure the transactions to notify \ninternal teams. ","B. Have the current application publish a message to four Amazon Simple Notification Service (Amazon \nSNS) topics. Have each team subscribe to one topic. ","C. Enable Amazon DynamoDB Streams on the table. Use triggers to write to a mingle Amazon Simple \nNotification Service (Amazon SNS) topic to which the teams can subscribe. ","D. Add a custom attribute to each record to flag new items. Write a cron job that scans the table every \nminute for items that are new and notifies an Amazon Simple Queue Service (Amazon SOS) queue to \nwhich the teams can subscribe. "],"Explanation":"Answer: C \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":44,"QuestionContent":" \nA company is designing a cloud communications platform that is driven by APIs. The application is hosted on \nAmazon EC2 instances behind a Network Load Balancer (NLB). The company uses Amazon API Gateway to \nprovide external users with access to the application through APIs. The company wants to protect the platform \nagainst web exploits like SQL injection and also wants to detect and mitigate large, sophisticated DDoS \nattacks. \n \nWhich combination of solutions provides the MOST protection? (Select TWO.) \n ","Option":["A. Use AWS WAF to protect the NLB. ","B. Use AWS Shield Advanced with the NLB. ","C. Use AWS WAF to protect Amazon API Gateway. ","D. Use Amazon GuardDuty with AWS Shield Standard. ","E. Use AWS Shield Standard with Amazon API Gateway. "],"Explanation":"Answer: B C \n \nExplanation \nAWS Shield Advanced provides expanded DDoS attack protection for your Amazon EC2 instances, Elastic \nLoad Balancing load balancers, CloudFront distributions, Route 53 hosted zones, and AWS Global \n \n141 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nAccelerator standard accelerators. \n \nAWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are \nforwarded to your protected web application resources. You can protect the following resource types: \n \nAmazon CloudFront distribution \nAmazon API Gateway REST API \nApplication Load Balancer \nAWS AppSync GraphQL API \nAmazon Cognito user pool \nhttps://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html \n \n","RightAnswer":["B","C"],"QuestionChoose":[]},{"QuestionNumber":45,"QuestionContent":" \nA company offers a food delivery service that is growing rapidly. Because of the growth, the company\u2019s order \nprocessing system is experiencing scaling problems during peak traffic hours. The current architecture \nincludes the following: \n \n\u2022 A group of Amazon EC2 instances that run in an Amazon EC2 Auto Scaling group to collect orders from the \napplication \n \n\u2022 Another group of EC2 instances that run in an Amazon EC2 Auto Scaling group to fulfill orders \n \nThe order collection process occurs quickly, but the order fulfillment process can take longer. Data must not \nbe lost because of a scaling event. \n \nA solutions architect must ensure that the order collection process and the order fulfillment process can both \nscale properly during peak traffic hours. The solution must optimize utilization of the company\u2019s AWS \nresources. \n \nWhich solution meets these requirements? \n ","Option":["A. Use Amazon CloudWatch metrics to monitor the CPU of each instance in the Auto Scaling groups. \nConfigure each Auto Scaling group\u2019s minimum capacity according to peak workload values. ","B. Use Amazon CloudWatch metrics to monitor the CPU of each instance in the Auto Scaling groups. \nConfigure a CloudWatch alarm to invoke an Amazon Simple Notification Service (Amazon SNS) topic \nthat creates additional Auto Scaling groups on demand. ","C. Provision two Amazon Simple Queue Service (Amazon SQS) queues: one for order collection and \nanother for order fulfillment. Configure the EC2 instances to poll their respective queue. Scale the Auto \nScaling groups based on notifications that the queues send. ","D. Provision two Amazon Simple Queue Service (Amazon SQS) queues: one for order collection and \nanother for order fulfillment. Configure the EC2 instances to poll their respective queue. Create a metric \n142 of 230 Practice Test Amazon Web Services - SAA-C03 \nbased on a backlog per instance calculation. Scale the Auto Scaling groups based on this metric. "],"Explanation":"Answer: D \n \nExplanation \nThe number of instances in your Auto Scaling group can be driven by how long it takes to process a message \nand the acceptable amount of latency (queue delay). The solution is to use a backlog per instance metric with \nthe target value being the acceptable backlog per instance to maintain. \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":46,"QuestionContent":" \nA company hosts rts sialic website by using Amazon S3 The company wants to add a contact form to its \nwebpage The contact form will have dynamic server-sKle components for users to input their name, email \naddress, phone number and user message The company anticipates that there will be fewer than 100 site visits \neach month \n \nWhich solution will meet these requirements MOST cost-effectively? \n ","Option":["A. Host a dynamic contact form page in Amazon Elastic Container Service (Amazon ECS) Set up Amazon \nSimple Email Service (Amazon SES) to connect to any third-party email provider. ","B. Create an Amazon API Gateway endpoinl with an AWS Lambda backend that makes a call to Amazon \nSimple Email Service (Amazon SES) ","C. Convert the static webpage to dynamic by deploying Amazon Ughtsail Use client-side scnpting to build \nthe contact form Integrate the form with Amazon WorkMail ","D. Create a Q micro Amazon EC2 instance Deploy a LAMP (Linux Apache MySQL. PHP/Perl/Python) \nstack to host the webpage Use client-side scripting to buiW the contact form Integrate the form with \nAmazon WorkMail "],"Explanation":"Answer: D \n \nExplanation \nCreate a t2 micro Amazon EC2 instance. Deploy a LAMP (Linux Apache MySQL, PHP/Perl/Python) stack to \nhost the webpage. Use client-side scripting to build the contact form. Integrate the form with Amazon \nWorkMail. This solution will provide the company with the necessary components to host the contact form \npage and integrate it with Amazon WorkMail at the lowest cost. Option A requires the use of Amazon ECS, \nwhich is more expensive than EC2, and Option B requires the use of Amazon API Gateway, which is also \nmore expensive than EC2. Option C requires the use of Amazon Lightsail, which is more expensive than EC2. \n \nUsing AWS Lambda with Amazon API Gateway - AWS Lambda \nhttps://docs.aws.amazon.com/lambda/latest/dg/services-apigateway.html \nAWS Lambda FAQs \nhttps://aws.amazon.com/lambda/faqs/ \n \n143 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \n \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":47,"QuestionContent":" \nAn IAM user made several configuration changes to AWS resources m their company\u0027s account during a \nproduction deployment last week. A solutions architect learned that a couple of security group rules are not \nconfigured as desired. The solutions architect wants to confirm which IAM user was responsible for making \nchanges. \n \nWhich service should the solutions architect use to find the desired information? \n ","Option":["A. Amazon GuardDuty ","B. Amazon Inspector ","C. AWS CloudTrail ","D. AWS Config "],"Explanation":"Answer: C \nExplanation \nUESTION NO: 549 \nA company has a three-tier environment on AWS that ingests sensor data from its users\u0027 devices The traffic \nflows through a Network Load Balancer (NIB) then to Amazon EC2 instances for the web tier and finally to \nEC2 instances for the application tier that makes database calls \n \nWhat should a solutions architect do to improve the security of data in transit to the web tier? \n \nA. Configure a TLS listener and add the server certificate on the NLB \n \nB. Configure AWS Shield Advanced and enable AWS WAF on the NLB \n \nC. Change the load balancer to an Application Load Balancer and attach AWS WAF to it \n \nD. Encrypt the Amazon Elastic Block Store (Amazon EBS) volume on the EC2 instances using AWS Key \nManagement Service (AWS KMS) \n \nAnswer: A \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":48,"QuestionContent":" \nA solutions architect must migrate a Windows Internet Information Services (IIS) web application to AWS \nThe application currently relies on a file share hosted in the user\u0027s on-premises network-attached storage \n(NAS) The solutions architect has proposed migrating the MS web servers to Amazon EC2 instances in \nmultiple Availability Zones that are connected to the storage solution, and configuring an Elastic Load \nBalancer attached to the instances \n \n144 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nWhich replacement to the on-premises file share is MOST resilient and durable? \n ","Option":["A. Migrate the file share to Amazon RDS ","B. Migrate the file share to AWS Storage Gateway ","C. Migrate the file share to Amazon FSx for Windows File Server ","D. Migrate the file share to Amazon Elastic File System (Amazon EFS) "],"Explanation":"Answer: C \n \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":49,"QuestionContent":" \nA company runs an application that receives data from thousands of geographically dispersed remote devices \nthat use UDP The application processes the data immediately and sends a message back to the device if \nnecessary No data is stored. \n \nThe company needs a solution that minimizes latency for the data transmission from the devices. The solution \nalso must provide rapid failover to another AWS Region \n \nWhich solution will meet these requirements? \n ","Option":["A. Configure an Amazon Route 53 failover routing policy Create a Network Load Balancer (NLB) in each \nof the two Regions Configure the NLB to invoke an AWS Lambda function to process the data ","B. Use AWS Global Accelerator Create a Network Load Balancer (NLB) in each of the two Regions as an \nendpoint. Create an Amazon Elastic Container Service (Amazon ECS) cluster with the Fargate launch \ntype Create an ECS service on the cluster Set the ECS service as the target for the NLB Process the data \nin Amazon ECS. ","C. Use AWS Global Accelerator Create an Application Load Balancer (ALB) in each of the two Regions \nas an endpoint Create an Amazon Elastic Container Service (Amazon ECS) cluster with the Fargate \nlaunch type Create an ECS service on the cluster. Set the ECS service as the target for the ALB Process \nthe data in Amazon ECS ","D. Configure an Amazon Route 53 failover routing policy Create an Application Load Balancer (ALB) in \neach of the two Regions Create an Amazon Elastic Container Service (Amazon ECS) cluster with the \nFargate launch type Create an ECS service on the cluster Set the ECS service as the target for the ALB \nProcess the data in Amazon ECS "],"Explanation":"Answer: C \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":50,"QuestionContent":" \nA company stores its data objects in Amazon S3 Standard storage. A solutions architect has found that 75% of \nthe data is rarely accessed after 30 days. The company needs all the data to remain immediately accessible \nwith the same high availability and resiliency, but the company wants to minimize storage costs. \n \n145 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nWhich storage solution will meet these requirements? \n ","Option":["A. Move the data objects to S3 Glacier Deep Archive after 30 days. ","B. Move the data objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days. ","C. Move the data objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days. ","D. Move the data objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) immediately. "],"Explanation":"Answer: B \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":51,"QuestionContent":" \nA company is reviewing a recent migration of a three-tier application to a VPC. The security team discovers \nthat the principle of least privilege is not being applied to Amazon EC2 security group ingress and egress rules \nbetween the application tiers. \n \nWhat should a solutions architect do to correct this issue? \n ","Option":["A. Create security group rules using the instance ID as the source or destination. ","B. Create security group rules using the security group ID as the source or destination. ","C. Create security group rules using the VPC CIDR blocks as the source or destination. ","D. Create security group rules using the subnet CIDR blocks as the source or destination. "],"Explanation":"Answer: B \n \nExplanation \nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/security-group-rules.html \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":52,"QuestionContent":" \nA company manages its own Amazon EC2 instances that run MySQL databases. The company is manually \nmanaging replication and scaling as demand increases or decreases. The company needs a new solution that \nsimplifies the process of adding or removing compute capacity to or from its database tier as needed. The \nsolution also must offer improved performance, scaling, and durability with minimal effort from operations. \n \nWhich solution meets these requirements? \n ","Option":["A. Migrate the databases to Amazon Aurora Serverless for Aurora MySQL. ","B. Migrate the databases to Amazon Aurora Serverless for Aurora PostgreSQL. ","C. Combine the databases into one larger MySQL database. Run the larger database on larger EC2 \n146 of 230 Practice Test Amazon Web Services - SAA-C03 \ninstances. ","D. Create an EC2 Auto Scaling group for the database tier. Migrate the existing databases to the new \nenvironment. "],"Explanation":"Answer: A \n \nExplanation \nhttps://aws.amazon.com/rds/aurora/serverless/ \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":53,"QuestionContent":" \nA company is using Amazon Route 53 latency-based routing to route requests to its UDP-based application for \nusers around the world. The application is hosted on redundant servers in the company\u0027s on-premises data \ncenters in the United States. Asia, and Europe. The company\u0027s compliance requirements state that the \napplication must be hosted on premises The company wants to improve the performance and availability of the \napplication \n \nWhat should a solutions architect do to meet these requirements? \n ","Option":["A. A Configure three Network Load Balancers (NLBs) in the three AWS Regions to address the \non-premises endpoints Create an accelerator by using AWS Global Accelerator, and register the NLBs \nas its endpoints. Provide access to the application by using a CNAME that points to the accelerator DNS ","B. Configure three Application Load Balancers (ALBs) in the three AWS Regions to address the \non-premises endpoints. Create an accelerator by using AWS Global Accelerator and register the ALBs \nas its endpoints Provide access to the application by using a CNAME that points to the accelerator DNS ","C. Configure three Network Load Balancers (NLBs) in the three AWS Regions to address the on-premises \nendpoints In Route 53. create a latency-based record that points to the three NLBs. and use it as an \norigin for an Amazon CloudFront distribution Provide access to the application by using a CNAME that \npoints to the CloudFront DNS ","D. Configure three Application Load Balancers (ALBs) in the three AWS Regions to address the \non-premises endpoints In Route 53 create a latency-based record that points to the three ALBs and use it \nas an origin for an Amazon CloudFront distribution- Provide access to the application by using a \nCNAME that points to the CloudFront DNS "],"Explanation":"Answer: A \n \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":54,"QuestionContent":" \nA company has an on-premises MySQL database used by the global tales team with infrequent access patterns. \nThe sales team requires the database to have minimal downtime. A database administrate wants to migrate this \ndatabase to AWS without selecting a particular instance type in anticipation of more users In the future. \n \nWhich service should a solutions architect recommend? \n \n147 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n ","Option":["A. Amazon Aurora MySQL ","B. Amazon Aurora Serverless tor MySQL ","C. Amazon Redshift Spectrum ","D. Amazon RDS for MySQL "],"Explanation":"Answer: B \n \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":55,"QuestionContent":" \nA company needs a backup strategy for its three-tier stateless web application The web application runs on \nAmazon EC2 instances in an Auto Scaling group with a dynamic scaling policy that is configured to respond \nto scaling events The database tier runs on Amazon RDS for PostgreSQL The web application does not require \ntemporary local storage on the EC2 instances The company\u0027s recovery point objective (RPO) is 2 hours \n \nThe backup strategy must maximize scalability and optimize resource utilization for this environment \nWhich solution will meet these requirements? ","Option":["A. Take snapshots of Amazon Elastic Block Store (Amazon EBS) volumes of the EC2 instances and \ndatabase every 2 hours to meet the RPO ","B. Configure a snapshot lifecycle policy to take Amazon Elastic Block Store (Amazon EBS) snapshots \nEnable automated backups in Amazon RDS to meet the RPO ","C. Retain the latest Amazon Machine Images (AMIs) of the web and application tiers Enable automated \nbackups in Amazon RDS and use point-in-time recovery to meet the RPO ","D. Take snapshots of Amazon Elastic Block Store (Amazon EBS) volumes of the EC2 instances every 2 \nhours Enable automated backups in Amazon RDS and use point-in-time recovery to meet the RPO "],"Explanation":"Answer: D \n \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":56,"QuestionContent":" \nA company is planning to migrate a commercial off-the-shelf application from is on-premises data center to \nAWS. The software has a software licensing model using sockets and cores with predictable capacity and \nuptime requirements. The company wants to use its existing licenses, which were purchased earlier this year. \n \nWhich Amazon EC2 pricing option is the MOST cost-effective? \n ","Option":["A. Dedicated Reserved Hosts ","B. Dedicated On-Demand Hosts \n148 of 230 Practice Test Amazon Web Services - SAA-C03 ","C. Dedicated Reserved Instances ","D. Dedicated On-Oemand Instances "],"Explanation":"Answer: A \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":57,"QuestionContent":" \nA company wants to restrict access to the content of one of its man web applications and to protect the content \nby using authorization techniques available on AWS. The company wants to implement a serverless \narchitecture end an authentication solution for fewer tian 100 users. The solution needs to integrate with the \nmain web application and serve web content globally. The solution must also scale as to company\u0027s user base \ngrows while providing lowest login latency possible. \n ","Option":["A. Use Amazon Cognito tor authentication. Use Lambda#Edge tor authorization Use Amazon CloudFront \n10 serve the web application globally ","B. Use AWS Directory Service for Microsoft Active Directory tor authentication Use AWS Lambda for \nauthorization Use an Application Load Balancer to serve the web application globally ","C. Usa Amazon Cognito for authentication Use AWS Lambda tor authorization Use Amazon S3 Transfer \nAcceleration 10 serve the web application globally. ","D. Use AWS Directory Service for Microsoft Active Directory for authentication Use Lambda@Edge for \nauthorization Use AWS Elastic Beanstalk to serve the web application. "],"Explanation":"Answer: A \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":58,"QuestionContent":" \nA company needs to ingested and handle large amounts of streaming data that its application generates. The \napplication runs on Amazon EC2 instances and sends data to Amazon Kinesis Data Streams. which is \ncontained wild default settings. Every other day the application consumes the data and writes the data to an \nAmazon S3 bucket for business intelligence (BI) processing the company observes that Amazon S3 is not \nreceiving all the data that trio application sends to Kinesis Data Streams. \n \nWhat should a solutions architect do to resolve this issue? \n ","Option":["A. Update the Kinesis Data Streams default settings by modifying the data retention period. ","B. Update the application to use the Kinesis Producer Library (KPL) lo send the data to Kinesis Data \nStreams. ","C. Update the number of Kinesis shards lo handle the throughput of me data that is sent to Kinesis Data \nStreams. ","D. Turn on S3 Versioning within the S3 bucket to preserve every version of every object that is ingested in \nthe S3 bucket. \n149 of 230 Practice Test Amazon Web Services - SAA-C03 "],"Explanation":"Answer: A \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":59,"QuestionContent":" \nA rapidly growing global ecommerce company is hosting its web application on AWS. The web application \nincludes static content and dynamic content. The website stores online transaction processing (OLTP) data in \nan Amazon RDS database. The website\u2019s users are experiencing slow page loads. \n \nWhich combination of actions should a solutions architect take to resolve this issue? (Select TWO.) \n ","Option":["A. Configure an Amazon Redshift cluster. ","B. Set up an Amazon CloudFront distribution ","C. Host the dynamic web content in Amazon S3 ","D. Create a t wd replica tor the RDS DB instance. ","E. Configure a Multi-AZ deployment for the RDS DB instance "],"Explanation":"Answer: B D \n \n","RightAnswer":["B","D"],"QuestionChoose":[]},{"QuestionNumber":60,"QuestionContent":" \nA company hosts a frontend application that uses an Amazon API Gateway API backend that is integrated \nwith AWS Lambda When the API receives requests, the Lambda function loads many libranes Then the \nLambda function connects to an Amazon RDS database processes the data and returns the data to the frontend \napplication. The company wants to ensure that response latency is as low as possible for all its users with the \nfewest number of changes to the company\u0027s operations \n \nWhich solution will meet these requirements\u0027? \n ","Option":["A. Establish a connection between the frontend application and the database to make queries faster by \nbypassing the API ","B. Configure provisioned concurrency for the Lambda function that handles the requests ","C. Cache the results of the queries in Amazon S3 for faster retneval of similar datasets. ","D. Increase the size of the database to increase the number of connections Lambda can establish at one time "],"Explanation":"Answer: B \n \nExplanation \nConfigure provisioned concurrency for the Lambda function that handles the requests. Provisioned \nconcurrency allows you to set the amount of compute resources that are available to the Lambda function, so \nthat it can handle more requests at once and reduce latency. Caching the results of the queries in Amazon S3 \ncould also help to reduce latency, but it would not be as effective as setting up provisioned concurrency. \n \n150 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nIncreasing the size of the database would not help to reduce latency, as this would not increase the number of \nconnections the Lambda function could establish, and establishing a direct connection between the frontend \napplication and the database would bypass the API, which would not be the best solution either. \n \nUsing AWS Lambda with Amazon API Gateway - AWS Lambda \nhttps://docs.aws.amazon.com/lambda/latest/dg/services-apigateway.html \nAWS Lambda FAQs \nhttps://aws.amazon.com/lambda/faqs/ \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":61,"QuestionContent":" \nA company has a three-tier application on AWS that ingests sensor data from its users\u0027 devices The traffic \nflows through a Network Load Balancer (NLB) then to Amazon EC2 instances for the web tier and finally to \nEC2 instances for the application tier The application tier makes calls to a database \n \nWhat should a solutions architect do to improve the security of the data in transit? \n ","Option":["A. Configure a TLS listener Deploy the server certrficate on the NLB ","B. Configure AWS Shield Advanced Enable AWS WAF on the NLB ","C. Change the load balancer to an Application Load Balancer (ALB) Enable AWS WAF on the ALB ","D. Encrypt the Amazon Elastic Block Store (Amazon EBS) volume on the EC2 instances by using AWS \nKey Management Service (AWS KMS) "],"Explanation":"Answer: A \n \nExplanation \nThe best option to improve the security of the data in transit is to configure a TLS listener and deploy the \nserver certificate on the NLB. This will ensure that the data is encrypted and secure as it travels through the \nnetwork. Additionally, you could also configure AWS Shield Advanced and enable AWS WAF on the NLB to \nfurther protect the network from malicious attacks. Alternatively, you could also change the load balancer to \nan Application Load Balancer (ALB) and enable AWS WAF on the ALB. Finally, you could also encrypt the \nAmazon Elastic Block Store (Amazon EBS) volume on the EC2 instances by using AWS Key Management \nService (AWS KMS). \n \nYou must specify an SSL certificate for a TLS listener. The load balancer uses the certificate to terminate the \nconnection and decrypt requests from clients before routing them to targets. \nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/network/create-listener.html \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":62,"QuestionContent":" \nA company is migrating its on-premises workload to the AWS Cloud. The company already uses several \nAmazon EC2 instances and Amazon RDS DB instances. The company wants a solution that automatically \n \n151 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nstarts and stops the EC2 instances and D6 instances outside of business hours. The solution must minimize \ncost and infrastructure maintenance. \n \nWhich solution will meet these requirement? \n ","Option":["A. Scale the EC2 instances by using elastic resize Scale the DB instances to zero outside of business hours ","B. Explore AWS Marketplace for partner solutions that will automatically start and stop the EC2 Instances \nand OB instances on a schedule ","C. Launch another EC2 instance. Configure a crontab schedule to run shell scripts that will start and stop \nthe existing EC2 instances and DB instances on a schedule. ","D. Create an AWS Lambda function that will start and stop the EC2 instances and DB instances Configure \nAmazon EventBridge to invoke the Lambda function on a schedule "],"Explanation":"Answer: D \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":63,"QuestionContent":" \nAn online retail company has more than 50 million active customers and receives more than 25,000 orders \neach day. The company collects purchase data for customers and stores this data in Amazon S3. Additional \ncustomer data is stored in Amazon RDS. \n \nThe company wants to make all the data available to various teams so that the teams can perform analytics. \nThe solution must provide the ability to manage fine-grained permissions for the data and must minimize \noperational overhead. \n \nWhich solution will meet these requirements? \n ","Option":["A. Migrate the purchase data to write directly to Amazon RDS. Use RDS access controls to limit access. ","B. Schedule an AWS Lambda function to periodically copy data from Amazon RDS to Amazon S3. Create \nan AWS Glue crawler. Use Amazon Athena to query the data. Use S3 policies to limit access. ","C. Create a data lake by using AWS Lake Formation. Create an AWS Glue JDBC connection to Amazon \nRDS. Register the S3 bucket in Lake Formation. Use Lake Formation access controls to limit access. ","D. Create an Amazon Redshift cluster. Schedule an AWS Lambda function to periodically copy data from \nAmazon S3 and Amazon RDS to Amazon Redshift. Use Amazon Redshift access controls to limit \naccess. "],"Explanation":"Answer: C \nExplanation \nhttps://aws.amazon.com/blogs/big-data/manage-fine-grained-access-control-using-aws-lake-formation/ \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":64,"QuestionContent":" \n152 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \n \nA media company collects and analyzes user activity data on premises. The company wants to migrate this \ncapability to AWS. The user activity data store will continue to grow and will be petabytes in size. The \ncompany needs to build a highly available data ingestion solution that facilitates on-demand analytics of \nexisting data and new data with SQL. \n \nWhich solution will meet these requirements with the LEAST operational overhead? \n ","Option":["A. Send activity data to an Amazon Kinesis data stream. Configure the stream to deliver the data to an \nAmazon S3 bucket. ","B. Send activity data to an Amazon Kinesis Data Firehose delivery stream. Configure the stream to deliver \nthe data to an Amazon Redshift cluster. ","C. Place activity data in an Amazon S3 bucket. Configure Amazon S3 to run an AWS Lambda function on \nthe data as the data arrives in the S3 bucket. ","D. Create an ingestion service on Amazon EC2 instances that are spread across multiple Availability \nZones. Configure the service to forward data to an Amazon RDS Multi-AZ database. "],"Explanation":"Answer: B \n \nExplanation \nAmazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud. You can start with \njust a few hundred gigabytes of data and scale to a petabyte or more. This allows you to use your data to gain \nnew insights for your business and customers. The first step to create a data warehouse is to launch a set of \nnodes, called an Amazon Redshift cluster. After you provision your cluster, you can upload your data set and \nthen perform data analysis queries. Regardless of the size of the data set, Amazon Redshift offers fast query \nperformance using the same SQL-based tools and business intelligence applications that you use today. \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":65,"QuestionContent":" \nA company must migrate 20 TB of data from a data center to the AWS Cloud within 30 days. The company\u0027s \nnetwork bandwidth is limited to 15 Mbps and cannot exceed 70% utilization. What should a solutions architect \ndo to meet these requirements? \n ","Option":["A. Use AWS Snowball. ","B. Use AWS DataSync. ","C. Use a secure VPN connection. ","D. Use Amazon S3 Transfer Acceleration. "],"Explanation":"Answer: A \n \nExplanation \nAWS Snowball is a secure data transport solution that accelerates moving large amounts of data into and out \n \n153 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nof the AWS cloud. It can move up to 80 TB of data at a time, and provides a network bandwidth of up to 50 \nMbps, so it is well-suited for the task. Additionally, it is secure and easy to use, making it the ideal solution for \nthis migration. \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":66,"QuestionContent":" \nA company is designing the network for an online multi-player game. The game uses the UDP networking \nprotocol and will be deployed in eight AWS Regions. The network architecture needs to minimize latency and \npacket loss to give end users a high-quality gaming experience. \n \nWhich solution will meet these requirements? \n ","Option":["A. Set up a transit gateway in each Region. Create inter-Region peering attachments between each transit \ngateway. ","B. Set up AWS Global Accelerator with UDP listeners and endpoint groups in each Region. ","C. Set up Amazon CloudFront with UDP turned on. Configure an origin in each Region. ","D. Set up a VPC peering mesh between each Region. Turn on UDP for each VPC. "],"Explanation":"Answer: B \n \nExplanation \nThe best solution for this situation is option B, setting up AWS Global Accelerator with UDP listeners and \nendpoint groups in each Region. AWS Global Accelerator is a networking service that improves the \navailability and performance of internet applications by routing user requests to the nearest AWS Region [1]. \nIt also improves the performance of UDP applications by providing faster, more reliable data transfers with \nlower latency and fewer packet losses. By setting up UDP listeners and endpoint groups in each Region, \nGlobal Accelerator will route traffic to the nearest Region for faster response times and a better user \nexperience. \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":67,"QuestionContent":"A company has implemented a self-managed DNS service on AWS. The solution consists of the following: \n\u2022 Amazon EC2 instances in different AWS Regions \n \n\u2022 Endpomts of a standard accelerator m AWS Global Accelerator \n \nThe company wants to protect the solution against DDoS attacks What should a solutions architect do to meet \nthis requirement? \n ","Option":["A. Subscribe to AWS Shield Advanced Add the accelerator as a resource to protect ","B. Subscribe to AWS Shield Advanced Add the EC2 instances as resources to protect ","C. Create an AWS WAF web ACL that includes a rate-based rule Associate the web ACL with the \n154 of 230 Practice Test Amazon Web Services - SAA-C03 \naccelerator ","D. Create an AWS WAF web ACL that includes a rate-based rule Associate the web ACL with the EC2 \ninstances "],"Explanation":"Answer: B \n \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":68,"QuestionContent":" \nA solutions architect observes that a nightly batch processing job is automatically scaled up for 1 hour before \nthe desired Amazon EC2 capacity is reached. The peak capacity is the \u2018same every night and the batch jobs \nalways start at 1 AM. The solutions architect needs to find a cost-effective solution that will allow for the \ndesired EC2 capacity to be reached quickly and allow the Auto Scaling group to scale down after the batch \njobs are complete. \n \nWhat should the solutions architect do to meet these requirements? \n ","Option":["A. Increase the minimum capacity for the Auto Scaling group. ","B. Increase the maximum capacity for the Auto Scaling group. ","C. Configure scheduled scaling to scale up to the desired compute level. ","D. Change the scaling policy to add more EC2 instances during each scaling operation. "],"Explanation":"Answer: C \n \nExplanation \nBy configuring scheduled scaling, the solutions architect can set the Auto Scaling group to automatically scale \nup to the desired compute level at a specific time (IAM) when the batch job starts and then automatically scale \ndown after the job is complete. This will allow the desired EC2 capacity to be reached quickly and also help in \nreducing the cost. \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":69,"QuestionContent":" \nA company needs to export its database once a day to Amazon S3 for other teams to access. The exported \nobject size vanes between 2 GB and 5 GB. The S3 access pattern for the data is variable and changes rapidly. \nThe data must be immediately available and must remain accessible for up to 3 months. The company needs \nthe most cost-effective solution that will not increase retrieval time \n \nWhich S3 storage class should the company use to meet these requirements? \n ","Option":["A. S3 Intelligent-Tiering ","B. S3 Glacier Instant Retrieval ","C. S3 Standard \n155 of 230 Practice Test Amazon Web Services - SAA-C03 ","D. S3 Standard-Infrequent Access (S3 Standard-IA) "],"Explanation":"Answer: D \n \nExplanation \nS3 Intelligent-Tiering is a cost-optimized storage class that automatically moves data to the most cost-effective \naccess tier based on changing access patterns. Although it offers cost savings, it also introduces additional \nlatency and retrieval time into the data retrieval process, which may not meet the requirement of \u0022immediately \navailable\u0022 data. On the other hand, S3 Standard-Infrequent Access (S3 Standard-IA) provides low cost storage \nwith low latency and high throughput performance. It is designed for infrequently accessed data that can be \nrecreated if lost, and can be retrieved in a timely manner if required. It is a cost-effective solution that meets \nthe requirement of immediately available data and remains accessible for up to 3 months. \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":70,"QuestionContent":" \nA company plans to use Amazon ElastiCache for its multi-tier web application A solutions architect creates a \nCache VPC for the ElastiCache cluster and an App VPC for the application\u0027s Amazon EC2 instances Both \nVPCs are in the us-east-1 Region \n \nThe solutions architect must implement a solution to provide tne application\u0027s EC2 instances with access to the \nElastiCache cluster \n \nWhich solution will meet these requirements MOST cost-effectively? \n ","Option":["A. Create a peering connection between the VPCs Add a route table entry for the peering connection in \nboth VPCs Configure an inbound rule for the ElastiCache cluster\u0027s security group to allow inbound \nconnection from the application\u0027s security group ","B. Create a Transit VPC Update the VPC route tables in the Cache VPC and the App VPC to route traffic \nthrough the Transit VPC Configure an inbound rule for the ElastiCache cluster\u0027s security group to allow \ninbound connection from the application\u0027s security group ","C. Create a peering connection between the VPCs Add a route table entry for the peering connection in \nboth VPCs Configure an inbound rule for the peering connection\u0027s security group to allow inbound \nconnection from the application\u0027s secunty group ","D. Create a Transit VPC Update the VPC route tables in the Cache VPC and the App VPC to route traffic \nthrough the Transit VPC Configure an inbound rule for the Transit VPCs security group to allow \ninbound connection from the application\u0027s security group "],"Explanation":"Answer: A \n \nExplanation \nCreating a peering connection between the two VPCs and configuring an inbound rule for the ElastiCache \ncluster\u0027s security group to allow inbound connection from the application\u0027s security group is the most \n \n156 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \ncost-effective solution. Peering connections are free and you only incur the cost of configuring the security \ngroup rules. The Transit VPC solution requires additional VPCs and associated resources, which would incur \nadditional costs. \n \nBefore Testing | AWS Certification Information and Policies | AWS \nhttps://aws.amazon.com/certification/policies/before-testing/ \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":71,"QuestionContent":" \nA company runs an internal browser-based application The application runs on Amazon EC2 instances behind \nan Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple \nAvailability Zones. The Auto Scaling group scales up to 20 instances during work hours but scales down to 2 \ninstances overnight Staff are complaining that the application is very slow when the day begins although it \nruns well by mid-morning. \n \nHow should the scaling be changed to address the staff complaints and keep costs to a minimum\u0027? \n ","Option":["A. Implement a scheduled action that sets the desired capacity to 20 shortly before the office opens ","B. Implement a step scaling action triggered at a lower CPU threshold, and decrease the cooldown period. ","C. Implement a target tracking action triggered at a lower CPU threshold, and decrease the cooldown \nperiod. ","D. Implement a scheduled action that sets the minimum and maximum capacity to 20 shortly before the \noffice opens "],"Explanation":"Answer: A \n \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":72,"QuestionContent":" \nA company uses Amazon EC2 instances and AWS Lambda functions to run its application. The company has \nVPCs with public subnets and private subnets in its AWS account. The EC2 instances run in a private subnet \nin one of the VPCs. The Lambda functions need direct network access to the EC2 instances for the application \nto work. \n \nThe application will run for at least 1 year. The company expects the number of Lambda functions that the \napplication uses to increase during that time. The company wants to maximize its savings on all application \nresources and to keep network latency between the services low. \n \nWhich solution will meet these requirements? \n ","Option":["A. Purchase on an EC2 instance Savings Plan. Optimize the Lambda functions duration and memory usage \nand the number of invocations. Connect the Lambda functions to the private subnet that contains the \nEC2 instances. ","B. Purchase on an EC2 instance Savings Plan. Optimize the Lambda functions duration and memory usage \n157 of 230 Practice Test Amazon Web Services - SAA-C03 \nand the number of invocation, and the amount of data that is transfered. Connect the Lambda functions \nto a public subnet in the same VPC where the EC2 instances run. ","C. Purchase a Compute Savings Plan. Optimize the Lambda functions duration and memory usage, the \nnumber of invocations, and the amount of data that is transferred Connect the Lambda function to the \nPrivate subnet that contains the EC2 instances. ","D. Purchase a Compute Savings Plan. Optimize the Lambda functions\u2018 duration and memory usage, the \nnumber of invocations, and the amount of data that is transferred Keep the Lambda functions in the \nLambda service VPC. "],"Explanation":"Answer: C \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":73,"QuestionContent":" \nA company is implementing new data retention policies for all databases that run on Amazon RDS DB \ninstances. The company must retain daily backups for a minimum period of 2 years. The backups must be \nconsistent and restorable. \n \nWhich solution should a solutions architect recommend to meet these requirements? \n ","Option":["A. Create a backup vault in AWS Backup to retain RDS backups. Create a new backup plan with a daily \nschedule and an expiration period of 2 years after creation. Assign the RDS DB instances to the backup \nplan. ","B. Configure a backup window for the RDS DB instances for daily snapshots. Assign a snapshot retention \npolicy of 2 years to each RDS DB instance. Use Amazon Data Lifecycle Manager (Amazon DLM) to \nschedule snapshot deletions. ","C. Configure database transaction logs to be automatically backed up to Amazon CloudWatch Logs with \nan expiration period of 2 years. ","D. Configure an AWS Database Migration Service (AWS DMS) replication task. Deploy a replication \ninstance, and configure a change data capture (CDC) task to stream database changes to Amazon S3 as \nthe target. Configure S3 Lifecycle policies to delete the snapshots after 2 years. "],"Explanation":"Answer: A \n \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":74,"QuestionContent":" \nA company is building an application that consists of several microservices. The company has decided to use \ncontainer technologies to deploy its software on AWS. The company needs a solution that minimizes the \namount of ongoing effort for maintenance and scaling. The company cannot manage additional infrastructure. \n \nWhich combination of actions should a solutions architect take to meet these requirements? (Choose two.) \n ","Option":["A. Deploy an Amazon Elastic Container Service (Amazon ECS) cluster. \n158 of 230 Practice Test Amazon Web Services - SAA-C03 ","B. Deploy the Kubernetes control plane on Amazon EC2 instances that span multiple Availability Zones. ","C. Deploy an Amazon Elastic Container Service (Amazon ECS) service with an Amazon EC2 launch type. \nSpecify a desired task number level of greater than or equal to 2. ","D. Deploy an Amazon Elastic Container Service (Amazon ECS) service with a Fargate launch type. \nSpecify a desired task number level of greater than or equal to 2. ","E. Deploy Kubernetes worker nodes on Amazon EC2 instances that span multiple Availability Zones. \nCreate a deployment that specifies two or more replicas for each microservice. "],"Explanation":"Answer: A D \n \nExplanation \nAWS Fargate is a technology that you can use with Amazon ECS to run containers without having to manage \nservers or clusters of Amazon EC2 instances. With Fargate, you no longer have to provision, configure, or \nscale clusters of virtual machines to run containers. \nhttps://docs.aws.amazon.com/AmazonECS/latest/userguide/what-is-fargate.html \n \n","RightAnswer":["A","D"],"QuestionChoose":[]},{"QuestionNumber":75,"QuestionContent":" \nA company has an application that places hundreds of .csv files into an Amazon S3 bucket every hour. The \nfiles are 1 GB in size. Each time a file is uploaded, the company needs to convert the file to Apache Parquet \nformat and place the output file into an S3 bucket. \n \nWhich solution will meet these requirements with the LEAST operational overhead? \n ","Option":["A. Create an AWS Lambda function to download the .csv files, convert the files to Parquet format, and \nplace the output files in an S3 bucket. Invoke the Lambda function for each S3 PUT event. ","B. Create an Apache Spark job to read the .csv files, convert the files to Parquet format, and place the \noutput files in an S3 bucket. Create an AWS Lambda function for each S3 PUT event to invoke the \nSpark job. ","C. Create an AWS Glue table and an AWS Glue crawler for the S3 bucket where the application places the \n.csv files. Schedule an AWS Lambda function to periodically use Amazon Athena to query the AWS \nGlue table, convert the query results into Parquet format, and place the output files into an S3 bucket. ","D. Create an AWS Glue extract, transform, and load (ETL) job to convert the .csv files to Parquet format \nand place the output files into an S3 bucket. Create an AWS Lambda function for each S3 PUT event to \ninvoke the ETL job. "],"Explanation":"Answer: D \n \nExplanation \nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/three-aws-glue-etl-job-types-for-converting-d \n \n159 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":76,"QuestionContent":" \nA rapidly growing ecommerce company is running its workloads in a single AWS Region. A solutions \narchitect must create a disaster recovery (DR) strategy that includes a different AWS Region The company \nwants its database to be up to date in the DR Region with the least possible latency The remaining \ninfrastructure in the DR Region needs to run at reduced capacity and must be able to scale up it necessary \n \nWhich solution will meet these requirements with the LOWEST recovery time objective (RTO)? \n ","Option":["A. Use an Amazon Aurora global database with a pilot light deployment ","B. Use an Amazon Aurora global database with a warm standby deployment ","C. Use an Amazon RDS Multi-AZ DB instance with a pilot light deployment ","D. Use an Amazon RDS Multi-AZ DB instance with a warm standby deployment "],"Explanation":"Answer: B \n \nExplanation \nhttps://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options- \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":77,"QuestionContent":" \nA company is launching a new application deployed on an Amazon Elastic Container Service (Amazon ECS) \ncluster and is using the Fargate launch type tor ECS tasks The company is monitoring CPU and memory usage \nbecause it is expecting high traffic to the application upon its launch However the company wants to reduce \ncosts when utilization decreases \n \nWhat should a solutions architect recommend? \n ","Option":["A. Use Amazon EC2 Auto Scaling to scale at certain periods based on previous traffic patterns ","B. Use an AWS Lambda function to scale Amazon ECS based on metric breaches that trigger an Amazon \nCloudWatch alarm ","C. Use Amazon EC2 Auto Scaling with simple scaling policies to scale when ECS metric breaches trigger \nan Amazon CloudWatch alarm ","D. Use AWS Application Auto Scaling with target tracking policies to scale when ECS metric breaches \ntrigger an Amazon CloudWatch alarm "],"Explanation":"Answer: C \n \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":78,"QuestionContent":"A company is developing a real-time multiplayer game that uses UDP for communications between the client \n \n160 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nand servers In an Auto Scaling group Spikes in demand are anticipated during the day, so the game server \nplatform must adapt accordingly Developers want to store gamer scores and other non-relational data in a \ndatabase solution that will scale without intervention \n \nWhich solution should a solutions architect recommend? \n ","Option":["A. Use Amazon Route 53 for traffic distribution and Amazon Aurora Serverless for data storage ","B. Use a Network Load Balancer for traffic distribution and Amazon DynamoDB on-demand for data \nstorage ","C. Use a Network Load Balancer for traffic distribution and Amazon Aurora Global Database for data \nstorage ","D. Use an Application Load Balancer for traffic distribution and Amazon DynamoDB global tables for data \nstorage "],"Explanation":"Answer: B \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":79,"QuestionContent":" \nA company runs a containerized application on a Kubernetes cluster in an on-premises data center. The \ncompany is using a MongoDB database for data storage. \n \nThe company wants to migrate some of these environments to AWS, but no code changes or deployment \nmethod changes are possible at this time. The company needs a solution that minimizes operational overhead. \n \nWhich solution meets these requirements? \n ","Option":["A. Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 worker nodes for compute \nand MongoDB on EC2 for data storage. ","B. Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for compute and Amazon \nDynamoDB for data storage. ","C. Use Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 worker nodes for compute \nand Amazon DynamoDB for data storage. ","D. Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate for compute and Amazon \nDocumentDB (with MongoDB compatibility) for data storage. "],"Explanation":"Answer: D \n \nExplanation \nAmazon DocumentDB (with MongoDB compatibility) is a fast, reliable, and fully managed database service. \nAmazon DocumentDB makes it easy to set up, operate, and scale MongoDB-compatible databases in the \ncloud. With Amazon DocumentDB, you can run the same application code and use the same drivers and tools \nthat you use with MongoDB. \n \n161 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nhttps://docs.aws.amazon.com/documentdb/latest/developerguide/what-is.html \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":80,"QuestionContent":" \nWhat should a solutions architect do to ensure that all objects uploaded to an Amazon S3 bucket are \nencrypted? \n ","Option":["A. Update the bucket policy to deny if the PutObject does not have an s3 x-amz-acl header set ","B. Update the bucket policy to deny if the PutObject does not have an s3:x-amz-aci header set to private. ","C. Update the bucket policy to deny if the PutObject does not have an aws SecureTransport header set to \ntrue ","D. Update the bucket policy to deny if the PutObject does not have an x-amz-server-side-encryption header \nset. "],"Explanation":"Answer: D \n \nExplanation \nhttps://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/#:~:text= \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":81,"QuestionContent":" \nAt part of budget planning. management wants a report of AWS billed dams listed by user. The data will be \nused to create department budgets. A solution architect needs to determine the most efficient way to obtain this \nreport Information \n \nWhich solution meets these requirement? \n ","Option":["A. Run a query with Amazon Athena to generate the report. ","B. Create a report in Cost Explorer and download the report ","C. Access the bill details from the runing dashboard and download Via bill. ","D. Modify a cost budget in AWS Budgets to alert with Amazon Simple Email Service (Amazon SES). "],"Explanation":"Answer: B \n \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":82,"QuestionContent":" \nA company recently migrated its entire IT environment to the AWS Cloud. The company discovers that users \nare provisioning oversized Amazon EC2 instances and modifying security group rules without using the \nappropriate change control process A solutions architect must devise a strategy to track and audit these \ninventory and configuration changes. \n \n162 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \n \nWhich actions should the solutions architect take to meet these requirements? (Select TWO ) \n ","Option":["A. Enable AWS CloudTrail and use it for auditing ","B. Use data lifecycie policies for the Amazon EC2 instances ","C. Enable AWS Trusted Advisor and reference the security dashboard ","D. Enable AWS Config and create rules for auditing and compliance purposes ","E. Restore previous resource configurations with an AWS CloudFormation template "],"Explanation":"Answer: A D \n \n \n","RightAnswer":["A","D"],"QuestionChoose":[]},{"QuestionNumber":83,"QuestionContent":" \nA solutions architect needs to design a system to store client case files. The files are core company assets and \nare important. The number of files will grow over time. \n \nThe files must be simultaneously accessible from multiple application servers that run on Amazon EC2 \ninstances. The solution must have built-in redundancy. \n \nWhich solution meets these requirements? \n ","Option":["A. Amazon Elastic File System (Amazon EFS) ","B. Amazon Elastic Block Store (Amazon EBS) ","C. Amazon S3 Glacier Deep Archive ","D. AWS Backup "],"Explanation":"Answer: A \n \nExplanation \nAmazon EFS provides a simple, scalable, fully managed file system that can be simultaneously accessed from \nmultiple EC2 instances and provides built-in redundancy. It is optimized for multiple EC2 instances to access \nthe same files, and it is designed to be highly available, durable, and secure. It can scale up to petabytes of data \nand can handle thousands of concurrent connections, and is a cost-effective solution for storing and accessing \nlarge amounts of data. \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":84,"QuestionContent":" \nA company\u0027s application runs on AWS. The application stores large documents in an Amazon S3 bucket that \nuses the S3 Standard-infrequent Access (S3 Standerd-IA) storage class. The company will continue paying to \nstore the data but wants to save on its total S3 costs. The company wants authorized external users to have the \n \n163 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nability to access the documents in milliseconds. \n \nWhich solution will meet these requirements MOST cost-effectively? \n ","Option":["A. Configure the S3 bucket to be a Requester Pays bucket ","B. Change the storage tier to S3 Standard for all existing and future objects. ","C. Turn on S3 Transfer Acceleration tor the S3 Docket ","D. Use Amazon CloudFront to handle all the requests to the S3 bucket "],"Explanation":"Answer: D \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":85,"QuestionContent":" \nA hospital is designing a new application that gathers symptoms from patients. The hospital has decided to use \nAmazon Simple Queue Service (Amazon SOS) and Amazon Simple Notification Service (Amazon SNS) in \nthe architecture. \n \nA solutions architect is reviewing the infrastructure design Data must be encrypted at test and in transit. Only \nauthorized personnel of the hospital should be able to access the data. \n \nWhich combination of steps should the solutions architect take to meet these requirements? (Select TWO.) \n ","Option":["A. Turn on server-side encryption on the SQS components Update tie default key policy to restrict key \nusage to a set of authorized principals. ","B. Turn on server-side encryption on the SNS components by using an AWS Key Management Service \n(AWS KMS) customer managed key Apply a key policy to restrict key usage to a set of authorized \nprincipals. ","C. Turn on encryption on the SNS components Update the default key policy to restrict key usage to a set \nof authorized principals. Set a condition in the topic pokey to allow only encrypted connections over \nTLS. ","D. Turn on server-side encryption on the SOS components by using an AWS Key Management Service \n(AWS KMS) customer managed key Apply a key pokey to restrict key usage to a set of authorized \nprincipals. Set a condition in the queue pokey to allow only encrypted connections over TLS. ","E. Turn on server-side encryption on the SOS components by using an AWS Key Management Service \n(AWS KMS) customer managed key. Apply an IAM pokey to restrict key usage to a set of authorized \nprincipals. Set a condition in the queue pokey to allow only encrypted connections over TLS "],"Explanation":"Answer: B D \n \n","RightAnswer":["B","D"],"QuestionChoose":[]},{"QuestionNumber":86,"QuestionContent":"A development team has launched a new application that is hosted on Amazon EC2 instances inside a \n \n164 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \ndevelopment VPC. A solution architect needs to create a new VPC in the same account. The new VPC will be \npeered with the development VPC. The VPC CIDR block for the development VPC is 192. 168. 00/24. The \nsolutions architect needs to create a CIDR block for the new VPC. The CIDR block must be valid for a VPC \npeering connection to the development VPC. \n \nWhat is the SMALLEST CIOR block that meets these requirements? \n ","Option":["A. 10.0.1.0/32 ","B. 192.168.0.0/24 ","C. 192.168.1.0/32 ","D. 10.0.1.0/24 "],"Explanation":"Answer: A \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":87,"QuestionContent":" \nA research laboratory needs to process approximately 8 TB of data The laboratory requires sub-millisecond \nlatencies and a minimum throughput of 6 GBps for the storage subsystem Hundreds of Amazon EC2 instances \nthat run Amazon Linux will distribute and process the data \n \nWhich solution will meet the performance requirements? \n ","Option":["A. Create an Amazon FSx for NetApp ONTAP file system Set each volume\u0027s tiering policy to ALL Import \nthe raw data into the file system Mount the file system on the EC2 instances ","B. Create an Amazon S3 bucket to stofe the raw data Create an Amazon FSx for Lustre file system that \nuses persistent SSD storage Select the option to import data from and export data to Amazon S3 Mount \nthe file system on the EC2 instances ","C. Create an Amazon S3 bucket to store the raw data Create an Amazon FSx for Lustre file system that \nuses persistent HDD storage Select the option to import data from and export data to Amazon S3 Mount \nthe file system on the EC2 instances ","D. Create an Amazon FSx for NetApp ONTAP file system Set each volume\u0027s tienng policy to NONE. \nImport the raw data into the file system Mount the file system on the EC2 instances "],"Explanation":"Answer: B \n \nExplanation \nCreate an Amazon S3 bucket to store the raw data Create an Amazon FSx for Lustre file system that uses \npersistent SSD storage Select the option to import data from and export data to Amazon S3 Mount the file \nsystem on the EC2 instances. Amazon FSx for Lustre uses SSD storage for sub-millisecond latencies and up to \n6 GBps throughput, and can import data from and export data to Amazon S3. Additionally, the option to select \npersistent SSD storage will ensure that the data is stored on the disk and not lost if the file system is stopped. \n \n165 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":88,"QuestionContent":"A solutions architect has created a new AWS account and must secure AWS account root user access. \nWhich combination of actions will accomplish this? (Choose two.) \n ","Option":["A. Ensure the root user uses a strong password. ","B. Enable multi-factor authentication to the root user. ","C. Store root user access keys in an encrypted Amazon S3 bucket. ","D. Add the root user to a group containing administrative permissions. ","E. Apply the required permissions to the root user with an inline policy document. "],"Explanation":"Answer: A B \n","RightAnswer":["A","B"],"QuestionChoose":[]},{"QuestionNumber":89,"QuestionContent":" \nA company is developing a new mobile app. The company must implement proper traffic filtering to protect \nits Application Load Balancer (ALB) against common application-level attacks, such as cross-site scripting or \nSQL injection. The company has minimal infrastructure and operational staff. The company needs to reduce \nits share of the responsibility in managing, updating, and securing servers for its AWS environment. \n \nWhat should a solutions architect recommend to meet these requirements? \n ","Option":["A. Configure AWS WAF rules and associate them with the ALB. ","B. Deploy the application using Amazon S3 with public hosting enabled. ","C. Deploy AWS Shield Advanced and add the ALB as a protected resource. ","D. Create a new ALB that directs traffic to an Amazon EC2 instance running a third-party firewall, which \nthen passes the traffic to the current ALB. "],"Explanation":"Answer: A \n \nExplanation \nA solutions architect should recommend option A, which is to configure AWS WAF rules and associate them \nwith the ALB. This will allow the company to apply traffic filtering at the application layer, which is \nnecessary for protecting the ALB against common application-level attacks such as cross-site scripting or SQL \ninjection. AWS WAF is a managed service that makes it easy to protect web applications from common web \nexploits that could affect application availability, compromise security, or consume excessive resources. The \ncompany can easily manage and update the rules to ensure the security of its application. \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":90,"QuestionContent":" \nA company\u0027s web application consists o( an Amazon API Gateway API in front of an AWS Lambda function \nand an Amazon DynamoDB database. The Lambda function \n \n166 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \n \nhandles the business logic, and the DynamoDB table hosts the data. The application uses Amazon Cognito \nuser pools to identify the individual users of the application. A solutions architect needs to update the \napplication so that only users who have a subscription can access premium content. \n ","Option":["A. Enable API caching and throttling on the API Gateway API ","B. Set up AWS WAF on the API Gateway API Create a rule to filter users who have a subscription ","C. Apply fine-grained IAM permissions to the premium content in the DynamoDB table ","D. Implement API usage plans and API keys to limit the access of users who do not have a subscription. "],"Explanation":"Answer: C \n \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":91,"QuestionContent":" \nA company collects data from thousands of remote devices by using a RESTful web services application that \nruns on an Amazon EC2 instance. The EC2 instance receives the raw data, transforms the raw data, and stores \nall the data in an Amazon S3 bucket. The number of remote devices will increase into the millions soon. The \ncompany needs a highly scalable solution that minimizes operational overhead. \n \nWhich combination of steps should a solutions architect take to meet these requirements9 (Select TWO.) \n ","Option":["A. Use AWS Glue to process the raw data in Amazon S3. ","B. Use Amazon Route 53 to route traffic to different EC2 instances. ","C. Add more EC2 instances to accommodate the increasing amount of incoming data. ","D. Send the raw data to Amazon Simple Queue Service (Amazon SOS). Use EC2 instances to process the \ndata. ","E. Use Amazon API Gateway to send the raw data to an Amazon Kinesis data stream. Configure Amazon \nKinesis Data Firehose to use the data stream as a source to deliver the data to Amazon S3. "],"Explanation":"Answer: A E \n \nExplanation \n\u0022RESTful web services\u0022 =\u003E API Gateway. \n \n\u0022EC2 instance receives the raw data, transforms the raw data, and stores all the data in an Amazon S3 bucket\u0022 \n=\u003E GLUE with (Extract - Transform - Load) \n \n","RightAnswer":["A","E"],"QuestionChoose":[]},{"QuestionNumber":92,"QuestionContent":"A company is using a content management system that runs on a single Amazon EC2 instance. The EC2 \n \n167 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \ninstance contains both the web server and the database software. The company must make its website platform \nhighly available and must enable the website to scale to meet user demand. \n \nWhat should a solutions architect recommend to meet these requirements? \n ","Option":["A. Move the database to Amazon RDS, and enable automatic backups. Manually launch another EC2 \ninstance in the same Availability Zone. Configure an Application Load Balancer in the Availability \nZone, and set the two instances as targets. ","B. Migrate the database to an Amazon Aurora instance with a read replica in the same Availability Zone as \nthe existing EC2 instance. Manually launch another EC2 instance in the same Availability Zone. \nConfigure an Application Load Balancer, and set the two EC2 instances as targets. ","C. Move the database to Amazon Aurora with a read replica in another Availability Zone. Create an \nAmazon Machine Image (AMI) from the EC2 instance. Configure an Application Load Balancer in two \nAvailability Zones. Attach an Auto Scaling group that uses the AMI across two Availability Zones. ","D. Move the database to a separate EC2 instance, and schedule backups to Amazon S3. Create an Amazon \nMachine Image (AMI) from the original EC2 instance. Configure an Application Load Balancer in two \nAvailability Zones. Attach an Auto Scaling group that uses the AMI across two Availability Zones. "],"Explanation":"Answer: C \n \nExplanation \nThis approach will provide both high availability and scalability for the website platform. By moving the \ndatabase to Amazon Aurora with a read replica in another availability zone, it will provide a failover option for \nthe database. The use of an Application Load Balancer and an Auto Scaling group across two availability \nzones allows for automatic scaling of the website to meet increased user demand. Additionally, creating an \nAMI from the original EC2 instance allows for easy replication of the instance in case of failure. \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":93,"QuestionContent":" \nA company has a three-tier application for image sharing. The application uses an Amazon EC2 instance for \nthe front-end layer, another EC2 instance for the application layer, and a third EC2 instance for a MySQL \ndatabase. A solutions architect must design a scalable and highly available solution that requires the least \namount of change to the application. \n \nWhich solution meets these requirements? \n ","Option":["A. Use Amazon S3 to host the front-end layer. Use AWS Lambda functions for the application layer. Move \nthe database to an Amazon DynamoDB table. Use Amazon S3 to store and serve users\u2019 images. ","B. Use load-balanced Multi-AZ AWS Elastic Beanstalk environments for the front-end layer and the \napplication layer. Move the database to an Amazon RDS DB instance with multiple read replicas to \nserve users\u2019 images. ","C. Use Amazon S3 to host the front-end layer. Use a fleet of EC2 instances in an Auto Scaling group for \nthe application layer. Move the database to a memory optimized instance type to store and serve users\u2019 \nimages. \n168 of 230 Practice Test Amazon Web Services - SAA-C03 ","D. Use load-balanced Multi-AZ AWS Elastic Beanstalk environments for the front-end layer and the \napplication layer. Move the database to an Amazon RDS Multi-AZ DB instance. Use Amazon S3 to \nstore and serve users\u2019 images. "],"Explanation":"Answer: D \n \nExplanation \nfor \u0022Highly available\u0022: Multi-AZ \u0026 for \u0022least amount of changes to the application\u0022: Elastic Beanstalk \nautomatically handles the deployment, from capacity provisioning, load balancing, auto-scaling to application \nhealth monitoring \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":94,"QuestionContent":" \nA company has deployed a database in Amazon RDS for MySQL. Due to increased transactions, the database \nsupport team is reporting slow reads against the DB instance and recommends adding a read replica. \n \nWhich combination of actions should a solutions architect take before implementing this change? (Choose \ntwo.) \n ","Option":["A. Enable binlog replication on the RDS primary node. ","B. Choose a failover priority for the source DB instance. ","C. Allow long-running transactions to complete on the source DB instance. ","D. Create a global table and specify the AWS Regions where the table will be available. ","E. Enable automatic backups on the source instance by setting the backup retention period to a value other \nthan 0. "],"Explanation":"Answer: C E \n \nExplanation \n\u0022An active, long-running transaction can slow the process of creating the read replica. We recommend that you \nwait for long-running transactions to complete before creating a read replica. If you create multiple read \nreplicas in parallel from the same source DB instance, Amazon RDS takes only one snapshot at the start of the \nfirst create action. When creating a read replica, there are a few things to consider. First, you must enable \nautomatic backups on the source DB instance by setting the backup retention period to a value other than 0. \nThis requirement also applies to a read replica that is the source DB instance for another read replica\u0022 \nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html \n \n","RightAnswer":["C","E"],"QuestionChoose":[]},{"QuestionNumber":95,"QuestionContent":" \nA company experienced a breach that affected several applications in its on-premises data center The attacker \ntook advantage of vulnerabilities in the custom applications that were running on the servers The company is \nnow migrating its applications to run on Amazon EC2 instances The company wants to implement a solution \n \n169 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nthat actively scans for vulnerabilities on the EC2 instances and sends a report that details the findings \nWhich solution will meet these requirements? ","Option":["A. Deploy AWS Shield to scan the EC2 instances for vulnerabilities Create an AWS Lambda function to \nlog any findings to AWS CloudTrail. ","B. Deploy Amazon Macie and AWS Lambda functions to scan the EC2 instances for vulnerabilities Log \nany findings to AWS CloudTrail ","C. Turn on Amazon GuardDuty Deploy the GuardDuty agents to the EC2 instances Configure an AWS \nLambda function to automate the generation and distribution of reports that detail the findings ","D. Turn on Amazon Inspector Deploy the Amazon Inspector agent to the EC2 instances Configure an AWS \nLambda function to automate the generation and distribution of reports that detail the findings "],"Explanation":"Answer: D \n \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":96,"QuestionContent":" \nA developer has an application that uses an AWS Lambda function to upload files to Amazon S3 and needs \nthe required permissions to perform the task The developer already has an IAM user with valid IAM \ncredentials required for Amazon S3 \n \nWhat should a solutions architect do to grant the permissions? \n ","Option":["A. Add required IAM permissions in the resource policy of the Lambda function ","B. Create a signed request using the existing IAM credentials n the Lambda function ","C. Create a new IAM user and use the existing IAM credentials in the Lambda function. ","D. Create an IAM execution role with the required permissions and attach the IAM rote to the Lambda \nfunction "],"Explanation":"Answer: D \n \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":97,"QuestionContent":" \nA company has an application that is backed ny an Amazon DynamoDB table. The company\u0027s compliance \nrequirements specify that database backups must be taken every month, must be available for 6 months, and \nmust be retained for 7 years. \n \nWhich solution will meet these requirements? \n ","Option":["A. Create an AWS Backup plan to back up the DynamoDB table on the first day of each month. Specify a \nlifecycle policy that transitions the backup to cold storage after 6 months. Set the retention period for \n170 of 230 Practice Test Amazon Web Services - SAA-C03 \neach backup to 7 years. ","B. Create a DynamoDB on-damand backup of the DynamoDB table on the first day of each month \nTransition the backup to Amazon S3 Glacier Flexible Retrieval after 6 months. Create an S3 Lifecycle \npolicy to delete backups that are older than 7 years. ","C. Use the AWS SDK to develop a script that creates an on-demand backup of the DynamoDB table. Set \nup an Amzon EvenlBridge rule that runs the script on the first day of each month. Create a second script \nthat will run on the second day of each month to transition DynamoDB backups that are older than 6 \nmonths to cold storage and to delete backups that are older than 7 years. ","D. Use the AWS CLI to create an on-demand backup of the DynamoDB table Set up an Amazon \nEventBridge rule that runs the command on the first day of each month with a cron expression Specify \nin the command to transition the backups to cold storage after 6 months and to delete the backups after 7 \nyears. "],"Explanation":"Answer: A \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":98,"QuestionContent":" \nA company is using a fleet of Amazon EC2 instances to ingest data from on-premises data sources. The data is \nin JSON format and Ingestion rates can be as high as 1 MB/s. When an EC2 instance is rebooted, the data \nin-flight is lost. The company\u0027s data science team wants to query Ingested data In near-real time. \nWhich solution provides near-real -time data querying that is scalable with minimal data loss? ","Option":["A. Publish data to Amazon Kinesis Data Streams Use Kinesis data Analytics to query the data. ","B. Publish data to Amazon Kinesis Data Firehose with Amazon Redshift as the destination Use Amazon \nRedshift to query the data ","C. Store ingested data m an EC2 Instance store Publish data to Amazon Kinesis Data Firehose with \nAmazon S3 as the destination. Use Amazon Athena to query the data. ","D. Store ingested data m an Amazon Elastic Block Store (Amazon EBS) volume Publish data to Amazon \nElastiCache tor Red Subscribe to the Redis channel to query the data "],"Explanation":"Answer: B \n \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":99,"QuestionContent":" \nA company deploys an appliation on five Amazon EC2 instances. An Applicatin Load Balancer (ALB) \ndistributes traffic to the instances by using a target group. The average CPU usage on each of the insatances is \nbelow 10% most of the time. With occasional surges to 65%. \n \nA solution architect needs to implement a solution to automate the scalability of the application. The solution \nmust optimize the cost of the architecture and must ensure that the application has enough CPU resources \nwhen surges occur. \n \n171 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nWhich solution will meet these requirements? \n ","Option":["A. Create an Amazon CloudWatch alarm that enters the ALARM state when the CPUUtilization metric is \nless than 20%. Create an AWS Lambda function that the CloudWatch alarm invokes to terminate one of \nthe EC2 instances in the ALB target group. ","B. Create an EC2 Auto Scaling. Select the exisiting ALB as the load balancer and the existing target group \nas the target group. Set a target tracking scaling policy that is based on the ASGAverageCPUUtilization \nmetric. Set the minimum instances to 2, the desired capacity to 3, the desired capacity to 3, the \nmaximum instances to 6, and the target value to 50%. And the EC2 instances to the Auto Scaling group. ","C. Create an EC2 Auto Scaling. Select the exisiting ALB as the load balancer and the existing target group. \nSet the minimum instances to 2, the desired capacity to 3, and the maximum instances to 6 Add the EC2 \ninstances to the Scaling group. ","D. Create two Amazon CloudWatch alarms. Configure the first CloudWatch alarm to enter the ALARM \nsatet when the average CPUTUilization metric is below 20%. Configure the seconnd CloudWatch alarm \nto enter the ALARM state when the average CPUUtilization metric is aboove 50%. Configure the \nalarms to publish to an Amazon Simple Notification Service (Amazon SNS) topic to send an email \nmessage. After receiving the message, log in to decrease or increase the number of EC2 instances that \nare running "],"Explanation":"Answer: B \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":100,"QuestionContent":" \nA company has a web application that is based on Java and PHP The company plans to move the application \nfrom on premises to AWS The company needs the ability to test new site features frequently. The company \nalso needs a highly available and managed solution that requires minimum operational overhead \n \nWhich solution will meet these requirements? \n ","Option":["A. Create an Amazon S3 bucket Enable static web hosting on the S3 bucket Upload the static content to the \nS3 bucket Use AWS Lambda to process all dynamic content ","B. Deploy the web application to an AWS Elastic Beanstalk environment Use URL swapping to switch \nbetween multiple Elastic Beanstalk environments for feature testing ","C. Deploy the web application lo Amazon EC2 instances that are configured with Java and PHP Use Auto \nScaling groups and an Application Load Balancer to manage the website\u0027s availability ","D. Containerize the web application Deploy the web application to Amazon EC2 instances Use the AWS \nLoad Balancer Controller to dynamically route traffic between containers thai contain the new site \nfeatures for testing "],"Explanation":"Answer: B \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":101,"QuestionContent":" \n172 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nA company runs a web application that is backed by Amazon RDS. A new database administrator caused data \nloss by accidentally editing information in a database table To help recover from this type of incident, the \ncompany wants the ability to restore the database to its state from 5 minutes before any change within the last \n30 days. \n \nWhich feature should the solutions architect include in the design to meet this requirement? \n ","Option":["A. Read replicas ","B. Manual snapshots ","C. Automated backups ","D. Multi-AZ deployments "],"Explanation":"Answer: C \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":102,"QuestionContent":" \nA company hosts its web application on AWS using seven Amazon EC2 instances. The company requires that \nthe IP addresses of all healthy EC2 instances be returned in response to DNS queries. \n \nWhich policy should be used to meet this requirement? \n ","Option":["A. Simple routing policy ","B. Latency routing policy ","C. Multivalue routing policy ","D. Geolocation routing policy "],"Explanation":"Answer: C \n \nExplanation \nUse a multivalue answer routing policy to help distribute DNS responses across multiple resources. For \nexample, use multivalue answer routing when you want to associate your routing records with a Route 53 \nhealth check. For example, use multivalue answer routing when you need to return multiple values for a DNS \nquery and route traffic to multiple IP addresses. \nhttps://aws.amazon.com/premiumsupport/knowledge-center/multivalue-versus-simple-policies/ \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":103,"QuestionContent":" \nA company plans to use Amazon ElastiCache for its multi-tier web application. A solutions architect creates a \nCache VPC for the ElastiCache cluster and an App VPC for the application\u2019s Amazon EC2 instances. Both \nVPCs are in the us-east-1 Region. \n \nThe solutions architect must implement a solution to provide the application\u2019s EC2 instances with access to \n \n173 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nthe ElastiCache cluster. \n \nWhich solution will meet these requirements MOST cost-effectively? \n ","Option":["A. Create a peering connection between the VPCs. Add a route table entry for the peering connection in \nboth VPCs. Configure an inbound rule for the ElastiCache cluster\u2019s security group to allow inbound \nconnection from the application\u2019s security group. ","B. Create a Transit VPC. Update the VPC route tables in the Cache VPC and the App VPC to route traffic \nthrough the Transit VPC. Configure an inbound rule for the ElastiCache cluster\u0027s security group to allow \ninbound connection from the application\u2019s security group. ","C. Create a peering connection between the VPCs. Add a route table entry for the peering connection in \nboth VPCs. Configure an inbound rule for the peering connection\u2019s security group to allow inbound \nconnection from the application\u2019s security group. ","D. Create a Transit VPC. Update the VPC route tables in the Cache VPC and the App VPC to route traffic \nthrough the Transit VPC. Configure an inbound rule for the Transit VPC\u2019s security group to allow \ninbound connection from the application\u2019s security group. "],"Explanation":"Answer: A \n \nExplanation \nCreating a peering connection between the VPCs allows the application\u0027s EC2 instances to communicate with \nthe ElastiCache cluster directly and efficiently. This is the most cost-effective solution as it does not involve \ncreating additional resources such as a Transit VPC, and it does not incur additional costs for traffic passing \nthrough the Transit VPC. Additionally, it is also more secure as it allows you to configure a more restrictive \nsecurity group rule to allow inbound connection from only the application\u0027s security group. \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":104,"QuestionContent":" \nA company is using AWS to design a web application that will process insurance quotes Users will request \nquotes from the application Quotes must be separated by quote type, must be responded to within 24 hours, \nand must not get lost The solution must maximize operational efficiency and must minimize maintenance. \nWhich solution meets these requirements? \n ","Option":["A. Create multiple Amazon Kinesis data streams based on the quote type Configure the web application to \nsend messages to the proper data stream Configure each backend group of application servers to use the \nKinesis Client Library (KCL) to pool messages from its own data stream ","B. Create an AWS Lambda function and an Amazon Simple Notification Service (Amazon SNS) topic for \neach quote type Subscribe the Lambda function to its associated SNS topic Configure the application to \npublish requests tot quotes to the appropriate SNS topic ","C. Create a single Amazon Simple Notification Service (Amazon SNS) topic Subscribe Amazon Simple \nQueue Service (Amazon SQS) queues to the SNS topic Configure SNS message filtering to publish \nmessages to the proper SQS queue based on the quote type Configure each backend application server to \nuse its own SQS queue \n174 of 230 Practice Test Amazon Web Services - SAA-C03 ","D. Create multiple Amazon Kinesis Data Firehose delivery streams based on the quote type to deliver data \nstreams to an Amazon Elasucsearch Service (Amazon ES) cluster Configure the application to send \nmessages to the proper delivery stream Configure each backend group of application servers to search \nfor the messages from Amazon ES and process them accordingly "],"Explanation":"Answer: C \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":105,"QuestionContent":" \nA solutions architect wants all new users to have specific complexity requirements and mandatory rotation \nperiods tor IAM user passwords What should the solutions architect do to accomplish this? \n ","Option":["A. Set an overall password policy for the entire AWS account ","B. Set a password policy for each IAM user in the AWS account ","C. Use third-party vendor software to set password requirements ","D. Attach an Amazon CloudWatch rule to the Create_newuser event to set the password with the \nappropriate requirements "],"Explanation":"Answer: A \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":106,"QuestionContent":" \nAn application that is hosted on Amazon EC2 instances needs to access an Amazon S3 bucket Traffic must not \ntraverse the internet How should a solutions architect configure access to meet these requirements? \n ","Option":["A. Create a private hosted zone by using Amazon Route 53 ","B. Set up a gateway VPC endpoint for Amazon S3 in the VPC ","C. Configure the EC2 instances to use a NAT gateway to access the S3 bucket ","D. Establish an AWS Site-to-Site VPN connection between the VPC and the S3 bucket "],"Explanation":"Answer: B \n \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":107,"QuestionContent":" \nA company has launched an Amazon RDS for MySQL D6 instance Most of the connections to the database \ncome from serverless applications. Application traffic to the database changes significantly at random intervals \nAt limes of high demand, users report that their applications experience database connection rejection errors. \n \nWhich solution will resolve this issue with the LEAST operational overhead? \n ","Option":["A. Create a proxy in RDS Proxy Configure the users\u0027 applications to use the DB instance through RDS \n175 of 230 Practice Test Amazon Web Services - SAA-C03 \nProxy ","B. Deploy Amazon ElastCache for Memcached between the users\u0027 application and the DB instance ","C. Migrate the DB instance to a different instance class that has higher I/O capacity. Configure the users\u0027 \napplications to use the new DB instance. ","D. Configure Multi-AZ for the DB instance Configure the users\u0027 application to switch between the DB \ninstances. "],"Explanation":"Answer: A \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":108,"QuestionContent":" \nA solutions architect needs to optimize storage costs. The solutions architect must identify any Amazon S3 \nbuckets that are no longer being accessed or are rarely accessed. \n \nWhich solution will accomplish this goal with the LEAST operational overhead? \n ","Option":["A. Analyze bucket access patterns by using the S3 Storage Lens dashboard for advanced activity metrics. ","B. Analyze bucket access patterns by using the S3 dashboard in the AWS Management Console. ","C. Turn on the Amazon CloudWatch BucketSizeBytes metric for buckets. Analyze bucket access patterns \nby using the metrics data with Amazon Athena. ","D. Turn on AWS CloudTrail for S3 object monitoring. Analyze bucket access patterns by using CloudTrail \nlogs that are integrated with Amazon CloudWatch Logs. "],"Explanation":"Answer: A \n \nExplanation \nS3 Storage Lens is a fully managed S3 storage analytics solution that provides a comprehensive view of object \nstorage usage, activity trends, and recommendations to optimize costs. Storage Lens allows you to analyze \nobject access patterns across all of your S3 buckets and generate detailed metrics and reports. \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":109,"QuestionContent":" \nA company runs an application on a large fleet of Amazon EC2 instances. The application reads and write \nentries into an Amazon DynamoDB table. The size of the DynamoDB table continuously grows, but the \napplication needs only data from the last 30 days. The company needs a solution that minimizes cost and \ndevelopment effort. \n \nWhich solution meets these requirements? \n ","Option":["A. Use an AWS CloudFormation template to deploy the complete solution. Redeploy the CloudFormation \nstack every 30 days, and delete the original stack. ","B. Use an EC2 instance that runs a monitoring application from AWS Marketplace. Configure the \n176 of 230 Practice Test Amazon Web Services - SAA-C03 \nmonitoring application to use Amazon DynamoDB Streams to store the timestamp when a new item is \ncreated in the table. Use a script that runs on the EC2 instance to delete items that have a timestamp that \nis older than 30 days. ","C. Configure Amazon DynamoDB Streams to invoke an AWS Lambda function when a new item is \ncreated in the table. Configure the Lambda function to delete items in the table that are older than 30 \ndays. ","D. Extend the application to add an attribute that has a value of the current timestamp plus 30 days to each \nnew item that is created in the table. Configure DynamoDB to use the attribute as the TTL attribute. "],"Explanation":"Answer: D \n \nExplanation \nAmazon DynamoDB Time to Live (TTL) allows you to define a per-item timestamp to determine when an \nitem is no longer needed. Shortly after the date and time of the specified timestamp, DynamoDB deletes the \nitem from your table without consuming any write throughput. TTL is provided at no extra cost as a means to \nreduce stored data volumes by retaining only the items that remain current for your workload\u2019s needs. \n \nTTL is useful if you store items that lose relevance after a specific time. The following are example TTL use \ncases: \n \nRemove user or sensor data after one year of inactivity in an application. \n \nArchive expired items to an Amazon S3 data lake via Amazon DynamoDB Streams and AWS Lambda. \nRetain sensitive data for a certain amount of time according to contractual or regulatory obligations. \nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":110,"QuestionContent":" \nA company has a custom application with embedded credentials that retrieves information from an Amazon \nRDS MySQL DB instance. Management says the application must be made more secure with the least amount \nof programming effort. \n \nWhat should a solutions architect do to meet these requirements? \n ","Option":["A. Use AWS Key Management Service (AWS KMS) customer master keys (CMKs) to create keys. \nConfigure the application to load the database credentials from AWS KMS. Enable automatic key \nrotation. ","B. Create credentials on the RDS for MySQL database for the application user and store the credentials in \nAWS Secrets Manager. Configure the application to load the database credentials from Secrets \nManager. Create an AWS Lambda function that rotates the credentials in Secret Manager. ","C. Create credentials on the RDS for MySQL database for the application user and store the credentials in \nAWS Secrets Manager. Configure the application to load the database credentials from Secrets \nManager. Set up a credentials rotation schedule for the application user in the RDS for MySQL database \n177 of 230 Practice Test Amazon Web Services - SAA-C03 \nusing Secrets Manager. ","D. Create credentials on the RDS for MySQL database for the application user and store the credentials in \nAWS Systems Manager Parameter Store. Configure the application to load the database credentials from \nParameter Store. Set up a credentials rotation schedule for the application user in the RDS for MySQL \ndatabase using Parameter Store. "],"Explanation":"Answer: B \n \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":111,"QuestionContent":" \nA solutions architect needs to design a highly available application consisting of web, application, and \ndatabase tiers. HTTPS content delivery should be as close to the edge as possible, with the least delivery time. \n \nWhich solution meets these requirements and is MOST secure? \n ","Option":["A. Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2 instances in \npublic subnets. Configure Amazon CloudFront to deliver HTTPS content using the public ALB as the \norigin. ","B. Configure a public Application Load Balancer with multiple redundant Amazon EC2 instances in \nprivate subnets. Configure Amazon CloudFront to deliver HTTPS content using the EC2 instances as \nthe origin. ","C. Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2 instances in \nprivate subnets. Configure Amazon CloudFront to deliver HTTPS content using the public ALB as the \norigin. ","D. Configure a public Application Load Balancer with multiple redundant Amazon EC2 instances in public \nsubnets. Configure Amazon CloudFront to deliver HTTPS content using the EC2 instances as the origin. "],"Explanation":"Answer: C \n \nExplanation \nThis solution meets the requirements for a highly available application with web, application, and database \ntiers, as well as providing edge-based content delivery. Additionally, it maximizes security by having the ALB \nin a private subnet, which limits direct access to the web servers, while still being able to serve traffic over the \nInternet via the public ALB. This will ensure that the web servers are not exposed to the public Internet, which \nreduces the attack surface and provides a secure way to access the application. \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":112,"QuestionContent":" \nA company needs to migrate a legacy application from an on-premises data center to the AWS Cloud because \nof hardware capacity constraints. The application runs 24 hours a day. \u0026 days a week,. The application \ndatabase storage continues to grow over time. \n \nWhat should a solution architect do to meet these requirements MOST cost-affectivity? \n \n178 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \n ","Option":["A. Migrate the application layer to Amazon FC2 Spot Instances Migrate the data storage layer to Amazon \nS3. ","B. Migrate the application layer to Amazon EC2 Reserved Instances Migrate the data storage layer to \nAmazon RDS On-Demand Instances. ","C. Migrate the application layer to Amazon EC2 Reserved instances Migrate the data storage layer to \nAmazon Aurora Reserved Instances. ","D. Migrate the application layer to Amazon EC2 On Demand Amazon Migrate the data storage layer to \nAmazon RDS Reserved instances. "],"Explanation":"Answer: C \n \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":113,"QuestionContent":" \nA company uses a 100 GB Amazon RDS for Microsoft SQL Server Single-AZ DB instance in the us-east-1 \nRegion to store customer transactions. The company needs high availability and automate recovery for the DB \ninstance. \n \nThe companu must also run reports on the RDS database several times a year. The report process causes \ntransactions to take longer than usual to post to the customer\u2018 accounts. \n \nWhich combination of steps will meet these requirements? (Select TWO.) \n ","Option":["A. Modify the DB instance from a Single-AZ DB instance to a Multi-AZ deployment. ","B. Take a snapshot of the current DB instance. Restore the snapshot to a new RDS deployment in another \nAvailability Zone. ","C. Create a read replica of the DB instance in a different Availability Zone. Point All requests for reports to \nthe read replica. ","D. Migrate the database to RDS Custom. ","E. Use RDS Proxy to limit reporting requests to the maintenance window. "],"Explanation":"Answer: B D \n \n","RightAnswer":["B","D"],"QuestionChoose":[]},{"QuestionNumber":114,"QuestionContent":" \nA company recently migrated its web application to AWS by rehosting the application on Amazon EC2 \ninstances in a single AWS Region. The company wants to redesign its application architecture to be highly \navailable and fault tolerant. Traffic must reach all running EC2 instances randomly. \n \nWhich combination of steps should the company take to meet these requirements? (Choose two.) \n \n179 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n ","Option":["A. Create an Amazon Route 53 failover routing policy. ","B. Create an Amazon Route 53 weighted routing policy. ","C. Create an Amazon Route 53 multivalue answer routing policy. ","D. Launch three EC2 instances: two instances in one Availability Zone and one instance in another \nAvailability Zone. ","E. Launch four EC2 instances: two instances in one Availability Zone and two instances in another \nAvailability Zone. "],"Explanation":"Answer: C E \n \nExplanation \nhttps://aws.amazon.com/premiumsupport/knowledge-center/multivalue-versus-simple-policies/ \n \n","RightAnswer":["C","E"],"QuestionChoose":[]},{"QuestionNumber":115,"QuestionContent":" \nA company is running a batch application on Amazon EC2 instances. The application consists of a backend \nwith multiple Amazon RDS databases. The application is causing a high number of leads on the databases. A \nsolutions architect must reduce the number of database reads while ensuring high availability. \n \nWhat should the solutions architect do to meet this requirement? \n ","Option":["A. Add Amazon RDS read replicas ","B. Use Amazon ElastCache for Redis ","C. Use Amazon Route 53 DNS caching ","D. Use Amazon ElastiCache for Memcached "],"Explanation":"Answer: A \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":116,"QuestionContent":" \nA company is building a solution that will report Amazon EC2 Auto Scaling events across all the applications \nin an AWS account. The company needs to use a serverless solution to store the EC2 Auto Scaling status data \nin Amazon S3. The company then will use the data in Amazon S3 to provide near-real-time updates in a \ndashboard. The solution must not affect the speed of EC2 instance launches. \n \nHow should the company move the data to Amazon S3 to meet these requirements? \n ","Option":["A. Use an Amazon CloudWatch metric stream to send the EC2 Auto Scaling status data to Amazon Kinesis \nData Firehose. Store the data in Amazon S3. ","B. Launch an Amazon EMR cluster to collect the EC2 Auto Scaling status data and send the data to \nAmazon Kinesis Data Firehose. Store the data in Amazon S3. \n180 of 230 Practice Test Amazon Web Services - SAA-C03 ","C. Create an Amazon EventBridge rule to invoke an AWS Lambda function on a schedule. Configure the \nLambda function to send the EC2 Auto Scaling status data directly to Amazon S3. ","D. Use a bootstrap script during the launch of an EC2 instance to install Amazon Kinesis Agent. Configure \nKinesis Agent to collect the EC2 Auto Scaling status data and send the data to Amazon Kinesis Data \nFirehose. Store the data in Amazon S3. "],"Explanation":"Answer: A \nExplanation \nYou can use metric streams to continually stream CloudWatch metrics to a destination of your choice, with \nnear-real-time delivery and low latency. One of the use cases is Data Lake: create a metric stream and direct it \nto an Amazon Kinesis Data Firehose delivery stream that delivers your CloudWatch metrics to a data lake \nsuch as Amazon S3. \nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Metric-Streams.html \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":117,"QuestionContent":" \nA medical research lab produces data that is related to a new study. The lab wants to make the data available \nwith minimum latency to clinics across the country for their on-premises, file-based applications. The data \nfiles are stored in an Amazon S3 bucket that has read-only permissions for each clinic. \n \nWhat should a solutions architect recommend to meet these requirements? \n ","Option":["A. Deploy an AWS Storage Gateway file gateway as a virtual machine (VM) on premises at each clinic ","B. Migrate the files to each clinic\u2019s on-premises applications by using AWS DataSync for processing. ","C. Deploy an AWS Storage Gateway volume gateway as a virtual machine (VM) on premises at each \nclinic. ","D. Attach an Amazon Elastic File System (Amazon EFS) file system to each clinic\u2019s on-premises servers. "],"Explanation":"Answer: A \n \nExplanation \nAWS Storage Gateway is a service that connects an on-premises software appliance with cloud-based storage \nto provide seamless and secure integration between an organization\u0027s on-premises IT environment and AWS\u0027s \nstorage infrastructure. By deploying a file gateway as a virtual machine on each clinic\u0027s premises, the medical \nresearch lab can provide low-latency access to the data stored in the S3 bucket while maintaining read-only \npermissions for each clinic. This solution allows the clinics to access the data files directly from their \non-premises file-based applications without the need for data transfer or migration. \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":118,"QuestionContent":"A transaction processing company has weekly scripted batch jobs that run on Amazon EC2 instances. The \n \n181 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nEC2 instances are in an Auto Scaling group. The number of transactions can vary but the beseline CPU \nutilization that is noted on each run is at least 60%. The company needs to provision the capacity 30 minutes \nbefore the jobs run. \n \nCurrently engineering complete this task by manually modifying the Auto Scaling group parameters. The \ncompany does not have the resources to analyze the required capacity trends for the Auto Scaling group \ncounts. The company needs an automated way to modify the Auto Scaling group\u2019s capacity. \n \nWhich solution will meet these requiements with the LEAST operational overhead? \n ","Option":["A. Ceate a dynamic scalling policy for the Auto Scaling group. Configure the policy to scale based on the \nCPU utilization metric to 60%. ","B. Create a scheduled scaling polcy for the Auto Scaling group. Set the appropriate desired capacity, \nminimum capacity, and maximum capacity. Set the recurrence to weekly. Set the start time to 30 \nminutes. Before the batch jobs run. ","C. Create a predictive scaling policy for the Auto Scaling group. Configure the policy to scale based on \nforecast. Set the scaling metric to CPU utilization. Set the target value for the metric to 60%. In the \nPolicy, set the instances to pre-launch 30 minutes before the jobs run. ","D. Create an Amazon EventBridge event to invoke an AWS Lamda function when the CPU utilization \nmetric value for the Auto Scaling group reaches 60%. Configure the Lambda function to increase the \nAuto Scaling group\u2019s desired capacity and maximum capacity by 20%. "],"Explanation":"Answer: C \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":119,"QuestionContent":" \nA company has a serverless website with millions of objects in an Amazon S3 bucket. The company uses the \nS3 bucket as the origin for an Amazon CloudFront distribution. The company did not set encryption on the S3 \nbucket before the objects were loaded. A solutions architect needs to enable encryption for all existing objects \nand for all objects that are added to the S3 bucket in the future. \n \nWhich solution will meet these requirements with the LEAST amount of effort? \n ","Option":["A. Create a new S3 bucket. Turn on the default encryption settings for the new S3 bucket. Download all \nexisting objects to temporary local storage. Upload the objects to the new S3 bucket. ","B. Turn on the default encryption settings for the S3 bucket. Use the S3 Inventory feature to create a .csv \nfile that lists the unencrypted objects. Run an S3 Batch Operations job that uses the copy command to \nencrypt those objects. ","C. Create a new encryption key by using AWS Key Management Service (AWS KMS). Change the \nsettings on the S3 bucket to use server-side encryption with AWS KMS managed encryption keys \n(SSE-KMS). Turn on versioning for the S3 bucket. ","D. Navigate to Amazon S3 in the AWS Management Console. Browse the S3 bucket\u2019s objects. Sort by the \nencryption field. Select each unencrypted object. Use the Modify button to apply default encryption \nsettings to every unencrypted object in the S3 bucket. \n182 of 230 Practice Test Amazon Web Services - SAA-C03 "],"Explanation":"Answer: B \n \nExplanation \nhttps://spin.atomicobject.com/2020/09/15/aws-s3-encrypt-existing-objects/ \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":120,"QuestionContent":" \nA solution architect needs to assign a new microsoft for a company\u2019s application. Clients must be able to call \nan HTTPS endpoint to reach the micoservice. The microservice also must use AWS identity and Access \nManagement (IAM) to authentication calls. The soltions architect will write the logic for this microservice by \nusing a single AWS Lambda function that is written in Go 1.x. \n \nWhich solution will deploy the function in the in the MOST operationally efficient way? \n ","Option":["A. Create an Amazon API Gateway REST API. Configure the method to use the Lambda function. Enable \nIAM authentication on the API. ","B. Create a Lambda function URL for the function. Specify AWS_IAM as the authentication type. ","C. Create an Amazon CloudFront distribution. Deploy the function to Lambda@Edge. Integrate IAM \nauthentication logic into the Lambda@Edge function. ","D. Create an Amazon CloudFront distribuion. Deploy the function to CloudFront Functions. Specify \nAWS_IAM as the authentication type. "],"Explanation":"Answer: A \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":121,"QuestionContent":" \nA company\u0027s facility has badge readers at every entrance throughout the building. When badges are scanned, \nthe readers send a message over HTTPS to indicate who attempted to access that particular entrance. \n \nA solutions architect must design a system to process these messages from the sensors. The solution must be \nhighly available, and the results must be made available for the company\u0027s security team to analyze. \n \nWhich system architecture should the solutions architect recommend? \n ","Option":["A. Launch an Amazon EC2 instance to serve as the HTTPS endpoint and to process the messages \nConfigure the EC2 instance to save the results to an Amazon S3 bucket. ","B. Create an HTTPS endpoint in Amazon API Gateway. Configure the API Gateway endpoint to invoke an \nAWS Lambda function to process the messages and save the results to an Amazon DynamoDB table. ","C. Use Amazon Route 53 to direct incoming sensor messages to an AWS Lambda function. Configure the \nLambda function to process the messages and save the results to an Amazon DynamoDB table. ","D. Create a gateway VPC endpoint for Amazon S3. Configure a Site-to-Site VPN connection from the \nfacility network to the VPC so that sensor data can be written directly to an S3 bucket by way of the \n183 of 230 Practice Test Amazon Web Services - SAA-C03 \nVPC endpoint. "],"Explanation":"Answer: B \n \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":122,"QuestionContent":" \nA company has multiple AWS accounts that use consolidated billing. The company runs several active high \nperformance Amazon RDS for Oracle On-Demand DB instances \n \nfor 90 days. The company\u0027s finance team has access to AWS Trusted Advisor in the consolidated billing \naccount and all other AWS accounts. \n \nThe finance team needs to use the appropriate AWS account to access the Trusted Advisor check \nrecommendations for RDS. The finance team must review the \n \nappropriate Trusted Advisor check to reduce RDS costs. \n \nWhich combination of steps should the finance team take to meet these requirements? (Select TWO.) \n ","Option":["A. Use the Trusted Advisor recommendations from the account where the RDS instances are running. ","B. Use the Trusted Advisor recommendations from the consolidated billing account to see all RDS instance \nchecks at the same time. ","C. Review the Trusted Advisor check for Amazon RDS Reserved Instance Optimization. ","D. Review the Trusted Advisor check for Amazon RDS Idle DB Instances. ","E. Review the Trusted Advisor check for Amazon Redshift Reserved Node Optimization. "],"Explanation":"Answer: B C \n \nExplanation \nB. Use the Trusted Advisor recommendations from the consolidated billing account to see all RDS instance \nchecks at the same time. \n \nThe consolidated billing account has access to all the other AWS accounts that use consolidated billing. Using \nthe Trusted Advisor recommendations from the consolidated billing account will allow the finance team to see \nall RDS instance checks for all accounts at the same time. \n \nC. Review the Trusted Advisor check for Amazon RDS Reserved Instance Optimization. \n \nThe Trusted Advisor check for Amazon RDS Reserved Instance Optimization provides recommendations for \npurchasing reserved instances to reduce RDS costs. By reviewing this check, the finance team can identify \nwhich RDS instances can be converted to reserved instances to save costs. \n \n","RightAnswer":["B","C"],"QuestionChoose":[]},{"QuestionNumber":123,"QuestionContent":" \n184 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \n \nA company is migrating a Linux-based web server group to AWS. The web servers must access files in a \nshared file store for some content. The company must not make any changes to the application. \n \nWhat should a solutions architect do to meet these requirements? \n ","Option":["A. Create an Amazon S3 Standard bucket with access to the web servers. ","B. Configure an Amazon CloudFront distribution with an Amazon S3 bucket as the origin. ","C. Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on all web \nservers. ","D. Configure a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume. Mount \nthe EBS volume to all web servers. "],"Explanation":"Answer: C \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":124,"QuestionContent":" \nA hospital needs to store patient records in an Amazon S3 bucket. The hospital\u0027s compliance team must ensure \nthat all protected health information (PHI) is encrypted in transit and at rest. The compliance team must \nadminister the encryption key for data at rest. \n \nWhich solution will meet these requirements? \n ","Option":["A. Create a public SSL/TLS certificate in AWS Certificate Manager (ACM). Associate the certificate with \nAmazon S3. Configure default encryption for each S3 bucket to use server-side encryption with AWS \nKMS keys (SSE-KMS). Assign the compliance team to manage the KMS keys. ","B. Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over \nHTTPS (TLS). Configure default encryption for each S3 bucket to use server-side encryption with S3 \nmanaged encryption keys (SSE-S3). Assign the compliance team to manage the SSE-S3 keys. ","C. Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over \nHTTPS (TLS). Configure default encryption for each S3 bucket to use server-side encryption with AWS \nKMS keys (SSE-KMS). Assign the compliance team to manage the KMS keys. ","D. Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over \nHTTPS (TLS). Use Amazon Macie to protect the sensitive data that is stored in Amazon S3. Assign the \ncompliance team to manage Macie. "],"Explanation":"Answer: C \nExplanation \nit allows the compliance team to manage the KMS keys used for server-side encryption, thereby providing the \nnecessary control over the encryption keys. Additionally, the use of the \u0022aws:SecureTransport\u0022 condition on \nthe bucket policy ensures that all connections to the S3 bucket are encrypted in transit. \n \n185 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":125,"QuestionContent":" \nAn online learning company is migrating to the AWS Cloud. The company maintains its student records in a \nPostgreSQL database. The company needs a solution in which its data is available and online across multiple \nAWS Regions at all times. \n \nWhich solution will meet these requirements with the LEAST amount of operational overhead? \n ","Option":["A. Migrate the PostgreSQL database to a PostgreSQL cluster on Amazon EC2 instances. ","B. Migrate the PostgreSQL database to an Amazon RDS for PostgreSQL DB instance with the Multi-AZ \nfeature turned on. ","C. Migrate the PostgreSQL database to an Amazon RDS for PostgreSQL DB instance. Create a read \nreplica in another Region. ","D. Migrate the PostgreSQL database to an Amazon RDS for PostgreSQL DB instance. Set up DB \nsnapshots to be copied to another Region. "],"Explanation":"Answer: C \n \nExplanation \n\u0022 online across multiple AWS Regions at all times\u0022. Currently only Read Replica supports cross-regions , \nMulti-AZ does not support cross-region (it works only in same region) \nhttps://aws.amazon.com/about-aws/whats-new/2018/01/amazon-rds-read-replicas-now-support-multi-az-deploy \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":126,"QuestionContent":" \nA data analytics company wants to migrate its batch processing system to AWS. The company receives \nthousands of small data files periodically during the day through FTP. A on-premises batch job processes the \ndata files overnight. However, the batch job takes hours to finish running. \n \nThe company wants the AWS solution to process incoming data files are possible with minimal changes to the \nFTP clients that send the files. The solution must delete the incoming data files the files have been processed \nsuccessfully. Processing for each file needs to take 3-8 minutes. \n \nWhich solution will meet these requirements in the MOST operationally efficient way? \n ","Option":["A. Use an Amazon EC2 instance that runs an FTP server to store incoming files as objects in Amazon S3 \nGlacier Flexible Retrieval. Configure a job queue in AWS Batch. Use Amazon EventBridge rules to \ninvoke the job to process the objects nightly from S3 Glacier Flexible Retrieval. Delete the objects after \nthe job has processed the objects. ","B. Use an Amazon EC2 instance that runs an FTP server to store incoming files on an Amazon Elastic \nBlock Store (Amazon EBS) volume. Configure a job queue in AWS Batch. Use Amazon EventBridge \nrules to invoke the process the files nightly from the EBS volume. Delete the files after the job has \nprocessed the files. ","C. Use AWS Transfer Family to create an FTP server to store incoming files on an Amazon Elastic Block \n186 of 230 Practice Test Amazon Web Services - SAA-C03 \nStore (Amazon EBS) volume. Configure a job queue in AWS Batch. Use an Amazon S3 event \nnotification when each files arrives to invoke the job in AWS Batch. Delete the files after the job has \nprocessed the files. ","D. Use AWS Transfer Family to create an FTP server to store incoming files in Amazon S3 Standard. \nCreate an AWS Lambda function to process the files and to delete the files after they are proessed.yse \nan S3 event notification to invoke the lambda function when the fils arrive "],"Explanation":"Answer: C \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":127,"QuestionContent":" \nA company wants to run an in-memory database for a latency-sensitive application that runs on Amazon EC2 \ninstances. The application processes more than 100,000 transactions each minute and requires high network \nthroughput. A solutions architect needs to provide a cost-effective network design that minimizes data transfer \ncharges. \n \nWhich solution meets these requirements? \n ","Option":["A. Launch all EC2 instances in the same Availability Zone within the same AWS Region. Specify a \nplacement group with cluster strategy when launching EC2 instances. ","B. Launch all EC2 instances in different Availability Zones within the same AWS Region. Specify a \nplacement group with partition strategy when launching EC2 instances. ","C. Deploy an Auto Scaling group to launch EC2 instances in different Availability Zones based on a \nnetwork utilization target. ","D. Deploy an Auto Scaling group with a step scaling policy to launch EC2 instances in different \nAvailability Zones. "],"Explanation":"Answer: A \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":128,"QuestionContent":" \nA company runs a fleet of web servers using an Amazon RDS for PostgreSQL DB instance After a routine \ncompliance check, the company sets a standard that requires a recovery pant objective (RPO) of less than 1 \nsecond for all its production databases. \n \nWhich solution meets these requirement? \n ","Option":["A. Enable a Multi-AZ deployment for the DB Instance ","B. Enable auto scaling for the OB instance m one Availability Zone. ","C. Configure the 06 instance in one Availability Zone and create multiple read replicas in a separate \nAvailability Zone ","D. Configure the 06 instance m one Availability Zone, and configure AWS Database Migration Service \n187 of 230 Practice Test Amazon Web Services - SAA-C03 \n(AWS DMS) change data capture (CDC) tasks "],"Explanation":"Answer: A \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":129,"QuestionContent":" \nA solutions architect is implementing a document review application using an Amazon S3 bucket for storage. \nThe solution must prevent accidental deletion of the documents and ensure that all versions of the documents \nare available. Users must be able to download, modify, and upload documents. \n \nWhich combination of actions should be taken to meet these requirements? (Choose two.) \n ","Option":["A. Enable a read-only bucket ACL. ","B. Enable versioning on the bucket. ","C. Attach an IAM policy to the bucket. ","D. Enable MFA Delete on the bucket. ","E. Encrypt the bucket using AWS KMS. "],"Explanation":"Answer: B D \n \n \n","RightAnswer":["B","D"],"QuestionChoose":[]},{"QuestionNumber":130,"QuestionContent":" \nA company has an On-premises volume backup solution that has reached its end of life. The company wants to \nuse AWS as part of a new backup solution and wants to maintain local access to all the data while it is backed \nup on AWS. The company wants to ensure that the data backed up on AWS is automatically and securely \ntransferred. \n \nWhich solution meets these requirements? \n ","Option":["A. Use AWS Snowball to migrate data out of the on-premises solution to Amazon S3. Configure \non-premises systems to mount the Snowball S3 endpoint to provide local access to the data. ","B. Use AWS Snowball Edge to migrate data out of the on-premises solution to Amazon S3.Use the \nSnowball Edge file interface to provide on-premises systems with local access to the data. ","C. Use AWS Storage Gateway and configure a cached volume gateway. Run the Storage Gateway software \napplication on premises and configure a percentage of data to cache locally. Mount the gateway storage \nvolumes to provide local access to the data. ","D. Use AWS Storage Gateway and configure a stored volume gateway. Run the Storage software \napplication on premises and map the gateway storage volumes to on-premises storage. Mount the \ngateway storage volumes to provide local access to the data. "],"Explanation":"Answer: C \n \n188 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \n \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":131,"QuestionContent":" \nA company is concerned that two NAT instances in use will no longer be able to support the traffic needed for \nthe company\u2019s application. A solutions architect wants to implement a solution that is highly available, fault \ntolerant, and automatically scalable. \n \nWhat should the solutions architect recommend? \n ","Option":["A. Remove the two NAT instances and replace them with two NAT gateways in the same Availability \nZone. ","B. Use Auto Scaling groups with Network Load Balancers for the NAT instances in different Availability \nZones. ","C. Remove the two NAT instances and replace them with two NAT gateways in different Availability \nZones. ","D. Replace the two NAT instances with Spot Instances in different Availability Zones and deploy a \nNetwork Load Balancer. "],"Explanation":"Answer: C \n \nExplanation \nIf you have resources in multiple Availability Zones and they share one NAT gateway, and if the NAT \ngateway\u2019s Availability Zone is down, resources in the other Availability Zones lose internet access. To create \nan Availability Zone-independent architecture, create a NAT gateway in each Availability Zone and configure \nyour routing to ensure that resources use the NAT gateway in the same Availability Zone. \nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html#nat-gateway-basics \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":132,"QuestionContent":" \nA company hosts a marketing website in an on-premises data center. The website consists of static documents \nand runs on a single server. An administrator updates the website content infrequently and uses an SFTP client \nto upload new documents. \n \nThe company decides to host its website on AWS and to use Amazon CloudFront. The company\u0027s solutions \narchitect creates a CloudFront distribution. The solutions architect must design the most cost-effective and \nresilient architecture for website hosting to serve as the CloudFront origin. \n \nWhich solution will meet these requirements? \n ","Option":["A. Create a virtual server by using Amazon Lightsail. Configure the web server in the Lightsail instance. \nUpload website content by using an SFTP client. ","B. Create an AWS Auto Scaling group for Amazon EC2 instances. Use an Application Load Balancer. \nUpload website content by using an SFTP client. ","C. Create a private Amazon S3 bucket. Use an S3 bucket policy to allow access from a CloudFront origin \n189 of 230 Practice Test Amazon Web Services - SAA-C03 \naccess identity (OAI). Upload website content by using theAWSCLI. ","D. Create a public Amazon S3 bucket. Configure AWS Transfer for SFTP. Configure the S3 bucket for \nwebsite hosting. Upload website content by using the SFTP client. "],"Explanation":"Answer: C \n \nExplanation \nhttps://docs.aws.amazon.com/cli/latest/reference/transfer/describe-server.html \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":133,"QuestionContent":" \nA company needs to create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster to host a digital \nmedia streaming application. The EKS cluster will use a managed node group that is backed by Amazon \nElastic Block Store (Amazon EBS) volumes for storage. The company must encrypt all data at rest by using a \ncustomer managed key that is stored in AWS Key Management Service (AWS KMS) \n \nWhich combination of actions will meet this requirement with the LEAST operational overhead? (Select \nTWO.) \n ","Option":["A. Use a Kubernetes plugin that uses the customer managed key to perform data encryption. ","B. After creation of the EKS cluster, locate the EBS volumes. Enable encryption by using the customer \nmanaged key. ","C. Enable EBS encryption by default in the AWS Region where the EKS cluster will be created. Select the \ncustomer managed key as the default key. ","D. Create the EKS cluster Create an IAM role that has cuwlicy that grants permission to the customer \nmanaged key. Associate the role with the EKS cluster. ","E. Store the customer managed key as a Kubernetes secret in the EKS cluster. Use the customer managed \nkey to encrypt the EBS volumes. "],"Explanation":"Answer: A D \n \n","RightAnswer":["A","D"],"QuestionChoose":[]},{"QuestionNumber":134,"QuestionContent":" \nA company is using Amazon CloudFront with this website. The company has enabled logging on the \nCloudFront distribution, and logs are saved in one of the company\u0027s Amazon S3 buckets The company needs \nto perform advanced analyses on the logs and build visualizations \n \nWhat should a solutions architect do to meet these requirements\u0027? \n ","Option":["A. Use standard SQL queries in Amazon Athena to analyze the CloudFront togs in the S3 bucket Visualize \nthe results with AWS Glue ","B. Use standard SQL queries in Amazon Athena to analyze the CloudFront togs in the S3 bucket Visualize \n190 of 230 Practice Test Amazon Web Services - SAA-C03 \nthe results with Amazon QuickSight ","C. Use standard SQL queries in Amazon DynamoDB to analyze the CloudFront logs m the S3 bucket \nVisualize the results with AWS Glue ","D. Use standard SQL queries in Amazon DynamoDB to analyze the CtoudFront logs m the S3 bucket \nVisualize the results with Amazon QuickSight "],"Explanation":"Answer: D \n \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":135,"QuestionContent":" \nA company is deploying a two-tier web application in a VPC. The web tier is using an Amazon EC2 Auto \nScaling group with public subnets that span multiple Availability Zones. The database tier consists of an \nAmazon RDS for MySQL DB instance in separate private subnets. The web tier requires access to the \ndatabase to retrieve product information. \n \nThe web application is not working as intended. The web application reports that it cannot connect to the \ndatabase. The database is confirmed to be up and running. All configurations for the network ACLs. security \ngroups, and route tables are still in their default states. \n \nWhat should a solutions architect recommend to fix the application? \n ","Option":["A. Add an explicit rule to the private subnet\u0027s network ACL to allow traffic from the web tier\u0027s EC2 \ninstances. ","B. Add a route in the VPC route table to allow traffic between the web tier\u0027s EC2 instances and Ihe \ndatabase tier. ","C. Deploy the web tier\u0027s EC2 instances and the database tier\u0027s RDS instance into two separate VPCs. and \nconfigure VPC peering. ","D. Add an inbound rule to the security group of the database tier\u0027s RDS instance to allow traffic from the \nweb tier\u0027s security group. "],"Explanation":"Answer: D \n \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":136,"QuestionContent":" \nA company is using a centralized AWS account to store log data in various Amazon S3 buckets. A solutions \narchitect needs to ensure that the data is encrypted at rest before the data is uploaded to the S3 buckets. The \ndata also must be encrypted in transit. \n \nWhich solution meets these requirements? \n ","Option":["A. Use client-side encryption to encrypt the data that is being uploaded to the S3 buckets. \n191 of 230 Practice Test Amazon Web Services - SAA-C03 ","B. Use server-side encryption to encrypt the data that is being uploaded to the S3 buckets. ","C. Create bucket policies that require the use of server-side encryption with S3 managed encryption keys \n(SSE-S3) for S3 uploads. ","D. Enable the security option to encrypt the S3 buckets through the use of a default AWS Key Management \nService (AWS KMS) key. "],"Explanation":"Answer: A \n \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":137,"QuestionContent":" \nA company is running a publicly accessible serverless application that uses Amazon API Gateway and AWS \nLambda. The application\u0027s traffic recently spiked due to fraudulent requests from botnets. \n \nWhich steps should a solutions architect take to block requests from unauthorized users? (Select TWO.) \n ","Option":["A. Create a usage plan with an API key that is shared with genuine users only. ","B. Integrate logic within the Lambda function to ignore the requests from fraudulent IP addresses. ","C. Implement an AWS WAF rule to target malicious requests and trigger actions to filter them out. ","D. Convert the existing public API to a private API. Update the DNS records to redirect users to the new \nAPI endpoint. ","E. Create an IAM role for each user attempting to access the API. A user will assume the role when \nmaking the API call. "],"Explanation":"Answer: A C \n \nExplanation \nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html#:~:text=Don \nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html \n \n","RightAnswer":["A","C"],"QuestionChoose":[]},{"QuestionNumber":138,"QuestionContent":" \nA company has a web application hosted over 10 Amazon EC2 instances with traffic directed by Amazon \nRoute 53. The company occasionally experiences a timeout error when attempting to browse the application. \nThe networking team finds that some DNS queries return IP addresses of unhealthy instances, resulting in the \ntimeout error. \n \nWhat should a solutions architect implement to overcome these timeout errors? \n ","Option":["A. Create a Route 53 simple routing policy record for each EC2 instance. Associate a health check with \neach record. \n192 of 230 Practice Test Amazon Web Services - SAA-C03 ","B. Create a Route 53 failover routing policy record for each EC2 instance. Associate a health check with \neach record. ","C. Create an Amazon CloudFront distribution with EC2 instances as its origin. Associate a health check \nwith the EC2 instances. ","D. Create an Application Load Balancer (ALB) with a health check in front of the EC2 instances. Route to \nthe ALB from Route 53. "],"Explanation":"Answer: D \n \nExplanation \nAn Application Load Balancer (ALB) allows you to distribute incoming traffic across multiple backend \ninstances, and can automatically route traffic to healthy instances while removing traffic from unhealthy \ninstances. By using an ALB in front of the EC2 instances and routing traffic to it from Route 53, the load \nbalancer can perform health checks on the instances and only route traffic to healthy instances, which should \nhelp to reduce or eliminate timeout errors caused by unhealthy instances. \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":139,"QuestionContent":" \nA company is planning to store data on Amazon RDS DB instances. The company must encrypt the data at \nrest. \n \nWhat should a solutions architect do to meet this requirement? \n ","Option":["A. Create an encryption key and store the key in AWS Secrets Manager Use the key to encrypt the DB \ninstances ","B. Generate a certificate in AWS Certificate Manager (ACM). Enable SSL/TLS on the DB instances by \nusing the certificate ","C. Create a customer master key (CMK) in AWS Key Management Service (AWS KMS) Enable \nencryption for the DB instances ","D. Generate a certificate in AWS Identity and Access Management {IAM) Enable SSUTLS on the DB \ninstances by using the certificate "],"Explanation":"Answer: C \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":140,"QuestionContent":" \nA company wants to use Amazon S3 for the secondary copy of its on-premises dataset. The company would \nrarely need to access this copy. The storage solution\u2019s cost should be minimal. \n \nWhich storage solution meets these requirements? \n ","Option":["A. S3 Standard \n193 of 230 Practice Test Amazon Web Services - SAA-C03 ","B. S3 Intelligent-Tiering ","C. S3 Standard-Infrequent Access (S3 Standard-IA) ","D. S3 One Zone-Infrequent Access (S3 One Zone-IA) "],"Explanation":"Answer: C \n \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":141,"QuestionContent":" \nA company will deployed a web application on AWS. The company hosts the backend database on Amazon \nRDS for MySQL with a primary DB instance and five read replicas to support scaling needs. The read replicas \nmust log no more than 1 second bahind the primary DB Instance. The database routinely runs scheduled stored \nprocedures. \n \nAs traffic on the website increases, the replicas experinces addtional lag during periods of peak lead. A \nsolutions architect must reduce the replication lag as much as possible. The solutions architect must minimize \nchanges to the applicatin code and must minimize ongoing overhead. \n \nWhich solution will meet these requirements? \n \nMigrate the database to Amazon Aurora MySQL. Replace the read replicas with Aurora Replicas, and \nconfigure Aurora Auto Scaling. Replace the stored procedures with Aurora MySQL native functions. \n \nDeploy an Amazon ElasticCache for Redis cluser in front of the database. Modify the application to check the \ncache before the application queries the database. Repace the stored procedures with AWS Lambda funcions. \n ","Option":["A. Migrate the database to a MYSQL database that runs on Amazn EC2 instances. Choose large, compute \noptimized for all replica nodes. Maintain the stored procedures on the EC2 instances. ","B. Deploy an Amazon ElastiCache for Redis cluster in fornt of the database. Modify the application to \ncheck the cache before the application queries the database. Replace the stored procedures with AWS \nLambda functions. ","C. Migrate the database to a MySQL database that runs on Amazon EC2 instances. Choose large, compute \noptimized EC2 instances for all replica nodes, Maintain the stored procedures on the EC2 instances. ","D. Migrate the database to Amazon DynamoDB, Provision number of read capacity units (RCUs) to \nsupport the required throughput, and configure on-demand capacity scaling. Replace the stored \nprocedures with DynamoDB streams. "],"Explanation":"Answer: A \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":142,"QuestionContent":" \nA solutions architect is designing the architecture for a software demonstration environment The environment \nwill run on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB) The \nsystem will experience significant increases in traffic during working hours but Is not required to operate on \n \n194 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nweekends. \n \nWhich combination of actions should the solutions architect take to ensure that the system can scale to meet \ndemand? (Select TWO) \n ","Option":["A. Use AWS Auto Scaling to adjust the ALB capacity based on request rate ","B. Use AWS Auto Scaling to scale the capacity of the VPC internet gateway ","C. Launch the EC2 instances in multiple AWS Regions to distribute the load across Regions ","D. Use a target tracking scaling policy to scale the Auto Scaling group based on instance CPU utilization ","E. Use scheduled scaling to change the Auto Scaling group minimum, maximum, and desired capacity to \nzero for weekends Revert to the default values at the start of the week "],"Explanation":"Answer: D E \n \n \n","RightAnswer":["D","E"],"QuestionChoose":[]},{"QuestionNumber":143,"QuestionContent":" \nA company runs an application on a group of Amazon Linux EC2 instances. For compliance reasons, the \ncompany must retain all application log files for 7 years. The log files will be analyzed by a reporting tool that \nmust be able to access all the files concurrently. \n \nWhich storage solution meets these requirements MOST cost-effectively? \n ","Option":["A. Amazon Elastic Block Store (Amazon EBS) ","B. Amazon Elastic File System (Amazon EFS) ","C. Amazon EC2 instance store ","D. Amazon S3 "],"Explanation":"Answer: D \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":144,"QuestionContent":" \nA company needs to provide its employee with secure access to confidential and sensitive files. The company \nwants to ensure that the files can be accessed only by authorized users. The files must be downloaded security \nto the employees devices. \n \nThe files are stored in an on-premises Windows files server. However, due to an increase in remote usage, the \nfile server out of capacity. \n \nWhich solution will meet these requirement? \n ","Option":["A. Migrate the file server to an Amazon EC2 instance in a public subnet. Configure the security group to \nlimit inbound traffic to the employees \u201AIP addresses. \n195 of 230 Practice Test Amazon Web Services - SAA-C03 ","B. Migrate the files to an Amazon FSx for Windows File Server file system. Integrate the Amazon FSx file \nsystem with the on-premises Active Directory Configure AWS Client VPN. ","C. Migrate the files to Amazon S3, and create a private VPC endpoint. Create a signed URL to allow \ndownload. ","D. Migrate the files to Amazon S3, and create a public VPC endpoint Allow employees to sign on with \nAWS IAM identity Center (AWS Sing-On). "],"Explanation":"Answer: C \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":145,"QuestionContent":" \nA company has an application that collects data from loT sensors on automobiles. The data is streamed and \nstored in Amazon S3 through Amazon Kinesis Date Firehose The data produces trillions of S3 objects each \nyear. Each morning, the company uses the data from the previous 30 days to retrain a suite of machine \nlearning (ML) models. \n \nFour times each year, the company uses the data from the previous 12 months to perform analysis and train \nother ML models The data must be available with minimal delay for up to 1 year. After 1 year, the data must \nbe retained for archival purposes. \n \nWhich storage solution meets these requirements MOST cost-effectively? \n ","Option":["A. Use the S3 Intelligent-Tiering storage class. Create an S3 Lifecycle policy to transition objects to S3 \nGlacier Deep Archive after 1 year ","B. Use the S3 Intelligent-Tiering storage class. Configure S3 Intelligent-Tiering to automatically move \nobjects to S3 Glacier Deep Archive after 1 year. ","C. Use the S3 Standard-Infrequent Access (S3 Standard-IA) storage class. Create an S3 Lifecycle policy to \ntransition objects to S3 Glacier Deep Archive after 1 year. ","D. Use the S3 Standard storage class. Create an S3 Lifecycle policy to transition objects to S3 \nStandard-Infrequent Access (S3 Standard-IA) after 30 days, and then to S3 Glacier Deep Archive after 1 \nyear. "],"Explanation":"Answer: B \n \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":146,"QuestionContent":" \nA company wants to deploy a new public web application on AWS The application includes a web server tier \nthat uses Amazon EC2 instances The application also includes a database tier that uses an Amazon RDS for \nMySQL DB instance \n \nThe application must be secure and accessible for global customers that have dynamic IP addresses \nHow should a solutions architect configure the security groups to meet these requirements\u0027? \n \n196 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n ","Option":["A. Configure the security group tor the web servers lo allow inbound traffic on port 443 from 0.0.0. 0/0) \nConfigure the security group for the DB instance to allow inbound traffic on port 3306 from the security \ngroup of the web servers ","B. Configure the security group for the web servers to allow inbound traffic on port 443 from the IP \naddresses of the customers Configure the security group for the DB instance lo allow inbound traffic on \nport 3306 from the security group of the web servers ","C. Configure the security group for the web servers to allow inbound traffic on port 443 from the IP \naddresses of the customers Configure the security group for the DB instance to allow inbound traffic on \nport 3306 from the IP addresses of the customers ","D. Configure the security group for the web servers to allow inbound traffic on port 443 from 0.0.0.0.0 \nConfigure the security group for the DB instance to allow inbound traffic on port 3306 from 0.0.0.0/0) "],"Explanation":"Answer: A \n \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":147,"QuestionContent":" \nA company is moving its on-premises Oracle database to Amazon Aurora PostgreSQL. The database has \nseveral applications that write to the same tables. The applications need to be migrated one by one with a \nmonth in between each migration. Management has expressed concerns that the database has a high number of \nreads and writes. The data must be kept in sync across both databases throughout the migration. \n \nWhat should a solutions architect recommend? \n ","Option":["A. Use AWS DataSync for the initial migration. Use AWS Database Migration Service (AWS DMS) to \ncreate a change data capture (CDC) replication task and a table mapping to select all tables. ","B. Use AWS DataSync for the initial migration. Use AWS Database Migration Service (AWS DMS) to \ncreate a full load plus change data capture (CDC) replication task and a table mapping to select all \ntables. ","C. Use the AWS Schema Conversion Tool with AWS Database Migration Service (AWS DMS) using a \nmemory optimized replication instance. Create a full load plus change data capture (CDC) replication \ntask and a table mapping to select all tables. ","D. Use the AWS Schema Conversion Tool with AWS Database Migration Service (AWS DMS) using a \ncompute optimized replication instance. Create a full load plus change data capture (CDC) replication \ntask and a table mapping to select the largest tables. "],"Explanation":"Answer: C \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":148,"QuestionContent":" \nAn Amazon EC2 instance is located in a private subnet in a new VPC. This subnet does not have outbound \ninternet access, but the EC2 instance needs the ability to download monthly security updates from an outside \nvendor. \n \n197 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nWhat should a solutions architect do to meet these requirements? \n ","Option":["A. Create an internet gateway, and attach it to the VPC. Configure the private subnet route table to use the \ninternet gateway as the default route. ","B. Create a NAT gateway, and place it in a public subnet. Configure the private subnet route table to use \nthe NAT gateway as the default route. ","C. Create a NAT instance, and place it in the same subnet where the EC2 instance is located. Configure the \nprivate subnet route table to use the NAT instance as the default route. ","D. Create an internet gateway, and attach it to the VPC. Create a NAT instance, and place it in the same \nsubnet where the EC2 instance is located. Configure the private subnet route table to use the internet \ngateway as the default route. "],"Explanation":"Answer: B \n \nExplanation \nThis approach will allow the EC2 instance to access the internet and download the monthly security updates \nwhile still being located in a private subnet. By creating a NAT gateway and placing it in a public subnet, it \nwill allow the instances in the private subnet to access the internet through the NAT gateway. And then, \nconfigure the private subnet route table to use the NAT gateway as the default route. This will ensure that all \noutbound traffic is directed through the NAT gateway, allowing the EC2 instance to access the internet while \nstill maintaining the security of the private subnet. \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":149,"QuestionContent":" \nA company uses a legacy application to produce data in CSV format The legacy application stores the output \ndata In Amazon S3 The company is deploying a new commercial off-the-shelf (COTS) application that can \nperform complex SQL queries to analyze data that is stored Amazon Redshift and Amazon S3 only However \nthe COTS application cannot process the csv files that the legacy application produces The company cannot \nupdate the legacy application to produce data in another format The company needs to implement a solution so \nthat the COTS application can use the data that the legacy applicator produces. \n \nWhich solution will meet these requirements with the LEAST operational overhead? \n ","Option":["A. Create a AWS Glue extract, transform, and load (ETL) job that runs on a schedule. Configure the ETL \njob to process the .csv files and store the processed data in Amazon Redshit. ","B. Develop a Python script that runs on Amazon EC2 instances to convert the. csv files to sql files invoke \nthe Python script on cron schedule to store the output files in Amazon S3. ","C. Create an AWS Lambda function and an Amazon DynamoDB table. Use an S3 event to invoke the \nLambda function. Configure the Lambda function to perform an extract transform, and load (ETL) job \nto process the .csv files and store the processed data in the DynamoDB table. ","D. Use Amazon EventBridge (Amazon CloudWatch Events) to launch an Amazon EMR cluster on a \nweekly schedule. Configure the EMR cluster to perform an extract, tractform, and load (ETL) job to \nprocess the .csv files and store the processed data in an Amazon Redshift table. \n198 of 230 Practice Test Amazon Web Services - SAA-C03 "],"Explanation":"Answer: C \n \nExplanation \nAccording to the Amazon website, Amazon S3 Select is an Amazon S3 feature that enables applications to \nretrieve only a subset of data from an object. It offers an efficient way to access data stored in Amazon S3 and \ncan significantly improve query performance, save money, and increase the scalability of applications that \nfrequently access data in S3. S3 Select allows applications to retrieve only the data that is needed, instead of \nthe entire object, and supports SQL expressions, CSV, and JSON. Additionally, S3 Select can be used to query \nobjects stored in the S3 Glacier storage class. The exact text from the Amazon website about S3 Select is: \n \n\u0022Amazon S3 Select is an Amazon S3 feature that enables applications to retrieve only a subset of data from an \nobject. It offers an efficient way to access data stored in Amazon S3 and can significantly improve query \nperformance, save money, and increase the scalability of applications that frequently access data in S3. S3 \nSelect allows applications to retrieve only the data that is needed, instead of the entire object, and supports \nSQL expressions, CSV, and JSON. Additionally, S3 Select can be used to query objects stored in the S3 \nGlacier storage class.\u0022 \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":150,"QuestionContent":" \nA company is launching an application on AWS. The application uses an Application Load (ALB) to direct \ntraffic to at least two Amazon EC2 instances in a single target group. \n \nThe instances are in an Auto Scaling group for each environment. The company requires a development and a \nproduction environment. The production environment will have periods of high traffic. \n \nWhich solution will configure the development environment MOST cost-effectively? \n ","Option":["A. Reconfigure the target group in the development environment to have one EC2 instance as a target. ","B. Change the ALB balancing algorithm to least outstanding requests. ","C. Reduce the size of the EC2 instances in both environments. ","D. Reduce the maximum number of EC2 instances in the development environment\u2019s Auto Scaling group "],"Explanation":"Answer: D \n \nExplanation \nThis option will configure the development environment in the most cost-effective way as it reduces the \nnumber of instances running in the development environment and therefore reduces the cost of running the \napplication. The development environment typically requires less resources than the production environment, \nand it is unlikely that the development environment will have periods of high traffic that would require a large \nnumber of instances. By reducing the maximum number of instances in the development environment\u0027s Auto \nScaling group, the company can save on costs while still maintaining a functional development environment. \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":151,"QuestionContent":" \n199 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nA company recently announced the deployment of its retail website to a global audience. The website runs on \nmultiple Amazon EC2 instances behind an Elastic Load Balancer. The instances run in an Auto Scaling group \nacross multiple Availability Zones. \n \nThe company wants to provide its customers with different versions of content based on the devices that the \ncustomers use to access the website. \n \nWhich combination of actions should a solutions architect take to meet these requirements? (Choose two.) \n ","Option":["A. Configure Amazon CloudFront to cache multiple versions of the content. ","B. Configure a host header in a Network Load Balancer to forward traffic to different instances. ","C. Configure a Lambda@Edge function to send specific objects to users based on the User-Agent header. ","D. Configure AWS Global Accelerator. Forward requests to a Network Load Balancer (NLB). Configure \nthe NLB to set up host-based routing to different EC2 instances. ","E. Configure AWS Global Accelerator. Forward requests to a Network Load Balancer (NLB). Configure \nthe NLB to set up path-based routing to different EC2 instances. "],"Explanation":"Answer: A C \n \nExplanation \nFor C: IMPROVED USER EXPERIENCE Lambda@Edge can help improve your users\u0027 experience with your \nwebsites and web applications across the world, by letting you personalize content for them without sacrificing \nperformance. Real-time Image Transformation You can customize your users\u0027 experience by transforming \nimages on the fly based on the user characteristics. For example, you can resize images based on the viewer\u0027s \ndevice type\u2014mobile, desktop, or tablet. You can also cache the transformed images at CloudFront Edge \nlocations to further improve performance when delivering images. https://aws.amazon.com/lambda/edge/ \n \n","RightAnswer":["A","C"],"QuestionChoose":[]},{"QuestionNumber":152,"QuestionContent":" \nA company\u2019s reporting system delivers hundreds of .csv files to an Amazon S3 bucket each day. The company \nmust convert these files to Apache Parquet format and must store the files in a transformed data bucket. \n \nWhich solution will meet these requirements with the LEAST development effort? \n ","Option":["A. Create an Amazon EMR cluster with Apache Spark installed. Write a Spark application to transform the \ndata. Use EMR File System (EMRFS) to write files to the transformed data bucket. ","B. Create an AWS Glue crawler to discover the data. Create an AWS Glue extract, transform, and load \n(ETL) job to transform the data. Specify the transformed data bucket in the output step. ","C. Use AWS Batch to create a job definition with Bash syntax to transform the data and output the data to \nthe transformed data bucket. Use the job definition to submit a job. Specify an array job as the job type. ","D. Create an AWS Lambda function to transform the data and output the data to the transformed data \nbucket. Configure an event notification for the S3 bucket. Specify the Lambda function as the \n200 of 230 Practice Test Amazon Web Services - SAA-C03 \ndestination for the event notification. "],"Explanation":"Answer: B \n \nExplanation \nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/three-aws-glue-etl-job-types-for-converting-d \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":153,"QuestionContent":"A company provides an API to its users that automates inquiries for tax computations based on item prices. \nThe company experiences a larger number of inquiries during the holiday season only that cause slower \nresponse times. A solutions architect needs to design a solution that is scalable and elastic. \n \nWhat should the solutions architect do to accomplish this? \n ","Option":["A. Provide an API hosted on an Amazon EC2 instance. The EC2 instance performs the required \ncomputations when the API request is made. ","B. Design a REST API using Amazon API Gateway that accepts the item names. API Gateway passes item \nnames to AWS Lambda for tax computations. ","C. Create an Application Load Balancer that has two Amazon EC2 instances behind it. The EC2 instances \nwill compute the tax on the received item names. ","D. Design a REST API using Amazon API Gateway that connects with an API hosted on an Amazon EC2 \ninstance. API Gateway accepts and passes the item names to the EC2 instance for tax computations. "],"Explanation":"Answer: B \n \nExplanation \nLambda server-less is scalable and elastic than EC2 api gateway solution \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":154,"QuestionContent":" \nA company wants to manage Amazon Machine Images (AMIs). The company currently copies AMIs to the \nsame AWS Region where the AMIs were created. The company needs to design an application that captures \nAWS API calls and sends alerts whenever the Amazon EC2 CreateImage API operation is called within the \ncompany\u2019s account. \n \nWhich solution will meet these requirements with the LEAST operational overhead? \n ","Option":["A. Create an AWS Lambda function to query AWS CloudTrail logs and to send an alert when a \nCreateImage API call is detected. ","B. Configure AWS CloudTrail with an Amazon Simple Notification Service (Amazon SNS) notification \nthat occurs when updated logs are sent to Amazon S3. Use Amazon Athena to create a new table and to \nquery on CreateImage when an API call is detected. \n201 of 230 Practice Test Amazon Web Services - SAA-C03 ","C. Create an Amazon EventBridge (Amazon CloudWatch Events) rule for the CreateImage API call. \nConfigure the target as an Amazon Simple Notification Service (Amazon SNS) topic to send an alert \nwhen a CreateImage API call is detected. ","D. Configure an Amazon Simple Queue Service (Amazon SQS) FIFO queue as a target for AWS \nCloudTrail logs. Create an AWS Lambda function to send an alert to an Amazon Simple Notification \nService (Amazon SNS) topic when a CreateImage API call is detected. "],"Explanation":"Answer: C \nExplanation \nhttps://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/monitor-ami-events.html#:~:text=For%20exampl \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":155,"QuestionContent":"A company hosts a serverless application on AWS. The application uses Amazon API Gateway, AWS \nLambda, and an Amazon RDS for PostgreSQL database. The company notices an increase in application \nerrors that result from database connection timeouts during times Of peak traffic or unpredictable traffic. The \ncompany needs a solution that reduces the application failures with the least amount of change to the code. \n \nWhat should a solutions architect do to meet these requirements? \n ","Option":["A. Reduce the Lambda concurrency rate. ","B. Enable RDS Proxy on the RDS DB instance. ","C. Resize the RDS DB instance class to accept more connections. ","D. Migrate the database to Amazon DynamoDB with on-demand scaling. "],"Explanation":"Answer: B \n \nExplanation \nUsing RDS Proxy, you can handle unpredictable surges in database traffic. Otherwise, these surges might \ncause issues due to oversubscribing connections or creating new connections at a fast rate. RDS Proxy \nestablishes a database connection pool and reuses connections in this pool. This approach avoids the memory \nand CPU overhead of opening a new database connection each time. To protect the database against \noversubscription, you can control the number of database connections that are created. \nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-proxy.html \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":156,"QuestionContent":" \nA company wants to migrate an Oracle database to AWS. The database consists of a single table that contains \nmillions of geographic information systems (GIS) images that are high resolution and are identified by a \ngeographic code. \n \nWhen a natural disaster occurs tens of thousands of images get updated every few minutes. Each geographic \ncode has a single image or row that is associated with it. The company wants a solution that is highly available \n \n202 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nand scalable during such events \n \nWhich solution meets these requirements MOST cost-effectively? \n ","Option":["A. Store the images and geographic codes in a database table Use Oracle running on an Amazon RDS \nMulti-AZ DB instance ","B. Store the images in Amazon S3 buckets Use Amazon DynamoDB with the geographic code as the key \nand the image S3 URL as the value ","C. Store the images and geographic codes in an Amazon DynamoDB table Configure DynamoDB \nAccelerator (DAX) during times of high load ","D. Store the images in Amazon S3 buckets Store geographic codes and image S3 URLs in a database table \nUse Oracle running on an Amazon RDS Multi-AZ DB instance. "],"Explanation":"Answer: A \n \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":157,"QuestionContent":" \nA company is building a data analysis platform on AWS by using AWS Lake Formation. The platform will \ningest data from different sources such as Amazon S3 and Amazon RDS. The company needs a secure \nsolution to prevent access to portions of the data that contain sensitive information. \n ","Option":["A. Create an IAM role that includes permissions to access Lake Formation tables. ","B. Create data filters to implement row-level security and cell-level security. ","C. Create an AWS Lambda function that removes sensitive information before Lake Formation ingests re \ndata. ","D. Create an AWS Lambda function that perodically Queries and removes sensitive information from Lake \nFormation tables. "],"Explanation":"Answer: A \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":158,"QuestionContent":" \nA company wants to migrate its 1 PB on-premises image repository to AWS. The images will be used by a \nserverless web application Images stored in the repository are rarely accessed, but they must be immediately \navailable Additionally, the images must be encrypted at rest and protected from accidental deletion \n \nWhich solution meets these requirements? \n ","Option":["A. Implement client-side encryption and store the images in an Amazon S3 Glacier vault Set a vault lock to \nprevent accidental deletion ","B. Store the images in an Amazon S3 bucket in the S3 Standard-Infrequent Access (S3 Standard-IA) \n203 of 230 Practice Test Amazon Web Services - SAA-C03 \nstorage class Enable versioning default encryption and MFA Delete on the S3 bucket. ","C. Store the images in an Amazon FSx for Windows File Server file share Configure the Amazon FSx file \nshare to use an AWS Key Management Service (AWS KMS) customer master key (CMK) to encrypt \nthe images in the file share Use NTFS permission sets on the images to prevent accidental deletion ","D. Store the images in an Amazon Elastic File System (Amazon EFS) file share in the Infrequent Access \nstorage class Configure the EFS file share to use an AWS Key Management Service (AWS KMS) \ncustomer master key (CMK) to encrypt the images in the file share. Use NFS permission sets on the \nimages to prevent accidental deletion "],"Explanation":"Answer: B \n \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":159,"QuestionContent":" \nAn image-hosting company stores its objects in Amazon S3 buckets. The company wants to avoid accidental \nexposure of the objects in the S3 buckets to the public. All S3 objects in the entire AWS account need to \nremain private \n \nWhich solution will meal these requirements? \n ","Option":["A. Use Amazon GuardDuty to monitor S3 bucket policies Create an automatic remediation action rule that \nuses an AWS Lambda function to remediate any change that makes the objects public ","B. Use AWS Trusted Advisor to find publicly accessible S3 Dockets Configure email notifications In \nTrusted Advisor when a change is detected manually change the S3 bucket policy if it allows public \naccess ","C. Use AWS Resource Access Manager to find publicly accessible S3 buckets Use Amazon Simple \nNotification Service (Amazon SNS) to invoke an AWS Lambda function when a change it detected. \nDeploy a Lambda function that programmatically remediates the change. ","D. Use the S3 Block Public Access feature on the account level. Use AWS Organizations to create a \nservice control policy (SCP) that prevents IAM users from changing the setting Apply tie SCP to tie \naccount "],"Explanation":"Answer: D \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":160,"QuestionContent":"The customers of a finance company request appointments with financial advisors by sending text messages. \nA web application that runs on Amazon EC2 instances accepts the appointment requests. The text messages \nare published to an Amazon Simple Queue Service (Amazon SQS) queue through the web application. \nAnother application that runs on EC2 instances then sends meeting invitations and meeting confirmation email \nmessages to the customers. After successful scheduling, this application stores the meeting information in an \nAmazon DynamoDB database. \n \nAs the company expands, customers report that their meeting invitations are taking longer to arrive. \n \n204 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nWhat should a solutions architect recommend to resolve this issue? \n ","Option":["A. Add a DynamoDB Accelerator (DAX) cluster in front of the DynamoDB database. ","B. Add an Amazon API Gateway API in front of the web application that accepts the appointment requests. ","C. Add an Amazon CloudFront distribution. Set the origin as the web application that accepts the \nappointment requests. ","D. Add an Auto Scaling group for the application that sends meeting invitations. Configure the Auto \nScaling group to scale based on the depth of the SQS queue. "],"Explanation":"Answer: D \n \nExplanation \nTo resolve the issue of longer delivery times for meeting invitations, the solutions architect can recommend \nadding an Auto Scaling group for the application that sends meeting invitations and configuring the Auto \nScaling group to scale based on the depth of the SQS queue. This will allow the application to scale up as the \nnumber of appointment requests increases, improving the performance and delivery times of the meeting \ninvitations. \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":161,"QuestionContent":" \nA company is running a critical business application on Amazon EC2 instances behind an Application Load \nBalancer The EC2 instances run in an Auto Scaling group and access an Amazon RDS DB instance \n \nThe design did not pass an operational review because the EC2 instances and the DB instance are all located in \na single Availability Zone A solutions architect must update the design to use a second Availability Zone \n \nWhich solution will make the application highly available? \n ","Option":["A. Provision a subnet in each Availability Zone Configure the Auto Scaling group to distribute the EC2 \ninstances across both \nAvailability Zones Configure the DB instance with connections to each network ","B. Provision two subnets that extend across both Availability Zones Configure the Auto Scaling group to \ndistribute the EC2 instances \nacross both Availability Zones Configure the DB instance with connections to each network ","C. Provision a subnet in each Availability Zone Configure the Auto Scaling group to distribute the EC2 \ninstances across both Availability Zones Configure the DB instance for Multi-AZ deployment ","D. Provision a subnet that extends across both Availability Zones Configure the Auto Scaling group to \ndistribute the EC2 instances \nacross both Availability Zones Configure the DB instance for Multi-AZ deployment \n205 of 230 Practice Test Amazon Web Services - SAA-C03 "],"Explanation":"Answer: C \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":162,"QuestionContent":" \nA company hosts its application on AWS The company uses Amazon Cognito to manage users When users \nlog in to the application the application fetches required data from Amazon DynamoDB by using a REST API \nthat is hosted in Amazon API Gateway. The company wants an AWS managed solution that will control \naccess to the REST API to reduce \n \ndevelopment efforts \n \nWhich solution will meet these requirements with the LEAST operational overhead? \n ","Option":["A. Configure an AWS Lambda function to be an authorize! in API Gateway to validate which user made \nthe request ","B. For each user, create and assign an API key that must be sent with each request Validate the key by \nusing an AWS Lambda function ","C. Send the user\u0027s email address in the header with every request Invoke an AWS Lambda function to \nvalidate that the user with that email address has proper access ","D. Configure an Amazon Cognito user pool authorizer in API Gateway to allow Amazon Cognito to \nvalidate each request "],"Explanation":"Answer: D \n \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":163,"QuestionContent":" \nA financial company hosts a web application on AWS. The application uses an Amazon API Gateway \nRegional API endpoint to give users the ability to retrieve current stock prices. The company\u0027s security team \nhas noticed an increase in the number of API requests. The security team is concerned that HTTP flood attacks \nmight take the application offline. \n \nA solutions architect must design a solution to protect the application from this type of attack. \nWhich solution meats these requirements with the LEAST operational overhead? ","Option":["A. Create an Amazon CloudFront distribution in front of the API Gateway Regional API endpoint with a \nmaximum TTL of 24 hours ","B. Create a Regional AWS WAF web ACL with a rate-based rule. Associate the web ACL with the API \nGateway stage. ","C. Use Amazon CloudWatch metrics to monitor the Count metric and alert the security team when the \npredefined rate is reached ","D. Create an Amazon CloudFront distribution with Lambda@Edge in front of the API Gateway Regional \n206 of 230 Practice Test Amazon Web Services - SAA-C03 \nAPI endpoint Create an AWS Lambda function to block requests from IP addresses that exceed the \npredefined rate. "],"Explanation":"Answer: B \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":164,"QuestionContent":" \nA company has an AWS Lambda function that needs read access to an Amazon S3 bucket that is located in the \nsame AWS account. Which solution will meet these requirement in the MOST secure manner? \n ","Option":["A. Apply an S3 bucket pokey that grants road access to the S3 bucket ","B. Apply an IAM role to the Lambda function Apply an IAM policy to the role to grant read access to the \nS3 bucket ","C. Embed an access key and a secret key In the Lambda function\u0027s coda to grant the required IAM \npermissions for read access to the S3 bucket ","D. Apply an IAM role to the Lambda function. Apply an IAM policy to the role to grant read access to all \nS3 buckets In the account "],"Explanation":"Answer: B \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":165,"QuestionContent":" \nA company has a web application with sporadic usage patterns There is heavy usage at the beginning of each \nmonth moderate usage at the start of each week and unpredictable usage during the week The application \nconsists of a web server and a MySQL database server running inside the data center The company would like \nto move the application to the AWS Cloud and needs to select a cost-effective database platform that will not \nrequire database modifications \n \nWhich solution will meet these requirements? \n ","Option":["A. Amazon DynamoDB ","B. Amazon RDS for MySQL ","C. MySQL-compatible Amazon Aurora Serverless ","D. MySQL deployed on Amazon EC2 in an Auto Scaling group "],"Explanation":"Answer: B \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":166,"QuestionContent":" \nA company has a popular gaming platform running on AWS. The application is sensitive to latency because \nlatency can impact the user experience and introduce unfair advantages to some players. The application is \ndeployed in every AWS Region. It runs on Amazon EC2 instances that are part of Auto Scaling groups \n \n207 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nconfigured behind Application Load Balancers (ALBs). A solutions architect needs to implement a mechanism \nto monitor the health of the application and redirect traffic to healthy endpoints. \n \nWhich solution meets these requirements? \n ","Option":["A. Configure an accelerator in AWS Global Accelerator. Add a listener for the port that the application \nlistens on, and attach it to a Regional endpoint in each Region. Add the ALB as the endpoint. ","B. Create an Amazon CloudFront distribution and specify the ALB as the origin server. Configure the \ncache behavior to use origin cache headers. Use AWS Lambda functions to optimize the traffic. ","C. Create an Amazon CloudFront distribution and specify Amazon S3 as the origin server. Configure the \ncache behavior to use origin cache headers. Use AWS Lambda functions to optimize the traffic. ","D. Configure an Amazon DynamoDB database to serve as the data store for the application. Create a \nDynamoDB Accelerator (DAX) cluster to act as the in-memory cache for DynamoDB hosting the \napplication data. "],"Explanation":"Answer: A \n \nExplanation \nAWS Global Accelerator directs traffic to the optimal healthy endpoint based on health checks, it can also \nroute traffic to the closest healthy endpoint based on geographic location of the client. By configuring an \naccelerator and attaching it to a Regional endpoint in each Region, and adding the ALB as the endpoint, the \nsolution will redirect traffic to healthy endpoints, improving the user experience by reducing latency and \nensuring that the application is running optimally. This solution will ensure that traffic is directed to the closest \nhealthy endpoint and will help to improve the overall user experience. \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":167,"QuestionContent":" \nA company wants to create an application to store employee data in a hierarchical structured relationship. The \ncompany needs a minimum-latency response to high-traffic queries for the employee data and must protect \nany sensitive data. The company also need to receive monthly email messages if any financial information is \npresent in the employee data. \n \nWhich combination of steps should a solutin architect take to meet these requirement? ( Select TWO.) \n ","Option":["A. Use Amazon Redshift to store the employee data in hierarchies. Unload the data to Amazon S3 every \nmonth. ","B. Use Amazon DynamoDB to store the employee data in hierarchies Export the data to Amazon S3 every \nmonth. ","C. Configure Amazon Macie for the AWS account Integrate Macie with Amazon EventBridge to send \nmonthly events to AWS Lambda. ","D. Use Amazon Athena to analyze the employee data in Amazon S3 integrate Athena with Amazon \nQuickSight to publish analysis dashboards and share the dashboards with users. \n208 of 230 Practice Test Amazon Web Services - SAA-C03 ","E. Configure Amazon Macie for the AWS account. integrate Macie with Amazon EventBridge to send \nmonthly notifications through an Amazon Simple Notification Service (Amazon SNS) subscription. "],"Explanation":"Answer: B E \n \n","RightAnswer":["B","E"],"QuestionChoose":[]},{"QuestionNumber":168,"QuestionContent":" \nA company is developing an ecommerce application that will consist of a load-balanced front end, a \ncontainer-based application, and a relational database. A solutions architect needs to create a highly available \nsolution that operates with as little manual intervention as possible. \n \nWhich solutions meet these requirements? (Select TWO.) \n ","Option":["A. Create an Amazon RDS DB instance in Multi-AZ mode. ","B. Create an Amazon RDS DB instance and one or more replicas in another Availability Zone. ","C. Create an Amazon EC2 in stance-based Docker cluster to handle the dynamic application load. ","D. Create an Amazon Elastic Container Service (Amazon ECS) cluster with a Fargate launch type to handle \nthe dynamic application load. ","E. Create an Amazon Elastic Container Service (Amazon ECS) cluster with an Amazon EC2 launch type to \nhandle the dynamic application load. "],"Explanation":"Answer: A D \n \nExplanation \nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html \n \n1. Relational database: RDS \n \n2. Container-based applications: ECS \n \n\u0022Amazon ECS enables you to launch and stop your container-based applications by using simple API calls. \nYou can also retrieve the state of your cluster from a centralized service and have access to many familiar \nAmazon EC2 features.\u0022 \n \n3. Little manual intervention: Fargate \n \nYou can run your tasks and services on a serverless infrastructure that is managed by AWS Fargate. \nAlternatively, for more control over your infrastructure, you can run your tasks and services on a cluster of \nAmazon EC2 instances that you manage. \n \n","RightAnswer":["A","D"],"QuestionChoose":[]},{"QuestionNumber":169,"QuestionContent":" \nAn ecommerce company is building a distributed application that involves several serverless functions and \nAWS services to complete order-processing tasks. These tasks require manual approvals as part of the \n \n209 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nworkflow A solutions architect needs to design an architecture for the order-processing application The \nsolution must be able to combine multiple AWS Lambda functions into responsive serverless applications The \nsolution also must orchestrate data and services that run on Amazon EC2 instances, containers, or on-premises \nservers \n \nWhich solution will meet these requirements with the LEAST operational overhead? \n ","Option":["A. Use AWS Step Functions to build the application. ","B. Integrate all the application components in an AWS Glue job ","C. Use Amazon Simple Queue Service (Amazon SQS) to build the application ","D. Use AWS Lambda functions and Amazon EventBridge (Amazon CloudWatch Events) events to build \nthe application "],"Explanation":"Answer: D \n \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":170,"QuestionContent":" \nA company provides an online service for posting video content and transcoding it for use by any mobile \nplatform. The application architecture uses Amazon Elastic File System (Amazon EFS) Standard to collect and \nstore the videos so that multiple Amazon EC2 Linux instances can access the video content for processing As \nthe popularity of the service has grown over time, the storage costs have become too expensive. \n \nWhich storage solution is MOST cost-effective? \n ","Option":["A. Use AWS Storage Gateway for files to store and process the video content ","B. Use AWS Storage Gateway for volumes to store and process the video content ","C. Use Amazon EFS for storing the video content Once processing is complete transfer the files to Amazon \nElastic Block Store (Amazon EBS) ","D. Use Amazon S3 for storing the video content Move the files temporarily over to an Amazon Elastic \nBlock Store (Amazon EBS) volume attached to the server for processing "],"Explanation":"Answer: D \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":171,"QuestionContent":" \nA company has a web server running on an Amazon EC2 instance in a public subnet with an Elastic IP \naddress. The default security group is assigned to the EC2 instance. The default network ACL has been \nmodified to block all traffic. A solutions architect needs to make the web server accessible from everywhere \non port 443. \n \nWhich combination of steps will accomplish this task? (Choose two.) \n \n210 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n ","Option":["A. Create a security group with a rule to allow TCP port 443 from source 0.0.0.0/0. ","B. Create a security group with a rule to allow TCP port 443 to destination 0.0.0.0/0. ","C. Update the network ACL to allow TCP port 443 from source 0.0.0.0/0. ","D. Update the network ACL to allow inbound/outbound TCP port 443 from source 0.0.0.0/0 and to \ndestination 0.0.0.0/0. ","E. Update the network ACL to allow inbound TCP port 443 from source 0.0.0.0/0 and outbound TCP port \n32768-65535 to destination 0.0.0.0/0. "],"Explanation":"Answer: A C \n \nExplanation \nThe combination of steps that will accomplish the task of making the web server accessible from everywhere \non port 443 is to create a security group with a rule to allow TCP port 443 from source 0.0.0.0/0 (A) and to \nupdate the network ACL to allow inbound TCP port 443 from source 0.0.0.0/0 (C). This will ensure that traffic \nto port 443 is allowed both at the security group level and at the network ACL level, which will make the web \nserver accessible from everywhere on port 443. \n \n","RightAnswer":["A","C"],"QuestionChoose":[]},{"QuestionNumber":172,"QuestionContent":" \nA company uses a payment processing system that requires messages for a particular payment ID to be \nreceived in the same order that they were sent Otherwise, the payments might be processed incorrectly. \n \nWhich actions should a solutions architect take to meet this requirement? (Select TWO.) \n ","Option":["A. Write the messages to an Amazon DynamoDB table with the payment ID as the partition key ","B. Write the messages to an Amazon Kinesis data stream with the payment ID as the partition key. ","C. Write the messages to an Amazon ElastiCache for Memcached cluster with the payment ID as the key ","D. Write the messages to an Amazon Simple Queue Service (Amazon SQS) queue Set the message \nattribute to use the payment ID ","E. Write the messages to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the message \ngroup to use the payment ID. "],"Explanation":"Answer: A E \n \n \n","RightAnswer":["A","E"],"QuestionChoose":[]},{"QuestionNumber":173,"QuestionContent":" \nA company has a Microsoft NET application that runs on an on-premises Windows Server Trie application \nstores data by using an Oracle Database Standard Edition server The company is planning a migration to AWS \nand wants to minimize development changes while moving the application The AWS application environment \n \n211 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nshould be highly available \n \nWhich combination of actions should the company take to meet these requirements? (Select TWO ) \n ","Option":["A. Refactor the application as serverless with AWS Lambda functions running NET Cote ","B. Rehost the application in AWS Elastic Beanstalk with the NET platform in a Multi-AZ deployment ","C. Replatform the application to run on Amazon EC2 with the Amazon Linux Amazon Machine Image \n(AMI) ","D. Use AWS Database Migration Service (AWS DMS) to migrate from the Oracle database to Amazon \nDynamoDB in a Multi-AZ deployment ","E. Use AWS Database Migration Service (AWS DMS) to migrate from the Oracle database to Oracle on \nAmazon RDS in a Multi-AZ deployment "],"Explanation":"Answer: B E \n \n","RightAnswer":["B","E"],"QuestionChoose":[]},{"QuestionNumber":174,"QuestionContent":" \nA company is moving its data management application to AWS. The company wants to transition to an \nevent-driven architecture. The architecture needs to the more distributed and to use serverless concepts whit \nperforming the different aspects of the workflow. The company also wants to minimize operational overhead. \nWhich solution will meet these requirements? ","Option":["A. Build out the workflow in AWS Glue Use AWS Glue to invoke AWS Lambda functions to process the \nworkflow slaps ","B. Build out the workflow in AWS Step Functions Deploy the application on Amazon EC2 Instances Use \nStep Functions to invoke the workflow steps on the EC2 instances ","C. Build out the workflow in Amazon EventBridge. Use EventBridge to invoke AWS Lambda functions on \na schedule to process the workflow steps. ","D. Build out the workflow m AWS Step Functions Use Step Functions to create a stale machine Use the \nstale machine to invoke AWS Lambda functions to process the workflow steps "],"Explanation":"Answer: C \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":175,"QuestionContent":" \nA company is designing a shared storage solution for a gaming application that is hosted in the AWS Cloud \nThe company needs the ability to use SMB clients to access data solution must be fully managed. \n \nWhich AWS solution meets these requirements? \n ","Option":["A. Create an AWS DataSync task that shares the data as a mountable file system Mount the file system to \n212 of 230 Practice Test Amazon Web Services - SAA-C03 \nthe application server ","B. Create an Amazon EC2 Windows instance Install and configure a Windows file share role on the \ninstance Connect the application server to the file share ","C. Create an Amazon FSx for Windows File Server file system Attach the file system to the origin server \nConnect the application server to the file system ","D. Create an Amazon S3 bucket Assign an IAM role to the application to grant access to the S3 bucket \nMount the S3 bucket to the application server "],"Explanation":"Answer: C \n \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":176,"QuestionContent":" \nA company recently deployed a new auditing system to centralize information about operating system versions \npatching and installed software for Amazon EC2 instances. A solutions architect must ensure all instances \nprovisioned through EC2 Auto Scaling groups successfully send reports to the auditing system as soon as they \nare launched and terminated \n \nWhich solution achieves these goals MOST efficiently? \n ","Option":["A. Use a scheduled AWS Lambda function and run a script remotely on all EC2 instances to send data to \nthe audit system. ","B. Use EC2 Auto Scaling lifecycle hooks to run a custom script to send data to the audit system when \ninstances are launched and terminated ","C. Use an EC2 Auto Scaling launch configuration to run a custom script through user data to send data to \nthe audit system when instances are launched and terminated ","D. Run a custom script on the instance operating system to send data to the audit system Configure the \nscript to be invoked by the EC2 Auto Scaling group when the instance starts and is terminated "],"Explanation":"Answer: B \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":177,"QuestionContent":" \nAn ecommerce company needs to run a scheduled daily job to aggregate and filler sales records for analytics. \nThe company stores the sales records in an Amazon S3 bucket. Each object can be up to 10 G6 in size Based \non the number of sales events, the job can take up to an hour to complete. The CPU and memory usage of the \nfob are constant and are known in advance. \n \nA solutions architect needs to minimize the amount of operational effort that is needed for the job to run. \nWhich solution meets these requirements? \n ","Option":["A. Create an AWS Lambda function that has an Amazon EventBridge notification Schedule the \nEventBridge event to run once a day \n213 of 230 Practice Test Amazon Web Services - SAA-C03 ","B. Create an AWS Lambda function Create an Amazon API Gateway HTTP API, and integrate the API \nwith the function Create an Amazon EventBridge scheduled avert that calls the API and invokes the \nfunction. ","C. Create an Amazon Elastic Container Service (Amazon ECS) duster with an AWS Fargate launch type. \nCreate an Amazon EventBridge scheduled event that launches an ECS task on the cluster to run the job. ","D. Create an Amazon Elastic Container Service (Amazon ECS) duster with an Amazon EC2 launch type \nand an Auto Scaling group with at least one EC2 instance. Create an Amazon EventBridge scheduled \nevent that launches an ECS task on the duster to run the job. "],"Explanation":"Answer: C \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":178,"QuestionContent":" \nA company sells datasets to customers who do research in artificial intelligence and machine learning (Al/ML) \nThe datasets are large, formatted files that are stored in an Amazon S3 bucket in the us-east-1 Region The \ncompany hosts a web application that the customers use to purchase access to a given dataset The web \napplication is deployed on multiple Amazon EC2 instances behind an Application Load Balancer After a \npurchase is made customers receive an S3 signed URL that allows access to the files. \n \nThe customers are distributed across North America and Europe The company wants to reduce the cost that is \nassociated with data transfers and wants to maintain or improve performance. \n \nWhat should a solutions architect do to meet these requirements? \n ","Option":["A. Configure S3 Transfer Acceleration on the existing S3 bucket Direct customer requests to the S3 \nTransfer Acceleration endpoint Continue to use S3 signed URLs for access control ","B. Deploy an Amazon CloudFront distribution with the existing S3 bucket as the origin Direct customer \nrequests to the CloudFront URL Switch to CloudFront signed URLs for access control ","C. Set up a second S3 bucket in the eu-central-1 Region with S3 Cross-Region Replication between the \nbuckets Direct customer requests to the closest Region Continue to use S3 signed URLs for access \ncontrol ","D. Modify the web application to enable streaming of the datasets to end users. Configure the web \napplication to read the data from the existing S3 bucket Implement access control directly in the \napplication "],"Explanation":"Answer: B \n \nExplanation \nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.html \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":179,"QuestionContent":" \n214 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nA company has migrated an application to Amazon EC2 Linux instances. One of these EC2 instances runs \nseveral 1-hour tasks on a schedule. These tasks were written by different teams and have no common \nprogramming language. The company is concerned about performance and scalability while these tasks run on \na single instance. A solutions architect needs to implement a solution to resolve these concerns. \n \nWhich solution will meet these requirements with the LEAST operational overhead? \n ","Option":["A. Use AWS Batch to run the tasks as jobs. Schedule the jobs by using Amazon EventBridge (Amazon \nCloudWatch Events). ","B. Convert the EC2 instance to a container. Use AWS App Runner to create the container on demand to \nrun the tasks as jobs. ","C. Copy the tasks into AWS Lambda functions. Schedule the Lambda functions by using Amazon \nEventBridge (Amazon CloudWatch Events). ","D. Create an Amazon Machine Image (AMI) of the EC2 instance that runs the tasks. Create an Auto \nScaling group with the AMI to run multiple copies of the instance. "],"Explanation":"Answer: C \n \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":180,"QuestionContent":" \nA company has deployed a web application on AWS. The company hosts the backend database on Amazon \nRDS for MySQL with a primary DB instance and five read replicas to support scallng needs. The read replicas \nmust lag no more than 1 second behind the primary DB instance. The database routinely runs scheduled stored \nprocedures. \n \nAs traffic on the website increases, the replicas experince addtional lag during periods of peak load. A \nsolutions architect must reduce the replication lag as much as possible. The solutin architect must minimize \nchanges to the application code and must minimize ongoing operational overhead. \n \nWhich solution will meet these requirements? \n ","Option":["A. Migrate the database to Amazon Aurora MySQL. Replace the read replicas with Aurora Replicas, and \nconfigure Aurora Auto Scaling. Replace the store procedures with Aurora MySQL native functions. ","B. Deploy an Amazon ElasticCache for Redis cluster in front of the database. Modify the application to \ncheck the cache before the applicatin queries the database. Replace the stored procedures with AWS \nLambda functions. ","C. Migrate the database to a MySQL database that runs on Amazon EC2 instances. Choose large, compute \noptimized EC2 instances for all replica nodes. Maintain the stored procedures on the EC2 instances. ","D. Migrate the database to Amazon DynamicDB provision a large number of read capacity units(RCUs) to \nsupport the required throught, and configure on-demand capacity scaling. Replace the store procedures \nwith DynamoDB streams "],"Explanation":"Answer: A \n \n215 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \n \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":181,"QuestionContent":" \nA company runs demonstration environments for its customers on Amazon EC2 instances. Each environment \nis isolated in its own VPC. The company\u2019s operations team needs to be notified when RDP or SSH access to \nan environment has been established. \n ","Option":["A. Configure Amazon CloudWatch Application Insights to create AWS Systems Manager OpsItems when \nRDP or SSH access is detected. ","B. Configure the EC2 instances with an IAM instance profile that has an IAM role with the \nAmazonSSMManagedInstanceCore policy attached. ","C. Publish VPC flow logs to Amazon CloudWatch Logs. Create required metric filters. Create an Amazon \nCloudWatch metric alarm with a notification action for when the alarm is in the ALARM state. ","D. Configure an Amazon EventBridge rule to listen for events of type EC2 Instance State-change \nNotification. Configure an Amazon Simple Notification Service (Amazon SNS) topic as a target. \nSubscribe the operations team to the topic. "],"Explanation":"Answer: C \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":182,"QuestionContent":" \nAn application running on an Amazon EC2 instance in VPC-A needs to access files in another EC2 instance in \nVPC-B. Both VPCs are in separate AWS accounts. The network administrator needs to design a solution to ","Option":["A. Set up a VPC peering connection between VPC-A and VPC-B. ","B. Set up VPC gateway endpoints for the EC2 instance running in VPC-B. ","C. Attach a virtual private gateway to VPC-B and set up routing from VPC-A. ","D. Create a private virtual interface (VIF) for the EC2 instance running in VPC-B and add appropriate \nroutes from VPC-A. "],"Explanation":"Answer: A \n \nExplanation \nAWS uses the existing infrastructure of a VPC to create a VPC peering connection; it is neither a gateway nor \na VPN connection, and does not rely on a separate piece of physical hardware. There is no single point of \nfailure for communication or a bandwidth bottleneck. \nhttps://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":183,"QuestionContent":" \n216 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nA company is building a new dynamic ordering website. The company wants to minimize server maintenance \nand patching. The website must be highly available and must scale read and write capacity as quickly as \npossible to meet changes in user demand. \n \nWhich solution will meet these requirements? \n ","Option":["A. Host static content in Amazon S3 Host dynamic content by using Amazon API Gateway and AWS \nLambda Use Amazon DynamoDB with on-demand capacity for the database Configure Amazon \nCloudFront to deliver the website content ","B. Host static content in Amazon S3 Host dynamic content by using Amazon API Gateway and AWS \nLambda Use Amazon Aurora with Aurora Auto Scaling for the database Configure Amazon CloudFront \nto deliver the website content ","C. Host al the website content on Amazon EC2 instances Create an Auto Scaling group to scale the EC2 \nInstances Use an Application Load Balancer to distribute traffic Use Amazon DynamoDB with \nprovisioned write capacity for the database ","D. Host at the website content on Amazon EC2 instances Create an Auto Scaling group to scale the EC2 \ninstances Use an Application Load Balancer to distribute traffic Use Amazon Aurora with Aurora Auto \nScaling for the database "],"Explanation":"Answer: A \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":184,"QuestionContent":" \nA solutions architect is designing a new API using Amazon API Gateway that will receive requests from \nusers. The volume of requests is highly variable; several hours can pass without receiving a single request. The \ndata processing will take place asynchronously, but should be completed within a few seconds after a request \nis made. \n \nWhich compute service should the solutions architect have the API invoke to deliver the requirements at the \nlowest cost? \n ","Option":["A. An AWS Glue job ","B. An AWS Lambda function ","C. A containerized service hosted in Amazon Elastic Kubernetes Service (Amazon EKS) ","D. A containerized service hosted in Amazon ECS with Amazon EC2 "],"Explanation":"Answer: B \n \nExplanation \nAPI Gateway \u002B Lambda is the perfect solution for modern applications with serverless architecture. \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":185,"QuestionContent":" \n217 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nA solutions architect is creating a new VPC design There are two public subnets for the load balancer, two \nprivate subnets for web servers and two private subnets for MySQL The web servers use only HTTPS The \nsolutions architect has already created a security group tor the load balancer allowing port 443 from 0 0 0 0/0 \nCompany policy requires that each resource has the teas! access required to still be able to perform its tasks \n \nWhich additional configuration strategy should the solutions architect use to meet these requirements? \n ","Option":["A. Create a security group for the web servers and allow port 443 from 0.0.0.0/0 Create a security group for \nthe MySQL servers and allow port 3306 from the web servers security group ","B. Create a network ACL for the web servers and allow port 443 from 0.0.0.0/0 Create a network ACL (or \nthe MySQL servers and allow port 3306 from the web servers security group ","C. Create a security group for the web servers and allow port 443 from the load balancer Create a security \ngroup for the MySQL servers and allow port 3306 from the web servers security group ","D. Create a network ACL \u0027or the web servers and allow port 443 from the load balancer Create a network \nACL for the MySQL servers and allow port 3306 from the web servers security group "],"Explanation":"Answer: C \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":186,"QuestionContent":" \nA company hosts a three-tier web application that includes a PostgreSQL database The database stores the \nmetadata from documents The company searches the metadata for key terms to retrieve documents that the \ncompany reviews in a report each month The documents are stored in Amazon S3 The documents are usually \nwritten only once, but they are updated frequency The reporting process takes a few hours with the use of \nrelational queries The reporting process must not affect any document modifications or the addition of new \ndocuments. \n \nWhat are the MOST operationally efficient solutions that meet these requirements? (Select TWO ) \n ","Option":["A. Set up a new Amazon DocumentDB (with MongoDB compatibility) cluster that includes a read replica \nScale the read replica to generate the reports. ","B. Set up a new Amazon RDS for PostgreSQL Reserved Instance and an On-Demand read replica Scale \nthe read replica to generate the reports ","C. Set up a new Amazon Aurora PostgreSQL DB cluster that includes a Reserved Instance and an Aurora \nReplica issue queries to the Aurora Replica to generate the reports. ","D. Set up a new Amazon RDS for PostgreSQL Multi-AZ Reserved Instance Configure the reporting \nmodule to query the secondary RDS node so that the reporting module does not affect the primary node ","E. Set up a new Amazon DynamoDB table to store the documents Use a fixed write capacity to support \nnew document entries Automatically scale the read capacity to support the reports "],"Explanation":"Answer: B C \n \n218 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \n","RightAnswer":["B","C"],"QuestionChoose":[]},{"QuestionNumber":187,"QuestionContent":" \nA company has an application thai runs on several Amazon EC2 instances Each EC2 instance has multiple \nAmazon Elastic Block Store (Amazon EBS) data volumes attached to it The application\u0027s EC2 instance \nconfiguration and data need to be backed up nightly The application also needs to be recoverable in a different \nAWS Region \n \nWhich solution will meet these requirements in the MOST operationally efficient way? \n ","Option":["A. Write an AWS Lambda function that schedules nightly snapshots of the application\u0027s EBS volumes and \ncopies the snapshots to a different Region ","B. Create a backup plan by using AWS Backup to perform nightly backups. Copy the backups to another \nRegion Add the application\u0027s EC2 instances as resources ","C. Create a backup plan by using AWS Backup to perform nightly backups Copy the backups to another \nRegion Add the application\u0027s EBS volumes as resources ","D. Write an AWS Lambda function that schedules nightly snapshots of the application\u0027s EBS volumes and \ncopies the snapshots to a different Availability Zone "],"Explanation":"Answer: B \n \nExplanation \nThe most operationally efficient solution to meet these requirements would be to create a backup plan by using \nAWS Backup to perform nightly backups and copying the backups to another Region. Adding the application\u0027s \nEBS volumes as resources will ensure that the application\u0027s EC2 instance configuration and data are backed \nup, and copying the backups to another Region will ensure that the application is recoverable in a different \nAWS Region. \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":188,"QuestionContent":" \nA company collects data from a large number of participants who use wearabledevices.The company stores \nthe data in an Amazon DynamoDB table and uses applications to analyze the data. The data workload is \nconstant and predictable. The company wants to stay at or below its forecasted budget for DynamoDB. \n \nWhihc solution will meet these requirements MOST cost-effectively? \n ","Option":["A. Use provisioned mode and DynamoDB Standard-Infrequent Access (DynamoDB Standard-IA). Reserve \ncapacity for the forecasted workload. ","B. Use provisioned mode Specify the read capacity units (RCUs) and write capacity units (WCUs). ","C. Use on-demand mode. Set the read capacity unite (RCUs) and write capacity units (WCUs) high enough \nto accommodate changes in the workload. ","D. Use on-demand mode. Specify the read capacity units (RCUs) and write capacity units (WCUs) with \nreserved capacity. \n219 of 230 Practice Test Amazon Web Services - SAA-C03 "],"Explanation":"Answer: C \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":189,"QuestionContent":" \nA company has one million users that use its mobile app. The company must analyze the data usage in \nnear-real time. The company also must encrypt the data in near-real time and must store the data in a \ncentralized location in Apache Parquet format for further processing. \n \nWhich solution will meet these requirements with the LEAST operational overhead? \n ","Option":["A. Create an Amazon Kinesis data stream to store the data in Amazon S3. Create an Amazon Kinesis Data \nAnalytics application to analyze the data. Invoke an AWS Lambda function to send the data to the \nKinesis Data Analytics application. ","B. Create an Amazon Kinesis data stream to store the data in Amazon S3. Create an Amazon EMR cluster \nto analyze the data. Invoke an AWS Lambda function to send the data to the EMR cluster. ","C. Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Create an \nAmazon EMR cluster to analyze the data. ","D. Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Create an \nAmazon Kinesis Data Analytics application to analyze the data "],"Explanation":"Answer: D \n \nExplanation \nThis solution will meet the requirements with the least operational overhead as it uses Amazon Kinesis Data \nFirehose, which is a fully managed service that can automatically handle the data collection, data \ntransformation, encryption, and data storage in near-real time. Kinesis Data Firehose can automatically store \nthe data in Amazon S3 in Apache Parquet format for further processing. Additionally, it allows you to create \nan Amazon Kinesis Data Analytics application to analyze the data in near real-time, with no need to manage \nany infrastructure or invoke any Lambda function. This way you can process a large amount of data with the \nleast operational overhead. \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":190,"QuestionContent":" \nA company wants to experiment with individual AWS accounts for its engineer team. The company wants to \nbe notified as soon as the Amazon EC2 instance usage for a given month exceeds a specific threshold for each \naccount. \n \nWhat should a solutions architect do to meet this requirement MOST cost-effectively? \n ","Option":["A. Use Cost Explorer to create a daily report of costs by service. Filter the report by EC2 instances. \nConfigure Cost Explorer to send an Amazon Simple Email Service (Amazon SES) notification when a \nthreshold is exceeded. ","B. Use Cost Explorer to create a monthly report of costs by service. Filter the report by EC2 instances. \nConfigure Cost Explorer to send an Amazon Simple Email Service (Amazon SES) notification when a \n220 of 230 Practice Test Amazon Web Services - SAA-C03 \nthreshold is exceeded. ","C. Use AWS Budgets to create a cost budget for each account. Set the period to monthly. Set the scope to \nEC2 instances. Set an alert threshold for the budget. Configure an Amazon Simple Notification Service \n(Amazon SNS) topic to receive a notification when a threshold is exceeded. ","D. Use AWS Cost and Usage Reports to create a report with hourly granularity. Integrate the report data \nwith Amazon Athena. Use Amazon EventBridge to schedule an Athena query. Configure an Amazon \nSimple Notification Service (Amazon SNS) topic to receive a notification when a threshold is exceeded. "],"Explanation":"Answer: C \n \nExplanation \nAWS Budgets allows you to create budgets for your AWS accounts and set alerts when usage exceeds a \ncertain threshold. By creating a budget for each account, specifying the period as monthly and the scope as \nEC2 instances, you can effectively track the EC2 usage for each account and be notified when a threshold is \nexceeded. This solution is the most cost-effective option as it does not require additional resources such as \nAmazon Athena or Amazon EventBridge. \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":191,"QuestionContent":" \nA company is developing a marketing communications service that targets mobile app users. The company \nneeds to send confirmation messages with Short Message Service (SMS) to its users. The users must be able to \nreply to the SMS messages. The company must store the responses for a year for analysis. \n \nWhat should a solutions architect do to meet these requirements? \n ","Option":["A. Create an Amazon Connect contact flow to send the SMS messages. Use AWS Lambda to process the \nresponses. ","B. Build an Amazon Pinpoint journey. Configure Amazon Pinpoint to send events to an Amazon Kinesis \ndata stream for analysis and archiving. ","C. Use Amazon Simple Queue Service (Amazon SQS) to distribute the SMS messages. Use AWS Lambda \nto process the responses. ","D. Create an Amazon Simple Notification Service (Amazon SNS) FIFO topic. Subscribe an Amazon \nKinesis data stream to the SNS topic for analysis and archiving. "],"Explanation":"Answer: B \nExplanation \nhttps://aws.amazon.com/pinpoint/product-details/sms/ Two-Way Messaging: Receive SMS messages from \nyour customers and reply back to them in a chat-like interactive experience. With Amazon Pinpoint, you can \ncreate automatic responses when customers send you messages that contain certain keywords. You can even \nuse Amazon Lex to create conversational bots. A majority of mobile phone users read incoming SMS \nmessages almost immediately after receiving them. If you need to be able to provide your customers with \nurgent or important information, SMS messaging may be the right solution for you. You can use Amazon \n \n221 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nPinpoint to create targeted groups of customers, and then send them campaign-based messages. You can also \nuse Amazon Pinpoint to send direct messages, such as appointment confirmations, order updates, and one-time \npasswords. \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":192,"QuestionContent":" \nA company is building a mobile app on AWS. The company wants to expand its reach to millions of users The \ncompany needs to build a platform so that authorized users can watch the company\u0027s content on their mobile \ndevices \n \nWhat should a solutions architect recommend to meet these requirements? \n ","Option":["A. Publish content to a public Amazon S3 bucket. Use AWS Key Management Service (AWS KMS) keys \nto stream content. ","B. Set up IPsec VPN between the mobile app and the AWS environment to stream content ","C. Use Amazon CloudFront Provide signed URLs to stream content. ","D. Set up AWS Client VPN between the mobile app and the AWS environment to stream content. "],"Explanation":"Answer: A \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":193,"QuestionContent":" \nA company that primarily runs its application servers on premises has decided to migrate to AWS. The \ncompany wants to minimize its need to scale its Internet Small \n \nComputer Systems Interface (iSCSI) storage on premises. The company wants only its recently accessed data \nto remain stored locally. \n \nWhich AWS solution should the company use to meet these requirements? \n ","Option":["A. Amazon S3 File Gateway ","B. AWS Storage Gateway Tape Gateway ","C. AWS Storage Gateway Volume Gateway stored volumes ","D. AWS Storage Gateway Volume Gateway cachea volumes "],"Explanation":"Answer: D \n \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":194,"QuestionContent":" \nA company has an Amazon S3 data lake that is governed by AWS Lake Formation The company wants to \ncreate a visualization in Amazon QuickSight by joining the data in the data lake with operational data that is \nstored in an Amazon Aurora MySQL database The company wants to enforce column-level authorization so \n \n222 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nthat the company\u0027s marketing team can access only a subset of columns in the database \nWhich solution will meet these requirements with the LEAST operational overhead? ","Option":["A. Use Amazon EMR to ingest the data directly from the database to the QuickSight SPICE engine Include \nonly the required columns ","B. Use AWS Glue Studio to ingest the data from the database to the S3 data lake Attach an IAM policy to \nthe QuickSight users to enforce column-level access control. Use Amazon S3 as the data source in \nQuickSight ","C. Use AWS Glue Elastic Views to create a materialized view for the database in Amazon S3 Create an S3 \nbucket policy to enforce column-level access control for the QuickSight users Use Amazon S3 as the \ndata source in QuickSight. ","D. Use a Lake Formation blueprint to ingest the data from the database to the S3 data lake Use Lake \nFormation to enforce column-level access control for the QuickSight users Use Amazon Athena as the \ndata source in QuickSight "],"Explanation":"Answer: D \n \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":195,"QuestionContent":" \nAn ecommerce company is experiencing an increase in user traffic. The company\u0027s store is deployed on \nAmazon EC2 instances as a two-tier web application consisting of a web tier and a separate database tier. As \ntraffic increases, the company notices that the architecture is causing significant delays in sending timely \nmarketing and order confirmation email to users. The company wants to reduce the time it spends resolving \ncomplex email delivery issues and minimize operational overhead. \n \nWhat should a solutions architect do to meet these requirements? \n ","Option":["A. Create a separate application tier using EC2 instances dedicated to email processing. ","B. Configure the web instance to send email through Amazon Simple Email Service (Amazon SES). ","C. Configure the web instance to send email through Amazon Simple Notification Service (Amazon SNS) ","D. Create a separate application tier using EC2 instances dedicated to email processing. Place the instances \nin an Auto Scaling group. "],"Explanation":"Answer: B \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":196,"QuestionContent":" \nA company is experiencing sudden increases in demand. The company needs to provision large Amazon EC2 \ninstances from an Amazon Machine image (AMI) The instances will run m an Auto Scaling group. The \ncompany needs a solution that provides minimum initialization latency to meet the demand. \n \n223 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nWhich solution meets these requirements? \n ","Option":["A. Use the aws ec2 register-image command to create an AMI from a snapshot Use AWS Step Functions to \nreplace the AMI in the Auto Scaling group ","B. Enable Amazon Elastic Block Store (Amazon EBS) fast snapshot restore on a snapshot Provision an \nAMI by using the snapshot Replace the AMI m the Auto Scaling group with the new AMI ","C. Enable AMI creation and define lifecycle rules in Amazon Data Lifecycle Manager (Amazon DLM) \nCreate an AWS Lambda function that modifies the AMI in the Auto Scaling group ","D. Use Amazon EventBridge (Amazon CloudWatch Events) to invoke AWS Backup lifecycle policies that \nprovision AMIs Configure Auto Scaling group capacity limits as an event source in EventBridge "],"Explanation":"Answer: B \n \nExplanation \nEnabling Amazon Elastic Block Store (Amazon EBS) fast snapshot restore on a snapshot allows you to \nquickly create a new Amazon Machine Image (AMI) from a snapshot, which can help reduce the initialization \nlatency when provisioning new instances. Once the AMI is provisioned, you can replace the AMI in the Auto \nScaling group with the new AMI. This will ensure that new instances are launched from the updated AMI and \nare able to meet the increased demand quickly. \n \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":197,"QuestionContent":" \nA company stores confidential data in an Amazon Aurora PostgreSQL database in the ap-southeast-3 Region \nThe database is encrypted with an AWS Key Management Service (AWS KMS) customer managed key The \ncompany was recently acquired and must securely share a backup of the database with the acquiring \ncompany\u0027s AWS account in ap-southeast-3. \n \nWhat should a solutions architect do to meet these requirements? \n ","Option":["A. Create a database snapshot Copy the snapshot to a new unencrypted snapshot Share the new snapshot \nwith the acquiring company\u0027s AWS account ","B. Create a database snapshot Add the acquiring company\u0027s AWS account to the KMS key policy Share the \nsnapshot with the acquiring company\u0027s AWS account ","C. Create a database snapshot that uses a different AWS managed KMS key Add the acquiring company\u0027s \nAWS account to the KMS key alias. Share the snapshot with the acquiring company\u0027s AWS account. ","D. Create a database snapshot Download the database snapshot Upload the database snapshot to an \nAmazon S3 bucket Update the S3 bucket policy to allow access from the acquiring company\u0027s AWS \naccount "],"Explanation":"Answer: A \n \n224 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":198,"QuestionContent":" \nA company is building a new web-based customer relationship management application. The application will \nuse several Amazon EC2 instances that are backed by Amazon Elastic Block Store (Amazon EBS) volumes \nbehind an Application Load Balancer (ALB). The application will also use an Amazon Aurora database. All \ndata for the application must be encrypted at rest and in transit. \n \nWhich solution will meet these requirements? \n ","Option":["A. Use AWS Key Management Service (AWS KMS) certificates on the ALB to encrypt data in transit. Use \nAWS Certificate Manager (ACM) to encrypt the EBS volumes and Aurora database storage at rest. ","B. Use the AWS root account to log in to the AWS Management Console. Upload the company\u2019s \nencryption certificates. While in the root account, select the option to turn on encryption for all data at \nrest and in transit for the account. ","C. Use a AWS Key Management Service (AWS KMS) to encrypt the EBS volumes and Aurora database \nstorage at rest. Attach an AWS Certificate Manager (ACM) certificate to the ALB to encrypt data in \ntransit. ","D. Use BitLocker to encrypt all data at rest. Import the company\u2019s TLS certificate keys to AWS key \nManagement Service (AWS KMS). Attach the KMS keys to the ALB to encrypt data in transit. "],"Explanation":"Answer: C \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":199,"QuestionContent":" \nA company hosts a web application on multiple Amazon EC2 instances The EC2 instances are in an Auto \nScaling group that scales in response to user demand The company wants to optimize cost savings without \nmaking a long-term commitment \n \nWhich EC2 instance purchasing option should a solutions architect recommend to meet these requirements\u0027? \n ","Option":["A. Dedicated Instances only ","B. On-Demand Instances only ","C. A mix of On-Demand instances and Spot Instances ","D. A mix of On-Demand instances and Reserved instances "],"Explanation":"Answer: A \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":200,"QuestionContent":" \nA company has an API that receives real-time data from a fleet of monitoring devices. The API stores this data \nin an Amazon RDS DB instance for later analysis. The amount of data that the monitoring devices send to the \nAPI fluctuates. During periods of heavy traffic, the API often returns timeout errors. \n \n225 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nAfter an inspection of the logs, the company determines that the database is not capable of processing the \nvolume of write traffic that comes from the API. A solutions architect must minimize the number of \nconnections to the database and must ensure that data is not lost during periods of heavy traffic. \n \nWhich solution will meet these requirements? \n ","Option":["A. Increase the size of the DB instance to an instance type that has more available memory. ","B. Modify the DB instance to be a Multi-AZ DB instance. Configure the application to write to all active \nRDS DB instances. ","C. Modify the API to write incoming data to an Amazon Simple Queue Service (Amazon SQS) queue. Use \nan AWS Lambda function that Amazon SQS invokes to write data from the queue to the database. ","D. Modify the API to write incoming data to an Amazon Simple Notification Service (Amazon SNS) topic. \nUse an AWS Lambda function that Amazon SNS invokes to write data from the topic to the database. "],"Explanation":"Answer: C \n \nExplanation \nUsing Amazon SQS will help minimize the number of connections to the database, as the API will write data \nto a queue instead of directly to the database. Additionally, using an AWS Lambda function that Amazon SQS \ninvokes to write data from the queue to the database will help ensure that data is not lost during periods of \nheavy traffic, as the queue will serve as a buffer between the API and the database. \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":201,"QuestionContent":" \nA solutions architect is designing the architecture of a new application being deployed to the AWS Cloud. The \napplication will run on Amazon EC2 On-Demand Instances and will automatically scale across multiple \nAvailability Zones. The EC2 instances will scale up and down frequently throughout the day. An Application \nLoad Balancer (ALB) will handle the load distribution. The architecture needs to support distributed session \ndata management. The company is willing to make changes to code if needed. \n \nWhat should the solutions architect do to ensure that the architecture supports distributed session data \nmanagement? \n ","Option":["A. Use Amazon ElastiCache to manage and store session data. ","B. Use session affinity (sticky sessions) of the ALB to manage session data. ","C. Use Session Manager from AWS Systems Manager to manage the session. ","D. Use the GetSessionToken API operation in AWS Security Token Service (AWS STS) to manage the \nsession "],"Explanation":"Answer: A \n \nExplanation \n \n226 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \nhttps://aws.amazon.com/vi/caching/session-management/ \n \nIn order to address scalability and to provide a shared data storage for sessions that can be accessible from any \nindividual web server, you can abstract the HTTP sessions from the web servers themselves. A common \nsolution to for this is to leverage an In-Memory Key/Value store such as Redis and Memcached. ElastiCache \nofferings for In-Memory key/value stores include ElastiCache for Redis, which can support replication, and \nElastiCache for Memcached which does not support replication. \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":202,"QuestionContent":" \nA company recently created a disaster recovery site in a Different AWS Region.The company needs to transfer \nlarge amounts of data back and forth between NFS file systems in the two Regions on a periods. \n \nWhich solution will meet these requirements with the LEAST operational overhead? \n ","Option":["A. Use AWS DataSync. ","B. Use AWS Snowball devices ","C. Set up an SFTP server on Amazon EC2 ","D. Use AWS Database Migration Service (AWS DMS) "],"Explanation":"Answer: A \n \n","RightAnswer":["A"],"QuestionChoose":[]},{"QuestionNumber":203,"QuestionContent":" \nA company hosts a three-tier ecommerce application on a fleet of Amazon EC2 instances. The instances run in \nan Auto Scaling group behind an Application Load Balancer (ALB) All ecommerce data is stored in an \nAmazon RDS for ManaDB Multi-AZ DB instance \n \nThe company wants to optimize customer session management during transactions The application must store \nsession data durably \n \nWhich solutions will meet these requirements? (Select TWO ) \n ","Option":["A. Turn on the sticky sessions feature (session affinity) on the ALB ","B. Use an Amazon DynamoDB table to store customer session information ","C. Deploy an Amazon Cognito user pool to manage user session information ","D. Deploy an Amazon ElastiCache for Redis cluster to store customer session information ","E. Use AWS Systems Manager Application Manager in the application to manage user session information "],"Explanation":"Answer: A B \n \n227 of 230 Practice Test Amazon Web Services - SAA-C03 \n \n \n","RightAnswer":["A","B"],"QuestionChoose":[]},{"QuestionNumber":204,"QuestionContent":" \nA company is migrating an old application to AWS The application runs a batch job every hour and is CPU \nintensive The batch job takes 15 minutes on average with an on-premises server The server has 64 virtual CPU \n(vCPU) and 512 GiB of memory \n \nWhich solution will run the batch job within 15 minutes with the LEAST operational overhead? \n ","Option":["A. Use AWS Lambda with functional scaling ","B. Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate ","C. Use Amazon Lightsail with AWS Auto Scaling ","D. Use AWS Batch on Amazon EC2 "],"Explanation":"Answer: D \n \nExplanation \nUse AWS Batch on Amazon EC2. AWS Batch is a fully managed batch processing service that can be used to \neasily run batch jobs on Amazon EC2 instances. It can scale the number of instances to match the workload, \nallowing the batch job to be completed in the desired time frame with minimal operational overhead. \n \nUsing AWS Lambda with Amazon API Gateway - AWS Lambda \nhttps://docs.aws.amazon.com/lambda/latest/dg/services-apigateway.html \nAWS Lambda FAQs \nhttps://aws.amazon.com/lambda/faqs/ \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":205,"QuestionContent":" \nA company needs to retain its AWS CloudTrail logs for 3 years. The company is enforcing CloudTrail across \na set of AWS accounts by using AWS Organizations from the parent account. The CloudTrail target S3 bucket \nis configured with S3 Versioning enabled. An S3 Lifecycle policy is in place to delete current objects after 3 \nyears. \n \nAfter the fourth year of use of the S3 bucket, the S3 bucket metrics show that the number of objects has \ncontinued to rise. However, the number of new CloudTrail logs that are delivered to the S3 bucket has \nremained consistent. \n \nWhich solution will delete objects that are older than 3 years in the MOST cost-effective manner? \n ","Option":["A. Configure the organization\u2019s centralized CloudTrail trail to expire objects after 3 years. ","B. Configure the S3 Lifecycle policy to delete previous versions as well as current versions. \n228 of 230 Practice Test Amazon Web Services - SAA-C03 ","C. Create an AWS Lambda function to enumerate and delete objects from Amazon S3 that are older than 3 \nyears. ","D. Configure the parent account as the owner of all objects that are delivered to the S3 bucket. "],"Explanation":"Answer: B \n \nExplanation \nhttps://docs.aws.amazon.com/awscloudtrail/latest/userguide/best-practices-security.html#:~:text=The%20Cloud \n","RightAnswer":["B"],"QuestionChoose":[]},{"QuestionNumber":206,"QuestionContent":"A company is running a multi-tier recommence web application in the AWS Cloud. The application runs on \nAmazon EC2 instances with an Amazon RDS for MySQL Multi-AZ OB instance. Amazon ROS is configured \nwith the latest generation DB instance with 2.000 GB of storage In a General Purpose SSD (gp3) Amazon \nElastic Block Store (Amazon EBSl volume. The database performance affects the application during periods \nhigh demand. \n \nA database administrator analyzes the logs in Amazon CloudWatch Logs and discovers that the application \nperformance always degrades when the number of read and write IOPS is higher than 20.000. \n \nWhat should a solutions architect do to improve the application performance? \n ","Option":["A. Replace the volume with a magnetic volume. ","B. Increase the number of IOPS on the gp3 volume. ","C. Replace the volume with a Provisioned IOPS SSD (Io2) volume. ","D. Replace the 2.000 GB gp3 volume with two 1.000 GB gp3 volumes "],"Explanation":"Answer: C \n \n \n","RightAnswer":["C"],"QuestionChoose":[]},{"QuestionNumber":207,"QuestionContent":" \nA company is hosting a three-tier ecommerce application in the AWS Cloud. The company hosts the website \non Amazon S3 and integrates the website with an API that handles sales requests. The company hosts the API \non three Amazon EC2 instances behind an Application Load Balancer (ALB). The API consists of static and \ndynamic front-end content along with backend workers that process sales requests asynchronously. \n \nThe company is expecting a significant and sudden increase in the number of sales requests during events for \nthe launch of new products \n \nWhat should a solutions architect recommend to ensure that all the requests are processed successfully? \n ","Option":["A. Add an Amazon CloudFront distribution for the dynamic content. Increase the number of EC2 instances \nto handle the increase in traffic. \n229 of 230 Practice Test Amazon Web Services - SAA-C03 ","B. Add an Amazon CloudFront distribution for the static content. Place the EC2 instances in an Auto \nScaling group to launch new instances based on network traffic. ","C. Add an Amazon CloudFront distribution for the dynamic content. Add an Amazon ElastiCache instance \nin front of the ALB to reduce traffic for the API to handle. ","D. Add an Amazon CloudFront distribution for the static content. Add an Amazon Simple Queue Service \n(Amazon SOS) queue to receive requests from the website for later processing by the EC2 instances. "],"Explanation":"Answer: D \n \n","RightAnswer":["D"],"QuestionChoose":[]},{"QuestionNumber":208,"QuestionContent":" \nA company is deploying a new application on Amazon EC2 instances. The application writes data to Amazon \nElastic Block Store (Amazon EBS) volumes. The company needs to ensure that all data that is written to the \nEBS volumes is encrypted at rest. \n \nWhich solution wil meet this requirement? \n ","Option":["A. Create an IAM role that specifies EBS encryption. Attach the role to the EC2 instances. ","B. Create the EBS volumes as encrypted volumes Attach the EBS volumes to the EC2 instances. ","C. Create an EC2 instance tag that has a key of Encrypt and a value of True. Tag all instances that require \nencryption at the ESS level. ","D. Create an AWS Key Management Service (AWS KMS) key policy that enforces EBS encryption in the \naccount Ensure that the key policy is active. "],"Explanation":"Answer: B \n \n230 of 230 About Marks4sure.com \n \nmarks4sure.com was founded in 2007. We provide latest \u0026 high quality IT / Business Certification Training Exam \nQuestions, Study Guides, Practice Tests. \n \nWe help you pass any IT / Business Certification Exams with 100% Pass Guaranteed or Full Refund. Especially \nCisco, CompTIA, Citrix, EMC, HP, Oracle, VMware, Juniper, Check Point, LPI, Nortel, EXIN and so on. \n \nView list of all certification exams: All vendors \n \n \n \n \n \n \n \n \n \n \n \n \n \nWe prepare state-of-the art practice tests for certification exams. You can reach us at any of the email addresses listed \nbelow. \n \n   Sales: sales@marks4sure.com \n   Feedback: feedback@marks4sure.com \n  Support: support@marks4sure.com \n \nAny problems about IT certification or our products, You can write us back and we will get back to you within 24 \nhours. \n ","RightAnswer":["B"],"QuestionChoose":[]}],"isFinish":false,"isShowExplanation":false}